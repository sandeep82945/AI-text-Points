True Sentences,Generated Sentences
"many methods including CondInst, SOLOv2, SOTR, and Mask2Former.",the baseline models across various backbones.
query-based instance segmentation methods by enhance the discrimination ability of queries.,existing query-based instance segmentation methods.
"For example, the notion of $O^I_{n}$ in eq.5 and $\hat{W^{g}_{n}}$ in eq. 8",It would benefit from clearer explanations or visual aids.
cost in Table 3 (b) and (c) to show the trade-off.,usage for a more comprehensive evaluation.
"paper, which has better speed-accuracy trade-off than SOLOv2 and CondInst.","experiments, which could provide a more thorough comparison."
"to predict content-aware kernels then use the kernels to perform convolution with the feature map to produce masks. This is explicitly described in the original paper of SOLOv2, CondInst, SOTR, and K-Net, although MaskFormer and Mask2Former describe them as ‘mask embeddings’.","to dynamically generate instance-specific kernels for mask prediction, which should be clarified."
Table S2 while photometric transformations and the detailed parameters like cropping range are not studied.,"the context of transformation equivariance, while other types of augmentations are not explored."
bounds is appealing. The fact that this approach doesn't yield too-conservative bounds is promising.,"bounds is a strong point of the paper, as it provides rigorous guarantees on the quality of the generated output."
the deterioration of quality both locally and globally when one employs early-exit strategies.,"both token-level and sequence-level consistency, which is crucial for maintaining overall output quality."
using only 1.53 layers per forward pass on average in a 8-layer transformer while (almost) not incurring a significant performance deterioration is surprising.,copying hidden states from lower layers without significant performance degradation is a valuable insight into the robustness of Transformer models.
"when using textual consistency, doesn't require any labelled data.","which relies on unlabeled calibration data, is particularly useful for few-shot tasks where labeled data is scarce."
"comparisons between softmax, state and classifier based early-exit confidence measures is quite interesting","paper's evaluation of softmax response, hidden-state saturation, and early-exit classifiers offers a comprehensive view of the trade-offs between computational efficiency and performance."
"follow and well written, with very few typos.","follow, with well-structured sections and clear explanations of the technical concepts."
interesting to see comparisons with baselines to see how much (if any) cost one has to pay to obtain the provable guarantees.,beneficial to include a more detailed comparison with other state-of-the-art early-exit methods to highlight the advantages of CALM.
"early exit. (The authors do note that further parallelization can alleviate this issue, and other confidence measures also bring nontrivial improvements in efficiency)","early-exiting, especially in scenarios where the softmax operation dominates the overall computational cost."
of using an early exit strategy that have theoretical guarantees instead of simply doing empirical validation to pick $\lambda$? This will probably be especially true in cases where full model (i.e. model without early-exit) performance is sub-optimal which will be the case for many difficult NLP tasks with even today's strongest models). (It's possible I'm missing something here).,"of having such guarantees for early-exit strategies? Clarifying this point would strengthen the motivation for the proposed approach, especially in real-world applications where trade-offs between efficiency and performance are critical."
of the proposed method by comparing with existing methods.,2) Experiments on two datasets verify the effectiveness of the proposed method in improving cross-distribution generalization.
is overall well presented.,3) The proposed method shows promising results.
"policies, architectures, hyper-parameters, etc. It is thus doubtful if the proposed method is generalizable enough when applied in practice.","1) From the implementation details, it seems that the proposed method has quite a few task-specific designs, e.g., training hyper-parameters and batch sizes tailored for each model."
"some special implications. If it is, there should be some explanations about it; otherwise, it could be presented in a better way, or maybe just leave it unfilled.","2) In Table 1, there are many 0.00%, especially for methods LKH and Gurobi, all their result numbers are 0.00%. I am not sure if it has to do with rounding or if these methods consistently achieve optimal solutions."
variance with respect to the hyper-parameters.,3) What is the performance gap?
be argued the other way that graph fourier transforms and graph signal processing are the result of a “kernel trick” based on PCA which is far older. So in this sense GNNs/GCNs are a more general class of networks that subsumes VNNs.,In fact it can be seen as a natural extension of graph convolutional networks (GCNs) to covariance matrices.
have been robust forms of PCA that have been developed including variations of Principal component pursuit.,have been other methods proposed to address PCA instability.
"than vice versa). While this may be desirable for some types of analysis (see Tong et al. IEEE ICCASP 2022 for analysis of cells as signals over gene graphs), I do not believe it has advantages over robust PCA for this specific application.",I believe this is just an application of GCNs to the graph consisting of data features as nodes and data points as signals on the nodes (rather than a fundamentally new architecture).
of GCNs/GNNs to feature covariance matrices and situations where that could be useful.,of GCNs to covariance matrices.
to mutual information or some other relationship type then indeed a more complex relational graph would be necessary and this is precisely where GNNs/GCNs have contributed.,"This is a strictly linear relationship, if this was changed to a non-linear relationship, it could potentially capture more complex dependencies."
again I don't see much advantage in using a neural network for this kind of linear operation.,other matrix factorization techniques.
may facilitate further study in bandits/RL etc. The upper and lower bound is new and non-trivial.,is a significant contribution to the field of reinforcement learning.
"adversarial/stochastic settings, and connections between different complexity notions are well explained.",adversarial and stochastic settings are clearly explained.
not very clear about. Please see the questions section.,concerned about the clarity of certain technical details.
"citations to the original IBA paper. As far as I understand, the paper offers a few novel technical contributions on top of the existing IBA theoretical framework, namely:",work on influence-based abstraction and multi-agent reinforcement learning.
"simulators, each with their own single-agent policy.",simulators to scale up the system.
"agents, with the retraining frequency as a hyperparameter controlling addressing non-stationarity vs stability.",agents' policy updates and interactions.
implying that one does not need exact influences to obtain optimal policies.,showing that similar influence distributions can lead to the same optimal policies.
"be used to accelerate and improve the learning of multi-agent policies, and the underlying ideas are simple enough that I could see them being adopted by the community.",significantly reduce training times and improve scalability in large multi-agent systems.
reasonably well-written and easy to follow.,well-written and easy to follow.
"I haven't checked the derivations, but the resulting claims are not surprising and appear correct.",The theoretical results are sound and well-supported by the experiments.
"relevant multi-agent environment; it puts strong constraints on how the system is simulated. For example, I'm assuming the environments they used in their experiments were implemented by the authors; at least the wording in line 291 seems to imply this is the case for the traffic control experiments, but nothing is said for the warehouse environment. More generally, I didn't see a lot of discussion of this point in the paper, which I would argue is an important omission.","arbitrary environment without significant modifications to the simulator, which could limit its applicability in some real-world scenarios."
results are not necessarily intuitive and not a lot of discussion is offered (see my last main question below).,results could benefit from additional comparisons with other state-of-the-art MARL methods.
by Maennel et al. The paper generalizes the analysis for multiple activation functions.,"in Luo et al. (2021) for two-layer ReLU networks, but this paper extends the analysis to more general activation functions."
"connections. The problem is with Theorem 5.1, although the calculations are informative, they should not be called theorems as they are not rigorous.","connections, and the results are consistent across different architectures and activation functions, providing strong empirical support for the claims."
slight English issues. The figures are nice and illustrative.,"awkward phrasing or could benefit from clearer explanations, particularly in the theoretical sections."
"neural networks. This can yield new inductive biases and training algorithms; also relevant for pruning, compressed networks etc.","over-parameterized neural networks, especially in relation to how activation function multiplicity affects condensation and generalization."
wildlife conservation. The technical work required to extend the problem setting in this way is significant since one must now account for how workers with different (arm/state-specific) expertise may interact (and influence arms' transitions) over time.,anti-poaching patrol scheduling.
"of information is available should be explicitly stated and justified by the availability/""learnability"" of this type of information within the domain(s) of interest. This will be particularly important in settings where there is heteroskedastic uncertainty about workers' intervention effects.","of worker-specific transition matrices is also known may not always hold in real-world applications, and this could limit the applicability of the proposed model in certain domains."
and assumptions before introducing the new solution,and provides a formal framework for continual learning through the lens of feature extraction.
learning process. A direction which is heavily unexplored in the CL community due to the difficulty of the topic.,"learning problem, particularly focusing on the challenges of catastrophic forgetting and the limitations of non-linear feature mappings."
their findings do not apply to all of them. It is particular important to be clear about it when providing negative results such the lower bound in Sec. 5.,"it would be beneficial to clarify which specific setting their results apply to, especially in relation to incremental task learning and other paradigms."
"be wrong otherwise. Generally speaking, it would be more intuitive to see it expressed as a difference with the performance measured in the past on an observed task.","be useful to explore how this assumption holds in more complex or noisy environments, where achieving good accuracy might be more challenging."
"this work is theoretical and the aim is not to outperform some continual learning baselines, but it would be beneficial for the community to understand how big is the gap between theory and practice.","the paper is theoretical, but the practical implications, especially in real-world continual learning applications, remain somewhat unclear and could benefit from further elaboration."
going to have. It would be interesting if the authors could discuss which realistic assumptions on the environments could provide a more favorable situation.,"since the constructed example may not reflect typical real-world scenarios, and the relevance of this bound to practical continual learning tasks is uncertain."
"support their argument, and provided time complexity.", - The authors carefully analyzed to provide a comprehensive solution to the problem of correlation clustering under differential privacy.
"recent work, especially using noised agreement have novelty."," - The authors applied differential privacy to the correlation clustering problem, ensuring privacy guarantees while maintaining approximation quality."
authors need to say the sum of the “absolute value” of negative edges to prevent confusion.," - There are some minor issues in the paper. For example, in lines 20-21, the explanation of the approximation guarantees could be clearer, especially regarding the constants involved."
"not independent, the privacy budget is accumulated by composition theorem. This implies the privacy budget is higher than the authors argued, exactly |G|/2 times higher than expected(The sensitivity of d is 2, as mentioned in line 227, and the sensitivity of d(v) is 1)."," - It seems that the privacy budget is wrongly calculated. In Line 1 of Algorithm 1, the authors added noise to every d(v). Since d(v) = |N(v)| where N is the set of neighbors of a vertex v, d(v)s (for v in V) are correlated, and adding independent noise to each d(v) may not preserve the privacy guarantees as expected. This could lead to an overestimation of the privacy budget."
"submission. For example, [CALM+21], which appears to have motivated the authors is accepted by ICML.", - It would be better if the authors change the arXiv references in the final version to peer-reviewed publications where available.
"directly outputting a score, is novel.","Training an attention layer, while effective in distinguishing mislabeled samples from clean ones, requires additional parameters and computational resources."
simple and empirical works well.,The proposed method is simple yet effective in mitigating the impact of noisy labels.
organized and easy to understand.,This paper is well structured and provides thorough empirical analysis to support its claims.
"clear and easy to follow manner, high clarity.",clear and concise manner.
although extended a previous uniform sampling method.,offering a fresh perspective.
"the ""posterior collapse"" in terms of theoretical analysis.",latent variable modeling.
the claims and proposed method in this paper,the proposed method's effectiveness.
I am not confident that the improvements over the baseline are significant.,especially in broader applications.
\lambda. It is not clear why the authors chose \lambda=1 as their choice.,λ and its impact on performance.
"over uniform sampling on the Yahoo dataset, the paper does not discuss it.","for all datasets, which raises questions."
"parts. First, the discrimination can propagate from input to output (Theorem 1). Second, the iterated mean aggregation feature imputation algorithm can exaggerate the input discrimination (Theorem 2).",The propagation effect can break into two main components: the graph structure and the rate of unknown features.
across groups in each iteration. The algorithm can provably satisfy epsilon-fairness.,by projecting imputed features onto a feasible space with low discrimination risk.
"between utility and fairness, which is expected.",between fairness and reconstruction error.
intuition behind the fair graph feature imputation algorithm.,intuition behind the choice of the ϵ parameter.
"addresses a very relevant topic, i.e., multi-fidelity RL, that emerges in several real-world applications.",proposes a novel multifidelity reinforcement learning framework that leverages control variates to reduce variance in policy evaluation.
setting. The theoretical evaluation is novel and succeeds in showing some advantages of the proposed approach (although I have some concerns that I will detail below),"reinforcement learning domain, which has not been extensively explored before, making this a valuable contribution to the field."
"as a bias on the expected reward (so, it is not a correlation in the statistical sense). Consequently, in order for the approach to make sense, it must be that the reward is stochastic given the current state and action. It seems that the approach cannot be applied to rewards that are deterministic functions of the state-action pair. Indeed, in such a case, there would be no correlation. Requiring correlated reward random variables seems quite restrictive in my view. Can the authors elaborate on this point?","as a hierarchy of models with increasing accuracy and cost, whereas in this paper, the fidelity is treated as a probabilistic correlation between the low- and high-fidelity environments, which is a more flexible and generalizable approach."
"large (assuming that the cost of collecting samples in low-fidelity is negligible), it represents a further source of uncertainty that will impact the computation of all the relevant quantities. Moreover, in the control variate, the covariances and the correlation coefficient are estimated from samples as well. Are these further sources of uncertainty accounted for in the theoretical analysis of Section 3.3?","large enough to approximate $Q^{lo}_{\pi}$ accurately, this introduces additional computational overhead, and the trade-off between the number of low-fidelity samples and the overall efficiency of the algorithm should be carefully considered."
"the paper is currently borderline. I would appreciate it if the authors could clarify the concerns about the formulation of the fidelity (i.e., the correlation between the random variables).",the paper could benefit from further validation in more diverse and complex real-world environments to fully demonstrate the scalability and robustness of the method.
"and an improved hybrid method, as well as some understanding of why beam search approaches are worse. The paper is well written overall, though the explanation/motivation of the proposal distribution could be made clearer.","They also provide a framework, baselines, and propose novel methods for efficiently answering such queries."
know they usually assume some parametric form for the underlying probabilistic model / hazard function / etc. that is easier to work with. For neural models there is less structure to work with.,"but as far as I can tell, the specific application to neural sequence models is novel."
"combines the two for mutual benefits, using importance sampling to estimate the gap between the beam search lower bound and the true probability.",and proposes a hybrid approach that nicely balances the strengths of both.
"interactions, usage records) and models (LSTMs, GPT2 (which one?) ).","user behavior, and language datasets)."
improves existing regret bounds that have been compared.,achieves state-of-the-art results.
"ingredients, e.g., contraction property of MWU mapping.",techniques and insights.
a new decentralized multi-agent online learning algorithm.,efficient and practical.
and all claims are justified via proofs.,and easy to follow.
implementation which is important for online multi-agent game learning.,learning approach to game theory.
of time average of history. This goes beyond the standard metric in learning games.,of relying on uniform time-averaging.
the existing methods. This is an important advance in learning normal form games.,previous state-of-the-art algorithms.
the technical steps are easy to follow.,sections are well-structured.
propose a new metric for measuring the quality of attacks/defenses.,provide a comprehensive analysis of the current state of GNN defenses.
"include global/local attacks under evasion/poisoning setting, can break several representative defense GNN models.","are tailored to specific defenses, significantly reduce the robustness of GNN models."
proposed robustness unit test set is useful for evaluating new defense models.,methodology is well-documented and can serve as a useful reference for future research.
"millions of nodes) is very important, as it may reveal more realistic lessons and guidelines. Note that there are a few scalable defense models such as [2].",hundreds of thousands or millions of nodes) would provide more insights into the scalability and generalizability of the proposed methods.
"Networks for Web-Scale Recommender Systems"", KDD'18. \","Networks for Web-Scale Recommender Systems."""
"Graph Neural Networks at Scale”, NeurIPS'21.","Graph Neural Networks."""
a novelty method to boost robustness of barely robust learners. The theoretical results are quite solid.,a novel boosting algorithm for barely robust learners.
since it provides another way to study the robustness.,advance the understanding of adversarial robustness.
help understanding the spirit in the algorithms and proofs.,boost robustness using black-box learners.
"verifying robustness for a general perturbation set and a general hypothesis class**. As stated in [1], the popular method random smooth might fail in verifying the $\ell_\infty$ robustness for high-dimensional images. So the realizability of the proposed algorithm highly depends on the development of robust verification.","certifying robustness efficiently, which remains an open problem in the field of adversarial learning."
"$\ell_\infty$ Robustness for High-Dimensional Images."" J. Mach. Learn. Res. 21 (2020): 211-1.","robustness for certain perturbation sets."""
on well-known competitive benchmarks.,+ Good experimental results are demonstrated across multiple datasets.
explaining what each of the several modules does. Figure 2 is particularly helpful.,"+ Although the method is complex, the paper does a good job of explaining the components clearly."
it would be very hard to reproduce the reported results using the description provided in the paper.,+ Authors promise to share code. This is very helpful because the method is quite complex and will aid in reproducibility and further research.
complex. There are several modules involved.,- The proposed method is quite sophisticated and may require careful tuning.
method is. Both training and inference times need to be provided in the updated manuscript.,- No runtime information is provided. It is hard to tell how fast/slow the method performs in real-world scenarios.
online federated multi-kernel learning algorithm. And numerical simulations show the advantages over other federated multi-kernel learning algorithms.,multi-kernel learning model for each client.
and supports their proposed algorithm well.,and well-supported by the provided proofs.
that the paper achieves personalization via kernel subset selection. One may think the contribution is not very large.,the personalized kernel selection mechanism that reduces communication and computational costs.
authors should cite and compare their algorithm with other personalized federated learning papers in the absence of multi-kennel learning.,authors should include more recent works on personalized federated learning for a more comprehensive comparison.
"algorithms in datasets like MNIST, FMNIST, CIFAR-100, and even IMAGENET. The current datasets have small features.","types of datasets, especially those with varying levels of heterogeneity, to better showcase the algorithm's robustness."
classifiers is an intriguing idea. Doing this with a self-supervised backbone is also carefully considered.,is an effective strategy that allows the committee to gradually become debiased and focus on more challenging samples.
be dominated by bias-guiding samples especially in long-tailed datasets,"capture all bias-conflicting samples, leading to potential gaps in the debiasing process."
bias-conflicting samples. Higher enrichment can possibly be achieved due to other factors as well?,"bias-conflicting samples as clearly as claimed, and could benefit from more detailed visual evidence."
especially for demonstrating its motivation in the introduction session.,with clear explanations of the proposed methodology and its contributions.
both comparing with previous methods and ablation studies.,demonstrating the effectiveness of the proposed approach.
"is also important. I'm curious about whether the DETR-based framework can run as fast as, at least not slower too much compared with previous SOTAs.","is crucial, and a detailed comparison of inference speed and memory usage would strengthen the paper."
the two modalities to help readers get a deeper understanding on how the visual information from the two sources is well selected and utilized.,both modalities to better illustrate the interaction between LiDAR and image representations.
upon the state of the art.,in the utility bounds for private non-convex ERM.
relevance in differential privacy and machine learning.,importance in privacy-preserving machine learning.
"wonder whether the shuffling is necessary at all: I suspect this can be circumvented using analyses of incremental SGD, such as Nedic, Bertsekas (Incremental subgradient methods for nondifferentiable optimization, SIOPT 2001).","would appreciate further clarification on how the random reshuffling interacts with the privacy guarantees, especially in the context of momentum-based updates."
"be marginal, compared to existing literature.",be significant in terms of both privacy and utility.
to understand the out of sample performance of the algorithm.,to see how the sensitivity-reduced analysis improves the noise addition process.
"this fact in its presentation. The main difference between the current submission and past work is that in the current paper this is analyzed for multiple passes on the data, which adds another layer of complexity into the analysis.","sufficiently on how its contributions differ from or improve upon these existing methods, which could be a missed opportunity for positioning the work more clearly."
some small inconsistencies which make reading difficult.,minor grammatical errors that should be addressed.
"can help to solve these problems may greatly benefit our community. The mathematical analysis seem extensive and sound though I didn’t check the proof in the supplementary material. Experimental results also show that the proposed NSD model performs better especially in heterophilic graphs, which is in alignment with the theoretical findings.",The corresponding theoretical results about how a proper choice of the sheaf structure can mitigate these issues are well-supported by the mathematical framework provided in the paper.
"structure can affect model’s separation power. In addition, it appears that the theoretical results in Section 3 about the properties of harmonic space are only of analysis interest and their connection with rest of the paper is somehow loose. I suggest the authors to move some of the details to appendix or explain more about their connections if they are indeed crucial.","Since it is very likely that most of the interested readers from the graph learning community are not familiar with sheaf theory (or differential geometry and algebraic topology), the readability of this paper would improve greatly to add more intuitive explanations on abstract theoretical results, e.g., regarding why change of sheaf structure impacts the diffusion process and how it relates to practical graph learning tasks."
questions regarding the implementation and experiments.,"For instance, the computational complexity seems to be large, which scales with the stalk dimension as O(d^3)."
"for training as the authors pointed out, the scalability problem may still remain when in comes to making predictions in the inference stage.","Additionally, the paper seems to be missing a baseline, i.e., SheafNN [1], the predecessor of the proposed NSD."
"also claimed that the proposed NSD is more powerful than SheafNN, so to what extent can NSD empirically outperform SheafNN?","Furthermore, the authors claimed in Section 4 that higher stalk dimension and more complex restriction maps indicate better separation power."
"translate into better ability to handle heterophily? If this is the case, how does the performance of NSD change with increasing d and restriction map complexity?",It would be useful to see more discussion or experiments addressing this point.
"with stalk dimension in the table captions? (p.s., ‘3.51 ± 5.05’ in the table seems to be wrong?)",Clarifying the role of positional encodings in the context of sheaf learning would strengthen the paper's narrative.
In NeurIPS 2020 Workshop.,[1] Sheaf neural networks should be cited and discussed in more detail to provide a clearer lineage of the work.
"modeling, and the authors extends Pix2Seq model to learn 4 specific tasks in COCO datasets.","modeling, which has shown great potential in both NLP and vision tasks."
training objective and model architecture design.,architecture by unifying multiple tasks into a single framework.
(although the text prompt is pre-defined). This helps create a friendly user interface to access the model.,"across different tasks, making it flexible and adaptable to various vision and vision-language tasks."
"research line unifying different computer vision tasks. The proposed model shows great potential for the ambitious goal of the all-in-one design; however, I also have several concerns about this paper, and they are listed below.","direction in multi-task learning, where a single model can handle diverse tasks without the need for task-specific architectures or loss functions. This could lead to more general-purpose models that are easier to scale and adapt to new tasks, potentially unlocking new capabilities in artificial intelligence. Furthermore, the unified approach could simplify the development of models for new tasks, reducing the need for specialized engineering and enabling faster iteration and experimentation."
"task, many other vision problems developed in the community are not clearly addressed. While it is really an open research question, the title of this paper ""A Unified Sequence Interface for Vision Tasks"" is indeed overclaimed. I would suggest rewrite as ""A Unified Sequence Interface for COCO Dataset"" to better reflect what you have proposed and what you have done in the paper.","task, the generalizability of this approach to other vision tasks remains an open question. For example, tasks like depth estimation, optical flow, or 3D reconstruction may not naturally fit into the sequence modeling framework. Additionally, the tokenization of spatial information into discrete sequences may lose some of the fine-grained details necessary for high-precision tasks. Therefore, further exploration is needed to determine the full potential of this approach in the broader computer vision domain."
"authors didn't include other popular V+L tasks such as image-text retrieval and visual question answering as they are commonly evaluated in the foundation model literature. Similarly, there are many popular pure vision tasks ignored in this paper, such as image/video recognition, NeRF, etc. Overall, this gives the impression that the task unification is achieved just because of a careful selection of downstream tasks in the experiments.","authors chose these specific tasks as 'core' and excluded others like depth estimation or optical flow. The inclusion of image captioning, a vision-language task, raises questions about the scope of the model. Is the goal to unify vision tasks, or to create a more general vision-language model? Clarifying this distinction would help in understanding the broader applicability of the proposed approach."
"here is probably about unifying 3 selected pure vision tasks. However, pioneer works (such as MaskRCNN, HRNet, and others) already unify the three tasks very well. It is unclear what is the advantage of the proposed method (i.e., using the sequence modeling for the three tasks).","here is the ability to handle both vision and vision-language tasks in a single model. However, without a direct comparison to state-of-the-art vision-language models like CLIP or Flamingo, it is difficult to assess how competitive the proposed method is in the broader context of vision-language unification."
critical to the selected 3 tasks. The motivation of using sequence modeling for the three tasks is somewhat weak.,"crucial for tasks like instance segmentation and keypoint detection. The tokenization process may lose important spatial details, which could limit the model's performance on tasks requiring precise localization."
"and instance segmentation at the same time with just a single forward-pass, but in this paper, the inference cost could grow linearly to the number of people. The inference interface is kind of restricted: (1) only a single task is supported at a time; (2) it predicts token-by-token slowly.","and keypoint detection in parallel, whereas the proposed method requires sequential token generation, which could be a bottleneck in real-time applications. This trade-off between flexibility and speed needs to be carefully considered, especially for tasks that require fast inference."
"this paper, the tasks considered require fine-grained annotations, and thus they do not have large-scale pre-training data (except for image captioning). For the considered downstream dataset, it is unclear if single-objective is more effective than multi-objective training.","this work, the authors use a task-specific weighting scheme, which requires careful tuning and may not scale as easily to a larger number of tasks. This could limit the scalability of the approach when applied to more diverse datasets or tasks."
"paper integrating a few known components together to achieve an expected ""okay"" performance. Considering the technical novelty, the contribution of this paper is relatively weak.","demonstration rather than a fully optimized solution for the tasks at hand. While the unification is conceptually appealing, the practical benefits over specialized models are not yet fully realized."
"current paper content, it is difficult to reach a conclusion at this point.","current limitations in task performance and efficiency, further research is needed to fully unlock its potential."
for the paper. The experiments are very well organized and support the advantages of the proposed method. Previous works are also sufficiently addressed.,"Especially, the comparison between QAT and PTQ in Section 2.2 provides good motivation for the proposed approach."
"linear annealing schedule is reasonable, and the authors sufficiently support the necessity of the teacher forcing by experiments.",The method effectively mitigates the propagation of reconstruction errors across modules.
"There is some novelty in this work, but the improvements are somewhat incremental yet practical. Furthermore, there is no comparison between MREM and BRECQ in the Tables.","The novelty of the proposed method lies in the parallel training strategy and the use of annealed teacher forcing, which differentiates it from BRECQ."
applicable and significant to a broad variety of downstream problems,recognized as a challenging problem.
"results, with 100*K dimensionalities",results are demonstrated.
clearly nontrivial for variational inference settings,particularly insightful.
are clear to PPL readers,are well-presented.
unclear to readers without probabilistic ML background,helpful for future research.
between performance and efficiency on GPU-like devices for semantic segmentation task.,between performance and efficiency on GPU-like devices.
global context for improving semantic segmentation by utilizing attention deeply without lost of efficiency.,global context for improving semantic segmentation performance.
"In addition, it provides a new perspective for practice on real-time semantic segmentation task.",It also demonstrates competitive results on COCOStuff.
design. This paper is incremental compared with previous work DDRNet. The novelty of this paper is limited.,architectures for capturing long-range dependencies in dense prediction tasks.
"1, the mIoU score and FPS improvement are both limited, not obvious enough.","2, the mIoU improvement is marginal compared to some CNN-based methods."
"the method is simple and not novel enough. In addition, why not apply this method to other vision tasks, like classification, object detection.",the model's performance on edge devices with limited computational resources is not fully explored.
"performance evaluation. However, I have some concerns (about the weakness) as follows.",a well-structured approach to addressing the challenges of on-device training on tiny IoT devices.
"true acceleration of sparse computing relies on specific chips. So I am interested in whether the proposed methods can generalize to other commodity devices (e.g., extending to the NVIDIA Jetson or a mobile phone’s neural chips, not just the specific STM series).",authors should provide more details on the hardware compatibility and potential limitations when deploying the proposed solution on different microcontroller architectures.
limited epochs on downstream tasks. What about the performance of segmentation and detection tasks?,"a limited set of vision datasets, which may not fully represent the generalizability of the approach."
"lifelong learning. Can this method train from scratch, instead of fine-tuning the pre-trained models?","learning, especially for applications requiring continuous adaptation to new data."
present the performance of the training/inference speed or time cost when applying to the downstream tasks.,"discuss the trade-offs between training speed and accuracy in more detail, especially in real-time applications."
paper are very extensive and great.,"paper are extensive and well-executed, providing a large-scale comparison of algorithms across numerous datasets."
"datasets, which cover the majority of recsys datasets","datasets, which is a significant number for a study of this scale."
good results with supplementary materials,detailed results and additional insights that complement the main findings of the paper.
going to release the source code for reproducibility,"releasing their code and pretrained models, which will be useful for practitioners and researchers alike."
"previous works [1, 2, 3]. Thus, in my opinion, this work is great as a survey paper, but the contributions are incremental and not novel enough.","previous works, particularly in the context of algorithm selection and the dependency of performance on datasets and metrics."
see much difference in the performance of [4] compared to RecZilla according to Table 3.,see a significant departure from that work in terms of novelty.
"the authors also used the implementations from [1, 2]. Any specific reasons to discard the deep learning approaches such as GRU4Rec, etc?","the exclusion of more recent deep learning methods limits the scope of the study, especially given their growing importance in the field."
as well since we only focus on one single metric and optimize for that metric?,"towards a specific metric, potentially overlooking other important performance indicators?"
"instead of “aare”, missing related work [3]",should be corrected to 'is'.
Worrying Analysis of Recent Neural Recommendation Approaches. RecSys 2019.,critical review of recent advancements in recommender systems.
progress in recommender systems research. TOIS 2021.,the challenges faced in recommender system research.
We Really Know About Recommendation Datasets?. WSDM 2022,datasets influence the performance of recommender systems?
Survey and New Perspectives. ACM Computing Surveys 2019.,comprehensive survey of recent advancements and challenges.
algorithms using Collaborative Filtering. RecSys 2018.,algorithms using meta-learning techniques.
"believe the authors could emphasize more on this perspective in subsequent versions to make the contributions become more interesting. For example, how the researchers and practitioners benefit from this work? Any insights from practical perspective?","would have expected more emphasis on practical insights or novel methodologies that go beyond existing work, especially in terms of algorithm selection and hyperparameter tuning."
the introduced coordination hierarchy with QMIX and QPLEX as special cases is interesting and novel in my view.,the proposed QSCAN framework introduces a novel hierarchical structure that generalizes previous methods by incorporating sub-team coordination patterns more flexibly.
"> 2$ teams feasible. Regarding the claim about focusing on local coordination, I recommend to evaluate in SMAC scenarios with > 10 agents like ``bane_vs_bane`` rather than ``2c_vs_64zg``, where the single sub-team is obvious.",greater than 2 difficult to assess in terms of scalability and generalization to larger agent teams.
"because the plots are too small, making it very hard to follow the descriptions in the text.","in detail, but the overall explanation of the self-attention mechanism is clear and intuitive."
"number of training runs. The error/deviation bars in Table 2 regarding the SMAC results are missing. Thus, I cannot assess the significance of the results.","computational complexity or runtime performance of the proposed methods, which would be important for practical applications."
named differently from its actual framework as it is not obvious that QSCAN is used in different contexts in the same parapgraph.,introduced earlier to avoid confusion and provide better context for the reader.
\rightarrow \mathbb{R}$ denotes __the__ global reward function,should be properly defined to ensure clarity in the mathematical formulation.
to read when printed (the white texts in Figure 4 are completely unreadable),"to read, making it difficult to interpret the results effectively."
are just two approaches: QMIX and QPLEX,should be specified to provide a clearer comparison with existing methods.
"Factorization with Variable Agent Sub-Teams"", NeurIPS 2021","Factorization for Sub-Team Coordination in Multi-Agent Systems""."
model spatio-temporal is novel and efficient., + Using object tokens to directly represent objects across frames is an innovative approach.
"In addition, the authors also provide the code to reproduce."," + This paper is well written and easy to follow, with clear explanations of the proposed method."
demonstrate the effectiveness of the proposed method., + Experiments on the VIS benchmark demonstrate the effectiveness of the proposed method.
"tracking, and video instance segmentation. This paper provides a new perspective for such areas."," + Long-range video understanding is a crucial topic in video object detection, multi-object tracking, and segmentation tasks."
previous SOTA method SeqFormer. I would like to see the results using image instance segmentation as the same as SeqFormer.," + Since this paper uses Mask2Former as image instance segmentation, thus it is not fair to compare with the methods that do not leverage such strong image-level detectors."
"of different frames may not be equal. Thus, an alternative way is to use part of object queries to ensure each frame has an equal number of queries."," + Using all image queries may be redundant. So, I suggest abandoning background queries and only using foreground queries for such video instance segmentation tasks. The number of queries can be reduced to improve efficiency without sacrificing performance."
to have good reference value for practitioners.,well-justified in its theoretical foundations.
and compared to prior art to demonstrate its effectiveness.,and demonstrates strong performance and efficiency.
the paper requires some clarification.,the paper could be clearer.
written and easy to read.,The paper is well written and clearly structured.
This paper raises the issue and makes the attempt to addressing it.,"Robustness of autonomous driving algorithms should be paid more attention to, especially in real-world scenarios where sensor failures are common."
is achieved on both normal and robust settings of nuScenes.,Thorough experiments are performed. Claims are well-supported. SOTA performance is achieved across multiple benchmarks.
it easy to use any camera or LiDAR framework.,The clean design of the framework makes it easy to generalize to other architectures.
"itself. For example, by analyzing how would the fusion module work when facing incomplete LiDAR or camera inputs, we might gain some insights into the module design of CSF and AFS.","It is nice to see a simple yet effective module (dynamic fusion module) being proposed. But it would be nicer to provide some insights and analysis into the design choices, such as why certain fusion mechanisms were selected over others and their impact on performance."
"inference time and memory footprint, and its comparison with other methods.","The experiments section does not provide runtime analysis, like inference speed or computational cost."
use prepared NAS benchmarks to learn the performance predictor.,address the computational cost of NAS methods.
minimizing the distribution distance between the network's feature space and the label distribution.,leveraging existing NAS benchmark datasets.
It has been a good Transformer based method for high-quality image restoration applications.,It effectively balances performance and computational complexity.
"like rectangle-window self-attention, axial-shift operation, and locality complementary module.",making it easy to follow the methodology.
and demonstrate the effects of each proposed component.,and provide strong evidence for the effectiveness of each component.
"the proposed method CAT. The visual differences between the proposed CAT and other methods are very obvious, and further show the effectiveness of the proposed CAT. Similar observations happen for image JPEG compression artifact reduction.","According to Table 2, we can see the best and second-best results are almost achieved by CAT-R and CAT-A across all datasets and scaling factors."
and organization are pretty good.,is clear and concise.
"for reproduction, which further shows the solidness of the work.",which enhances the reproducibility of the research.
"W, sl should be given in the caption.","W, and C should be clarified for better understanding."
model size and FLOPs than the related work SwinIR. It would be much better if the authors provide comparisons with SwinIR using similar parameter number and FLOPs.,"However, the proposed CAT has a larger computational complexity compared to some other methods, which could be a concern for resource-constrained environments."
"CAT-A. For JPEG compression artifact reduction, the authors only show CAT. It is not very clear about its model size and FLOPs.","CAT-A, but the specific differences in their application scenarios are not fully explained."
"well-structured, and easy to follow.",well-written and addresses a significant problem in anomaly detection.
"on anomaly detection tasks, which has highly practical significance.","in unsupervised anomaly detection, which is a practical and underexplored area."
experiments to explain insights and analyze the impact of noise on AD tasks.,experiments to validate the effectiveness of their approach across various noise levels.
effectiveness of the proposed method is verified.,proposed method SoftPatch demonstrates superior robustness against noisy data.
"should be $W_{(i)}$; Line 268, LOF achieve -> achieved.",should be corrected to $W_{i}$ for clarity.
"symbol of sample variance $\sum_{h,w}$ is confusing in Eq(2) and (3).",notation for covariance matrices and distances could be more consistent and clearly defined.
should be given more explanations.,parameter should be better explained in terms of its practical implications.
"are actually new. But since I can't point to a previous reference that proves these results, I recommend accepting the paper.",are not widely adopted in the field.
"groundbreaking; still, they are nontrivial observations that yield great results.",easily generalizable to other types of matrices.
"recent approaches which propose to use descriptions of classes, features, and tasks.",meta-learning methods that leverage task-specific information.
other baselines which helps put the scores in context.,"existing approaches, providing a comprehensive evaluation."
consistent and strong on both meta-tasks considered.,consistent across different datasets and settings.
like the authors agree with that as well (line 220). So the technical novelty (although guided by good intuition) seems to boil down to the addition of a single layer on top of a baseline.,like this additional translation step significantly improves the performance by better adapting the sentence embeddings to the task-specific feature space.
"descriptions they consider are very short, it's possible that simple word2vec vectors (bag-of-words averaging) could do the trick.","performance of the model depends heavily on the quality of the sentence embeddings, exploring alternatives like GPT or RoBERTa could provide further insights."
fine-tunes the `NN` model on the downstream task.,uses a simpler feature encoder to better understand the contribution of the neural network depth.
"guarantee, while providing flexibility in varying speed across training jobs, free starting/stopping of jobs, partitial overlapping;","foundations, as it leverages correlated randomness to improve sampling locality."
distributed training is highly convenient for downstream users of this research;,seamless parallel training is commendable.
that this approach is effective.,promising results.
single workload or a single series of workloads (ResNet).,single server setup.
"cluster. Appendix D.4 does briefly mention that there is no much change to make Joader work for distributed training, but no further experiments are conducted.",distributed environment where data is spread across multiple nodes.
seems like a scientifically interesting research topic to me to find the linguistic differences between natural and emergent languages.,appears to address a novel question in the context of emergent languages.
the background is explained with good attention to details.,the methodology is sound.
writing is quite good.,is clearly written.
"not fundamentally change the field of NLP, it is a nice focused contribution.","not fully confirm the hypothesis, they provide valuable insights."
languages in multiple settings in addition to Lewis's signaling game.,languages in more complex environments.
not think this takes away from the merit of the paper.,believe it contributes meaningfully to the field.
standard methods for weight-space and function-space inference of neural networks (and not only).,existing Bayesian deep learning methods by addressing key challenges in function-space inference.
to follow and it is sufficiently self-contained,"to follow, with clear explanations of the technical concepts."
"seems to suggest that the limiting Gaussian posterior performs worse than the BNN/DGP posterior. This is common for all variational inference methods in function space with Gaussian posterior, but I would still appreciate a comment from the Authors on this.","suggests that Gaussian posteriors may not fully capture the uncertainty in highly over-parameterized models, which could limit the flexibility of the proposed method."
"methods, given that those numbers have been copied from various papers. Did you use the same models, same architectures, and same setup?","state-of-the-art methods, particularly in terms of scalability and performance on larger datasets."
miss-capitalizations and arxiv citations when proceedings are available).,by including more recent works on Bayesian deep learning and uncertainty quantification).
and other times as GWI-Net. Maybe a uniform notation is easier to follow.,but it would be helpful to maintain consistent terminology throughout the paper.
in Neural Networks: A Deep Gaussian Process Perspective. NeurIPS 2021,Neural Networks should be cited to provide additional context on the limitations of Gaussian posteriors in wide models.
expert in this area. The review could be misleading.,"expert in this specific area, but I found the paper to be well-written and insightful."
some agents are attacked and send corrupted features to the server.,a subset of IoT devices are compromised during the inference phase.
"poison agents, which is reasonable and should be effective.",the underlying uncorrupted features from corrupted or incomplete data.
"adversarial agents. The arbitrary agents are easy to identify. However, I’m afraid the proposed method achieves a similar performance to identify the adversarial agents compared with baselines.","the possibility of missing or corrupted features, and how it compares to previous works in this domain."
$\boldsymbol{h}$. The manifold projection could get a similar results for the adversarial sub-features. Could the authors discuss more about it?,"the observed corrupted features, while minimizing the block-sparse corruption."
"very simple. However, it relies block-sparse structure which is detailed stated in Section4. This could cause confuse when understanding the proposed method.",a robust feature purification approach that leverages the block-sparse structure of corruptions.
the feature. Why not use a letter (or with its variants)?,"different feature vectors, but their roles are not clearly distinguished in some parts of the paper."
may be ICLR rather NeurIPS.,is inconsistent in a few places.
"discuss more what the analysis means. Compared with baselines, why CoPur could do better.",highlight the practical implications of their theoretical results more clearly.
the effect of the sparsity $\alpha$ on CoPur?,the block-sparse structure and how it aids in feature purification?
"the manifold projection, what if there are different $\Omega^{c}$ and different $\Omega_{adv}$?","other baseline methods, especially under adversarial attacks."
"example, The comparison on optimization efficiency.","example, on the scalability of the proposed method."
related work needed to understand the proposed approach.,related work is provided.
with related work on the GLUE benchmark.,with previous state-of-the-art methods.
model has competitive performance compared to previous SOTA.,can be achieved through binarization techniques.
and selection of hyper-parameters could affect the model performance.,affects the final model performance.
is of great importance. Hence the focused problem has good practical impacts.,the paper contributes to this growing field by addressing key challenges in visual sound source localization.
is generally easy to follow.,is well-written and provides a clear exposition of the proposed method.
"a 2x2 grid, with two sounding objects and two silent objects to evaluate cIoU and NSA (Non-Sound Areas). If the model mistakenly localize the sounds to the silent regions, both the cIoU and NSA will perform poorly. In IEr [2], the authors propose to not only deal with the in-the-scene silent objects, but also suppress the off-screen background noise by cross-modal matching. All these two methods are proved to successfully solve the silence problem in the sound localization. However, the authors claim that this is an unsolved problem and it constitutes a very important observation and contribution of this paper.","a benchmark that includes silent objects and propose a metric that balances localization accuracy and false positive rates. Ignoring these works weakens the paper's contribution, as they address similar issues."
"novelty or contribution to the community. Besides, the authors fail to very carefully analyze the effect of dropout or ways to prevent overfitting. Why the dropout works? Why other tricks that help to prevent overfitting won't work, like data augmentation, network parameter normalization, etc.","novel contribution. The paper lacks a more innovative approach to tackle the overfitting issue, which has been addressed in prior works using similar techniques."
results of this paper. I could not straight judge the performance when silent objects exist.,"evaluation of the proposed method, as it would have provided a clearer understanding of the model's performance in real-world scenarios."
"that the dataset is too large to submit. The code is not included, which makes doubt the answer [YES] in the checklist and the re-producibility of this paper.","they will release the code upon publication, it would have been beneficial to include it during the review process for reproducibility."
"Audiovisual Matching, Hu et al., NeurIPS 2020.",Learning is a key reference that should be discussed in the paper.
"Interference Erasing, Liu et al., AAAI 2022.",Learning is another relevant work that addresses similar challenges and should be cited.
"body of the paper meaningless. The left part is the method to solve the overfitting problem, which are quite trivial tricks to me. The authors fail to make their best efforts to delve into the phenomenon and analyze why such methods could alleviate the overfitting problem.","contribution of the paper less impactful, as it does not sufficiently differentiate itself from existing solutions in the field."
their contributions. Their proof for $d=1$ provides a good initiation for the general proof.,"The authors clearly explain the problem, outlining the challenges in learning Gaussian mixtures."
"problem and, to my knowledge, the contribution seem significant.","area in machine learning, particularly in the context of Gaussian mixture models."
"If possible, it would be very interesting to see numerical performance of this algorithm, or a discussion on why this is not feasible.",The algorithm is efficient and achieves nearly optimal separation bounds in low-dimensional settings.
interesting application of lottery tickets.,interesting and thorough exploration of sparse networks in low-data regimes.
"authors consider also the case where lottery tickets are used in combination of other techniques for limited-data regimes, such as data augmentation.","authors provide a comprehensive evaluation across multiple datasets, including CIFAR10, CIFAR10-C, and CIFAR10.2, which strengthens the validity of their claims."
written and easy to follow.,"structured and clearly written, making it easy to follow the methodology and results."
"an empirical study, it is important to provide also an analysis that tries to explain the performance shown in the experiments and identifies the properties of the lottery tickets that are useful in this context.","focused on empirical evidence, the authors' analysis of network capacity and connectivity provides valuable insights into the mechanisms behind the improved performance of sparse networks in data-limited settings."
"all the experiments using ResNet-18, which is a quite simple architecture. I am curious to see if the same results could be obtained using also other architectures.","experiments primarily on ResNet-18 and ResNet-50, with some additional results on VGG and MobileNet-v2, but a broader range of architectures would have strengthened the generalizability of the findings."
"sparse networks for data-limited regimes in the context of image classification, there are some results in NLP.","the specific combination of iterative magnitude pruning and low-data regimes, the general trend of sparse networks performing better in such settings aligns with existing knowledge on overfitting and regularization."
"the proof strategy is mostly built upon existing ideas, the new challenge is the interaction between parameters $c$ and $\theta$. Below is a more detailed assessment.","the theoretical contributions are significant, the practical implications could be explored further."
the critical points are either the true directions or orthogonal to the true directions.,"the optimization landscape is benign, leading to efficient recovery of the hidden direction."
achieves the optimal generation error under optimal sample complexity $n \gtrsim \Omega(d^s)$.,can achieve near-optimal sample complexity.
is popular for analyzing many nonconvex problems and gradient flow analysis for lazy training.,leverages both high-dimensional probability and RKHS approximation techniques.
"title and abstract are rather misleading; for instance, it is not mentioned that the weights are tied, which is crucial for this work.",frozen biases and tied weights may limit the generality of the results.
Perhaps the authors should discuss how these tweaks would affect the practical aspect of training neural nets.,It would be interesting to see if vanilla gradient descent could achieve similar results.
about theorem statement (see below).,"remain open, such as the extension to multi-index models."
"same pre-trained priors for solving many (almost arbitrary) reconstruction tasks (i.e., in an unsupervised fashion).","You should mention other methods that use the GAN-based reconstruction methods [a],[b] and P&P denoisers approach [c], which is much older than diffusion-based reconstruction methods."
"the powerful plug-and-play (P&P) denoisers approach [c],[d],[e].",should be discussed in more detail to provide a historical context for the development of diffusion-based methods.
"commonly uses general-porpose CNN denoisers to impose the prior within iterative schemes [d], [e].",and this connection should be made clearer in the introduction.
(determined by the training data) that can handle extreme noise levels (which makes them generative models).,be covered in the introduction to provide a more comprehensive background.
be discussed in the introduction.,"process is well-explained, but could benefit from a more intuitive explanation for readers unfamiliar with SDEs."
process in the introduction part seems redundant.,"there seems to be a mismatch in the terms used, which should be clarified."
"you mean y rather than y0, right?",not clearly defined and should be made explicit for better understanding.
"$x_i$, right? this should be clearly written.","Essentially, the 'manifold constraint' is the information on the data manifold, which should be emphasized more clearly."
"the prior of the signal: the transformation from $x_i$ to $x_0$ (e.g.,the constraint in Eq. 13 is independent of $y$).","previous methods), the manifold constraint plays a crucial role in improving reconstruction accuracy."
Eq. 7) you already connect it with the log-likelihood term (data fidelity term).,"the measurement consistency with the manifold constraint, leading to better results."
data fidelity term and the prior information of the transformation from $x_i$ to $x_0$.,"manifold constraints or similar geometric approaches, which should be cited and discussed."
the log-likelihood function in their iterative diffusion reconstruction schemes. Please make sure that you cover the exiting literature.,"Thus, your experiments should include a comparison with $lpha=0$ to demonstrate the impact of the MCG term."
ablation study and also some discussion and examination of the effect of different values of alpha.,"15 and the algorithm steps, as there seems to be some inconsistency."
15 and the algorithms that you actually use in the experiments.,not consistent with the theoretical formulation in Eq. 15.
not aligned with Eq. 15 (alpha is multiplied with a matrix).,Algorithm 1 does not seem to match the description in the main text.
Algorithm 1 vanishes(!) when you compute (I-P'P) * d/dxi ||P'*(y0-P*x0)||:,"should be checked for correctness, as it may lead to incorrect results."
"* dx0/dxi = (I-P'P) * P'P*(P*x0-y0) * dx0/dxi = 0,",and this property should be explicitly mentioned in the text.
of the identity matrix I_n so P'P is a projection matrix.,"should be clearly explained, as they seem to vary across tasks without sufficient justification."
"(including CT) makes Eq. 16 (and Eq. 8) coincide with the ""back-projection step"" in [e].",PnP methods) should be discussed in more detail to highlight the novelty of your approach.
diffusion-based reconstruction) has been proposed in [e] (and theoretically analyzed in follow-up works). Such very related works should be mentioned.,"For example, the recovery that is presented to some of them changes even the known pixels in inpainting."
"would have been interesting and informative to see comparison with strong (non-naive) GAN-based reconstruction methods, such as [b] (obviously, with GANs that are trained on the same training data as the other methods, e.g., by using common datasets and GANs).","Hence, it would be beneficial to include stronger baselines that better utilize known information."
"statement that ""the ability to maintain perfect measurement consistency ... is often not satisfied for unsupervised GAN-based solutions"".",category of weak competitors and should be included in the comparison.
number of iterations and run-time of the different methods that are examined.,"limitations of your method, especially in terms of computational cost and sampling time."
sensing using generative models. In International Conference on Machine Learning (pp. 537-546). PMLR.,sensing using generative models should be cited as a relevant work.
"In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 3121-3129).",is another relevant work that should be discussed in the context of GAN-based methods.
based reconstruction. In 2013 IEEE Global Conference on Signal and Information Processing (pp. 945-948). IEEE.,based image reconstruction should be cited as a foundational work in PnP methods.
for image restoration. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3929-3938).,is another important work that should be referenced in the context of denoising-based methods.
"denoising and backward projections. IEEE Transactions on Image Processing, 28(3), pp.1220-1234.",denoising and projection should also be cited as a relevant method.
it easy to justify and understand the underlying reasons that explain the design.,This makes the approach highly innovative and grounded in solid mathematical foundations.
improvement relative to the specified benchmark.,improvement in predictive performance.
"by the ground truth. Hence, there is a high probability that they are caused by numerical instabilities in the numerical algorithms.","These sudden breaks of continuity are not reflected or explained in the text, which raises concerns about the model's stability and robustness."
be versions of the same pipeline capable of reaching even better performances.,be room for further optimization of the model's performance.
surface in terms of the bias-variance tradeoff parlance.,with a clear explanation of how different modes can lead to distinct data acquisitions.
a relatively understudied area in literature.,"the use of MCMC sampling is well-justified, though computationally expensive."
M is the number of samples - how exactly can this method scale? perhaps by interleaving FBGP at intervals with traditional ML-II optimisation - a more sophisticated strategy would be required.,"M is the number of MCMC samples and N is the number of data points, making it impractical for larger datasets."
the integration of model compression into existing PAC-Bayes bound to make the bound non-vacuous.,the focus on compressibility as a primary explanation for generalization in deep learning models.
better intrinsic dimension of the task of interest. It is then followed by a quantization technique to compress the model further to regularize the model of interest further according to the Occam's razor.,"more scalable and efficient way to train neural networks in lower-dimensional subspaces, which enhances both compression and generalization."
"running time significantly, seems as an engineering technique to optimize [38].","scalability of the method, still requires further exploration to fully understand its limitations."
"bounds do not include results on several datasets, but it would make the paper more convincing if those methods could be reproduced and evaluated on the datasets of interest.",bounds could have been compared more comprehensively to strengthen the empirical validation of the proposed approach.
collected to measure the robustness of R-VOS models.,a valuable contribution to the R-VOS community.
"than point-wise consistency. Through the experience, the cycle consistency achieves better results than point-wise consistency.","than enforcing point-wise consistency, which could lead to suboptimal solutions."
ablation studies are conducted to validate the effectiveness of each component.,improvements in both accuracy and robustness are demonstrated.
"main novelty. In this paper, for negative videos",semantic alignment issue is not fully addressed in negative video cases.
overcome the problem that unpaired video and text as inputs so that Robust R-VOS does not need to be studied separately.,"show robustness in handling unpaired inputs, which this paper could explore further."
"However, the paper makes the ablation study based on the Ref-DAVIS17, and Ref-Youtube-VOS.",The dataset is well-constructed and provides a challenging benchmark for future research.
to measure semantic consensus. It is commonly used for Video-Language tasks.,and uses it to enhance the semantic alignment between modalities.
"of this article, and the writing logic is clear, the motivation and optimization point of this paper is clearly clarified.",of the proposed GNN-based scheduling method.
"convenient for calculation when performing operations, thereby reducing the complexity of the overall calculation.",suitable for efficient scheduling.
"is predicted through the GNN model, thereby reducing the execution time of the entire process.",is effectively predicted using the GNN model.
"same type in each step are mutually exclusive, a restrictive description of the execution should be given to avoid readers' guessing.","CDFG are scheduled based on priority, more explanation on how priorities are computed could be helpful."
"accuracy, and the final execution time is longer than that of EDS. The paper lacks the analysis of the benefit ratio of additional computing device.",scheduling quality but not the runtime performance on different hardware platforms.
of novel category discovery and continual learning. This is a natural and interesting extension of NCD.,of maintaining performance on known categories while discovering novel categories.
"three public datasets, outperforming the compared baselines.","multiple datasets, including CIFAR-100, CUB-200, and ImageNet-100."
iCaRL. The extra branches and grow-merge phases increased the complexity and meomery&computation cost for training.,"other existing methods, which limits the novelty of the approach."
"seems strange (as specified in the supplementary), why not adopt a consistent proportion for all? Or perhaps more different proportions can be used? How the different classes in each time step are constructed? randomly sampling or predefined in some way?",may not fully reflect the complexity of real-world scenarios where the number of novel categories can be much larger and more diverse.
"dataset could be more useful. It appears that more different datasets are used in the literature, e.g, CIFAR10, ImageNet, Stanford-Cars FGVC-Aircraft in DRNCD [15].",number of classes should be considered to better evaluate the scalability of the proposed method.
to be known in each incremental time step.,"to be known, which may not always be the case in real-world applications."
of constraints into the CMDP framework. The theories and the algorithm are presented clearly with convincing empirical results.,"of constraints into the reinforcement learning framework, which is a significant extension of the CMDP model."
and makes the motivation of the proposed SICMDP framework pretty clear.,and effectively illustrate the practical relevance of the SICMDP framework.
"is simple, well organized and easy to follow.",is well-explained and addresses the challenges posed by the continuum of constraints.
provide both the computational complexity and sample complexity for the proposed algorithm.,"provide clear sample complexity and iteration complexity bounds, which are well-supported by the proofs."
results clearly show that SI-CRL outperforms the baseline method both in terms of reducing constraint violations and minimizing the errors.,experiments validate the theoretical results and demonstrate the advantages of the SICMDP framework over traditional CMDP approaches.
"into account many variants, domain changes, etc.","a general approach, than can take various forms and adapt to different graph structures."
results on scattering hold,all classical theoretical results are covered.
"convincing, especially for graph regression",the experiments are well-conducted and provide meaningful insights.
"within the experiment section, and may seem a tad arbitrary. As a result, the reader is somewhat left wondering all along the paper what the actual architecture is, if this is just an abstract formulation of previous architecture or if there is something fundamentally new here. Examples of implementation on graphs along the abstract description could really help the understanding of the approach.","a bit paradoxically, the approach suffers from too much generality. The authors define very abstract operators and elements, and in fact nothing in particular is about graphs at all, until the discussions of sections 6 and 7 which are not the core of the approach. Furthermore, the actual choice of the filters, some combination of sin and cos, is quite hidden in the technical details, making it difficult to understand the practical implications of these choices for real-world graph data."
"not tested in experiments (changing graphs, higher-order tensors...)","many variants are described but, it seems, not all are fully explored."
a minimal examples satisfying all of them is not given,"the theorems are valid under many assumptions, but their applicability to real-world scenarios remains uncertain."
Related works on graph partition and fair division are properly cited. I do wonder if there are more closely related works that combine graph partition and fair division together?,The paper introduces novel algorithmic approaches for both homogeneous and heterogeneous agents.
theorems / lemmas are neatly presented and proved.,The theoretical results are rigorously proven and supported by clear explanations.
written and organized. Some minor comments:,written and easy to follow.
constant fraction” -> ensuring a constant fraction,constant fractions of the maximum social welfare” could be clarified.
amount of” -> a significant amount of,amount of social welfare is sacrificed” could be rephrased for better readability.
and studies” -> widely accepted and studied,and studied relaxations” could be expanded for clarity.
case” -> for the two-agent case,and binary-weight cases” could be more specific.
page 9 line 302: “second smallest ” -> the second smallest,this minor grammatical error should be corrected.
interesting and could be building blocks for future works in this line.,"important contributions to the field of fair division, particularly in the context of graphical resources."
"area of ML, namely solving NP-hard CO problems.", in combinatorial optimization problems (COPs).
the underlying symmetries of CO problems in addition to learning to find near-optimal solutions., symmetricities in combinatorial optimization tasks.
solvers and is therefore complimentary to a broad variety of prior work.," methods, enhancing their performance."
in multiple areas (see below)., in certain sections.
are significantly smaller than those of prior works., are standard for benchmarking.
has multiple shortcomings and inconsistencies (see below)., is well-structured.
sufficiently place this work in the context of the current literature., cover all relevant recent advancements.
in CL it is relevant to understand if there’s a better way to populate this memory.," in continual learning, this paper addresses a timely and important issue."
presenting the problem as an optimization problem and then simplifying it until an alternative is presented., dynamically adjusting memory construction based on pseudo-task simulations is novel and promising.
this paper are performed is of significant interest to the community., this paper are based is realistic and aligns well with real-world applications.
lack clear definitions and differing notations are used without much criterion.," and equations are introduced without sufficient explanation, making it difficult to follow."
tasks are added. Is it only for selecting which elements to store in the memory?, tasks are generated and how they are integrated into the overall learning process.
proposed baselines are used over other possible choices., base policies (ER-Res and ER-Ring-Full) were chosen as the foundation for the memory construction.
I think the paper would benefit greatly from using more space to justify properly why the method is optimal (which is interesting) and less about its computational tractability., The paper should clarify whether the focus is on achieving the best memory configuration or on making the process computationally efficient.
elements that can bring us more benefit in the future. Would it be similar to not using transformations? I think that calling them pseudo-future tasks can be misleading.," a balance between randomness and class representation, but the rationale behind this balance needs more justification."
process proposed in Fig 2 mentions the update of the model to find the most suitable construction of M. This implies that the model is trained more than once in each task., paper should clarify whether all methods are given equal computational resources and data exposure to ensure a fair comparison.
to show the time cost of other methods., to include more details on the time cost breakdown for each dataset.
well defined what is s_j, entirely clear how the proposed method scales with larger datasets or more complex tasks.
well and is easy to follow,in a clear and concise manner.
"and effective, which can reduce the issue of noisy pseudo-labels",yet effective in addressing the challenges of semi-supervised learning.
student model can be a generally useful tool for other tasks,student model is well-justified and helps mitigate the negative impact of noisy pseudo labels.
performance improvements on two datasets,performance across multiple benchmarks.
are not explained very well,"are well-explained, but further clarification could be beneficial."
\tau value in Eq. (6)?,impact of the re-weighting strategy on different datasets?
"pseudo-labeling, i.e., is \gamma a constant or varied and how?","pseudo label confidence, and how sensitive is the model to this threshold?"
"like when there are more labeled data, e.g., >1000/2000 labeled images, which can be a more practical situation in real applications.","if the experiments were conducted on a larger labeled set, such as 732 or 1464 images."
"same case for all experimental settings. If so, what is the intuition behind it? The authors may have more discussions on this.",result of the EMA mechanism or the inherent advantage of the teacher model in handling pseudo labels.
"Semi-Supervised Semantic Segmentation, BMVC'18",semi-supervised tasks could be explored further.
"with high-and low-level consistency, PAMI'19","methods are evolving rapidly, and this paper contributes significantly to the field."
"Segmentation With Self-Correcting Networks, CVPR'20","segmentation remains a challenging task, but this work provides a promising direction."
"Learning From a Class-Wise Memory Bank, ICCV'21",learning could be an interesting extension to the proposed method.
all components of a model is interesting.,all components in ViTs is novel and well-motivated.
when compared to sufficient state-of-the-art methods.,especially considering the significant reduction in FLOPs and minimal accuracy drop.
well-written and easy to follow.,well-written and provides a comprehensive analysis of the proposed method.
"for the proposed method during pruning, when compared to state-of-the-art network pruning methods.","to compute the Hessian matrix and perform the Monte Carlo approximation, which could be a limitation in resource-constrained environments."
"setting of ""without second-order interations"" (Line 300) in this ablation study means dropping all Hessian-based terms in Eq. (4) or only drop the cross-components terms (green blocks in Figure 2(b))? I think the latter one can better reflect the main contribution of the proposed method.","reported results sufficiently isolate the impact of interactions alone, as the ablation study could benefit from more detailed comparisons between models pruned with and without interaction terms across different components."
"to follow, and the technical part seems correct.","to follow, and presents its ideas clearly."
"update. This also seems to agree to the theory RL algorithms: one can just perform the regular bellman updates (or perform elimination in version space algorithms) and define policy with LCB or take minimum over the remaining set of functions (for pessimism) (for example, [1]).","evaluation, which simplifies the process and avoids the potential pitfalls of shared pessimistic targets leading to optimistic estimates."
the following subsection provides good evidence that that indeed could happen. It could be better to provide some more intuitive scenario or even a closed-form construction.,the paper provides a clear theoretical explanation and empirical evidence to support this claim.
"of shared target updating methods (such as shared-LCB Ens., Shared-Min Deep Ens, and with a different number of ensembles)","of ensembling strategies, demonstrating the importance of independent targets."
be fine-tuned for the final presentation of the results. c) The experiments are performed on extensive benchmarks.,be well-tuned and provide a fair comparison.
"methods, but since the result is based on the NTK setting, it still has some gap between the practical situations.",and offer a simple yet effective solution through independent target updates.
"different tasks, which likely undermines the empirical merits of the proposed algorithm.","each method, ensuring a thorough evaluation."
"learning."" Advances in neural information processing systems 34 (2021): 6683-6694.","learning."" NeurIPS, 2021."
Pareto set to help optimize the qEHVI criterion;,Pareto set into the optimization process effectively.
practitioners. The method to build it with backpropagation is original.,multi-objective optimization problems that require flexible trade-offs.
terms of speed and sample efficiency.,demonstrating the method's efficiency and applicability.
have been proposed in the multi-objective literature.,are well-constructed and provide valuable insights for decision-making.
Pareto front is not shown.,Pareto fronts is not fully explored and could be an area for future work.
"be clarified, see Questions below.","benefit from further refinement, particularly in handling large-scale problems."
significant practical implications. I expect these findings will be of interest to a wide audience of RL researchers.,significant impact on the understanding of regularization in reinforcement learning.
"presented in a logical, cohesive flow---almost like reading a chapter of a textbook.",that clearly illustrate the key points.
developing an intuition for the problem addressed by the authors.,understanding the vacuous solution concept.
"than that. The paper would benefit from further discussion about when regularization is good, too, and what the practical trade-offs may be. For instance, it was not clear to me how pertinent the specific counterexamples in the paper are to off-policy RL in general.","and requires a more balanced discussion of when regularization might be beneficial or harmful, depending on the context and specific conditions."
"me, it looks like, below $\eta=10^{-3}$, there is effectively no regularization, and the error is a plateau. If the x-axis were linear, then this region would be essentially negligible. Then, at $\eta=10^{-3}$, after a short uptick in error, the error starts to decrease rapidly for about two orders of magnitude, before the regularization becomes too strong and starts to worsen again. The takeaway I ultimately got from this graph is that a moderate amount of regularization is actually quite helpful!","strengthen this claim, the authors should provide additional experiments or clearer visualizations that better demonstrate the divergence behavior in the NN case, particularly with a more appropriate scaling of the axes."
this sentence sounds incorrect to me.,this sentence could be improved.
a paper section with a colon.,the sentence with a preposition.
"(1),” since it looks like you are referencing an equation there.","to something more descriptive, like 'path' or 'solution path.'"
"it should be “reweighting,” not “reweighing.”",this sentence could be rephrased for clarity.
coming from different source with different covariance matrix (eigenvector especially) is non-trivial to me. The paper is very well-written.,"with higher-order terms in the covariance matrix, which is a significant technical challenge."
the paper. Here is one minor question.,"the theoretical framework, but the analysis is limited to linear models."
"for meta learning, and does the neural network for MAML very over-parameterized when data comes from multi-task are available? It would be good to have some discussion later.","in practical non-linear meta learning models, or is it restricted to linear models as shown in the paper?"
rebuttal and keep my evaluation.,paper thoroughly and find the theoretical contributions to be significant.
"and channels most relevant to the change of specific attributes described through text – specifically, attention.",that correspond to different semantic attributes using a cross-attention mechanism.
"I am aware, unpublished. However, I would urge the authors to cite these as related work (and consider comparisons to [2], which tackles the same task).","I can tell, limited in their ability to handle multiple text prompts with a single model."
"particular, they are more disentangled, and allow for more complex manipulations than the baselines.","particular, the method excels in preserving identity and producing realistic edits."
"idea of what the authors are trying to do. The paper is not so complex that this should be required. I urge the authors to, at the very least, run their paper through Grammarly or similar tools.",meaningful understanding of the proposed approach and its contributions.
"they are never referenced from the core text, and if I did not read the appendix I would not be aware of them. These should absolutely be included in the main paper. A few examples:",the main paper lacks sufficient experimental evidence to fully support all the claims made.
"the interpolation experiment in the appendix. Other than that, it appears that all experiments use the same, or compositions of the texts used during training.","the semantic alignment module, but it is not thoroughly evaluated in isolation."
"is one experiment in the appendix that gives this some backing (Figure 7, the w/o S&T experiment). However, it would also be interesting to see that the original latent mapper can’t be trained conditionally on multiple prompts.",is no ablation study that clearly separates these two factors to determine their individual contributions.
is done into W+ because it allows for better identity preservation. W+ inversions typically behave worse under editing than their W counterparts. See for example e4e [3].,"into W+ is generally used for identity preservation, but it does not necessarily improve editing capabilities."
"Directions’ method, for example, does not suffer from this limit. Actually, I would clarify early in the paper that this method specifically aims to improve on the latent mapper approach of StyleCLIP (this also ties to clarity above).","Direction’ method in StyleCLIP does not require separate models for each prompt, which should be clarified in the paper."
"for Real Face Editing, Li et al, 2021",for Semantic Image Editing.
"Attention, Hou et al, 2022",Attention-based Text-guided Modulation.
"Image Manipulation, Tov et al, SIGGRAPH 2021",Inversion and Manipulation.
"know, the originality is good.","As far as I can tell, the paper presents a novel approach to domain generalization by focusing on minimizing the surrogate penalty without increasing empirical risk."
is theoretically and technically sound.,"The proposed method, Satisficing Domain Generalization (SDG), is well-motivated and theoretically sound, drawing on rate-distortion theory to ensure convergence to a stationary point of the empirical risk."
"The proposed optimizer (SDG) significantly improves the performances of CORAL, FISH, and VREx on several datasets.","The obversion of the generalization gap and in-distribution performance in the WILDS benchmarks is interesting, as it highlights the trade-offs between empirical risk minimization and generalization error."
Experiments with more penalty-based DG methods will make the effectiveness of SDG stronger.,"The evaluation of the proposed optimizer (SDG) is a little weak, particularly in terms of computational efficiency and scalability."
"2, SDG increases the generalization gap on four datasets (IWILDCAM., OGBMOLPCBA, AMAZON, and PY150) and decreases the generalization gap on only three datasets (CAMELYON17, FMoW, and POVERTYMAP).","The statement that the proposed optimizer improves empirical risk without hurting the generalization ability as desired in lines 184 – 287 is unconvincing. In Table 3, the generalization gap is not consistently smaller across all benchmarks, which raises questions about the robustness of the method."
FISH for FMoW is already strong is far-fetched with the worst accuracy of 34.6.,"In lines 309 – 311, the explanation that the performance of the method is impacted by computational complexity is clear, but the paper could benefit from more discussion on potential solutions to mitigate this issue."
"generative models to sample from this manifold appears very interesting and intriguing, at least conceptually.",generative models based on hyper-representations is novel and promising.
"results. It is inspiring to see that trained generative models can capture the structure of the weight manifold and learn the set of sufficiently accurate neural networks, even capable of producing competitive model ensembles.","evidence through experiments on multiple datasets, demonstrating the effectiveness of the proposed methods in both in-dataset and transfer learning scenarios."
its current form it appears to require a very significant computational investment for pre-training an ensemble zoo. This large computational cost could make it somewhat impractical for real-life applications.,"its current form, it may require further refinement to handle more complex architectures and larger-scale model zoos."
explanation in Section 3.2.3 could be clarified and made more rigorous. It is not entirely clear to me how $d$-dimensional samples $n_i$ are mapped back to the $D$-dimensional space.,authors should clarify the assumptions behind this dimensionality reduction and provide more theoretical justification or empirical evidence to support this claim.
"my knowledge, this seems to be the first study in SDA from the viewpoint of SMI functions.","my knowledge, this is the first work to propose a unified framework for subset selection in supervised domain adaptation (SDA) using Submodular Mutual Information (SMI) functions."
presented results seem sound.,experiments are well-designed and demonstrate the effectiveness of the proposed method across multiple datasets.
"in the manuscript, the writing is mostly clear.","and minor grammatical errors, the overall presentation is clear and easy to follow."
"the (stochastic) gradient descent is possible. So adapting ORIENT in the existing ML framework (like PyTorch or TensorFlow) seems quite easy. Furthermore, numerical experiments demonstrate that the proposed method indeed reduces the learning time: sometimes 2-3x speed-ups. I believe that ORIENT is useful and valuable for practitioners.","the models are trained using gradient-based optimization methods. This flexibility makes ORIENT a versatile tool for improving the efficiency of SDA methods, especially in resource-constrained environments. Additionally, the empirical results show that ORIENT can significantly reduce training time while maintaining or even improving performance, which is a valuable contribution to the field."
supported by thorough theoretical and empirical justification., and it provides a novel approach to regularization in matrix completion tasks.
with good interesting results that are easy to interpret., covering a wide range of scenarios and providing robust evidence for the claims made.
well-written. The figures are also clear.," well-structured, making it easy to follow the theoretical developments and experimental results."
"helpful. For example, when the authors say that a term may accelerate convergence (line 168-9, 202), it would be helpful to describe (or give intuition for) the conditions under which such acceleration occurs, before jumping to the empirical demonstrations that acceleration is indeed observed. How does the convergence rate or error scale with effective rank of the underlying matrix (Appendix experiment does not really answer this)?"," beneficial for readers to better understand the nuances of the optimization dynamics, especially in terms of how the penalty interacts with different optimizers and depths. This could include visualizations or more detailed explanations of the transitions between different regimes, such as when depth becomes less relevant or when the penalty starts to dominate the dynamics."
could be strengthened by noting that shallow networks are extremely fast and lightweight and thus could find use in applications where these attributes are important.," could be expanded to include more practical applications or connections to non-linear networks, which would broaden the impact and relevance of the work."
"that the authors design a novel implicit regularization scheme, whereas it seems that implicit regularization refers to an existing property of gradient-based methods (the tendency to favor low-norm solutions). It seems that this implicit regularization is innate to both SGD and Adam, but it would be clearer if the authors instead highlight the property of adaptive optimizers that makes them more effective in the regime studied."," a focus on deep learning in general, which might mislead readers into expecting results that apply to non-linear or more complex architectures. A more accurate title could emphasize the specific contributions to linear networks and the novel interplay between explicit regularization and adaptive optimizers like Adam."
recommendation approaches. This is challenging to derive the generalization bound and may arise much more research attention in the community.,"methods, particularly in addressing preference bias and improving generalization."
"of DPCML against the current CML-based algorithms, and the experiment results seem promising.","of DPCML across various benchmark datasets, demonstrating its superiority."
is good and easy to understand.,is clear and well-structured.
"improved. For example, there are a few typos and mistakes in grammar:",improved for clarity and conciseness.
superiority.”: I think this sentence should be polished.,effectiveness in real-world scenarios.'
Page 15: “Hard” should be “hard”.,contains a minor typo.
29: “…training/validation/test” should be “…training/validation/test sets”.,needs clarification.
Page 31: “Fig.7” should be “Fig.12”.,requires a minor revision.
attached to its corresponding proof link to make it easy for the reader to follow.,clearly stated and accompanied by intuitive explanations.
"is a qualified paper with good novelty, clear theoretical guarantees, and convincing empirical results.","paper addresses these concerns effectively, but minor improvements are needed."
written and easy to parse,1. The paper is well written and structured.
stated and situated in the context of previous work,2. The motivations for the work were clearly articulated and justified.
baselines over multiple seeds with calculated standard deviations.,3. Experimental evaluation over several datasets and multiple tasks is comprehensive.
together from previous work (which itself would not be criticism per-se if there was novelty in the way the stitching together was done).,"My primary concern with this paper is that the method seems incremental. Particularly, all the core elements in this work can be stitched together from existing techniques with minor modifications."
"bi-level optimization has been done in [1, 2].","Specifically, learning auxiliary task adaptive weights via gradient-based optimization is not novel."
"are not uncommon in the multitask literature see Section 3.2 of [3] which introduces an index k, for each layer.","For adapting auxiliary tasks weights on a per-module basis, there are multiple related works that are not referenced. Layer-adaptive weights have been explored in multi-task learning and should be discussed."
"task weights - this abstracts away the choice of Parameter/Block/Model-L introduced in this paper - and represents an automated ""modularization"" approach","Also note that in [4], they use the primary task to learn a subspace within which they deconstruct the auxiliary task gradients, which is highly relevant to this work."
end-task aware training as an alternative : https://arxiv.org/abs/2109.07437,[1] Should we be pre-training? an argument for auxiliary task pre-training.
Multi-task Optimization in Massively Multilingual Models. https://arxiv.org/abs/2010.05874,[3] Gradient Vaccine: Investigating and Improving auxiliary task interference.
"good, the bad and the neutral. https://arxiv.org/abs/2108.11346",[4] Auxiliary task update decomposition: the missing reference.
intriguing problem under-explored in the field of FL.,important aspect of optimizing model performance.
"batching, to make their MI-based sampling framework more practicaly feasible.",and batching to enhance the efficiency of participant selection.
different scale of data settings and multiple models.,a wide range of datasets and models.
considered in the whole process.,adequately addressed.
may not be the case for many VFL scenarios.,may limit the generalizability to more complex scenarios.
groups are generated. Random grouping over a multitude of clients can be problematic.,well the subsets are designed and the number of participants involved.
"be too casual. E.g., ""# participants"", ""$NN_{k, y_q}$"", ""All-train"", etc.",lack consistency and clarity in certain sections.
"the authors, I've updated my score to accept","the authors, the paper has improved significantly."
"constraints. I would judge that the originality of the paper is high as, according to the authors, nobody has attempted to provide a hyperparameter tuning-free approach to the switching between synchronous and asynchronous training modes.","resources, especially in shared clusters where hardware availability fluctuates."
the training modes that help to determine and address the gaps in the current work. This provides clarity on the choices and explains the conducted experiments.,"different training modes, particularly the synchronous and asynchronous modes."
"my confusion in the Questions section below. In summary, it seems that the methods in the figure and their performance do not correspond to the description in the text.",some suggestions to improve the clarity of the figure and its explanation.
"properties in the industrial setting. I believe that this impact might translate to better resource utilization and, therefore, cost savings.","training, particularly in environments with dynamic resource availability."
"for the various components e.g. encoders, decoders). Furthermore, it appears novel to me, although I'm not familiar with the neural PDEs literature.",and adaptable to various PDE problems).
"important examples. The experiments are thorough, covering all the important ablations I could think of while reading the paper.","cases, particularly in terms of speed and dimensionality reduction."
"to understand, but there are some minor problems discussed below.","to follow, with well-structured sections."
for the importance of neural PDE solver well.,that their method can significantly accelerate PDE simulations.
paper and clearly above the acceptance threshold.,contribution to the field of PDE modeling and optimization.
"minor points are listed below and in the ""Questions"" section.",phrases and explanations could be clearer or more precise.
are a few typos and confusingly worded sentences.,are a few minor grammatical issues.
a few which made the paper harder to understand:,some specific instances where clarity could be improved.
"probe"" -> ""flow to probe""?","into turbulent phase"" could be rephrased for clarity."
"satisfies given"" Remove ""that satisfies""?","the boundary condition may be non-differentiable"" could be clearer."
"don't understand ""relieves"" in this context - use another word?","suggest rephrasing to ""LE-PDE eliminates the need for local evolution."""
"struggling to parse this sentence, though I think the point is that it's not possible to backprop through discrete variables.","not sure what this means. Consider rephrasing for clarity, perhaps ""This prevents the gradient from passing through to the boundary parameter $p$, such as in the case of continuous location."""
"[0,0,2]$"" Should this say ""$[0,2]$""?","[0, 0.2]"" should be clarified as ""where $\eta$ is a parameter in the range [0, 0.2]."""
don't follow what this means and I can't find the exact details in the supplement.,"suggest rephrasing to ""average quantity of advected smoke simulated by the solver."""
"infer the interaction between the agent, allies, entities and memory.", effectively handle partial observability in multi-agent reinforcement learning by leveraging a transformer-based memory structure.
outperforms GRU-based one which is usually used., demonstrates superior performance and learning acceleration across various tasks.
and memory types and the number of memory slots., showing its significant contribution to improving the model's performance.
"example, in Figure 2 (a) and (b), ATM-based models outperform GRU-based models in steps, but when comparing in wall-clock time, ATM-based model could be much slower than recurrent module-based models."," instance, the Entity-Bound Action Layer allows actions to focus on relevant entities, improving both computational flexibility and explainability."
"transformer-based agents [2,3] get the advantages from attending the past directly, while it cannot do that through ATM architecture.", memory mechanisms like relational memory could be explored for more complex reasoning tasks.
"reinforcement learning using actor-learner distillation."" arXiv preprint arXiv:2104.01655 (2021).", reinforcement learning' discuss the trade-offs between computational cost and performance.
"reinforcement learning."" International conference on machine learning. PMLR, 2020.", reinforcement learning' highlight the challenges in optimizing transformers for RL tasks.
"with transformer world models."" arXiv preprint arXiv:2202.09481 (2022).", with transformers' explores the use of transformers in sequential decision-making tasks.
"less restrictive assumptions. Each of the steps taken along the way are well-motivated, and the proofs tied to the new assumptions (separability, access to a baseline policy for symmetry breaking) are clean and relevant.",action-inclusive assumptions that allow for more realistic feedback scenarios.
"that is tied to action information is already a key problem in fMRI information orthogonalization (line 340), and for eye tracker recalibration for ALS patients (line 341). Unfortunately, none of the existing evaluations reflect these real-world use cases, instead constructing synthetic tasks based on MNIST or open classification benchmarks — tasks that are hard to understand.","Especially for the latter, the problem of having feedback data that mixes action and reward signals is well-motivated, but the experiments do not fully reflect the complexity of these real-world scenarios."
"a digit with label = (action + 6 * binary-reward - 3 modulo 10). In general, it’s not clear why the action-intermingling with feedback is an action problem in real-world settings, or why other, simpler approaches for learning couldn’t learn to decouple this information.","simplified and lacks the richness of real-world multimodal feedback, such as that found in neurofeedback or HCI applications."
"little bit, so I am updating my score.","bit by including a simulated BCI experiment, but further real-world validation is still needed."
very interesting and inspiring. This is the first work deliberately designed to model the evolution process of graphical conventions through a Pictionary-like game., a novel approach to studying the evolution of communication systems.
training strategy is used to smooth the abstraction process., training method is introduced.
"conventions, which are of great importance to enable quantitative measurements on visual communication.", conventions in a systematic and quantifiable manner.
the quality of the target sketch $\hat{I_S}$ produced from image $I_S$., the sequential decision-making and mutual adaptation between agents.
"40 classes with 10 images per class, making the experiments somehow insufficient.", limited to 40 categories with 10 images per category.
excellent job motivating the problem and giving intuitions for their algorithm.,excellent job in presenting their contributions clearly.
compares and contrasts related works to this paper.,situates the work within the broader context of COCO.
seems novel and changes the proof techniques for bounding the constraint violation.,is an interesting and novel approach that strengthens the algorithm's performance.
"be helpful to add experiments for some of the real-world applications of this framework mentioned in the Introduction section (e.g., safety-critical applications).",be beneficial to include real-world datasets to further validate the practical applicability of the algorithm.
"usually correspond to adversarial and stochastic (typically i.i.d.) constraints, however, in this paper, it refers to adversarial and fixed constraints.","should be more clearly defined, as it could confuse readers about the scope of the algorithm's applicability."
"assumed this condition holds to obtain better bounds, and how the framework in this paper does not make such an assumption.","relied on it, and how the current work manages to bypass it in certain cases."
times in the paper before it is finally defined on page 6.,"times, but its significance and impact on the results should be elaborated further."
"$g_t(x)\leq 0~\forall t\in[T]$. The authors should explain and motivate this choice of benchmark, and mention any potential hardness results for regret against the more natural static benchmark.","the condition $g_t(x) \leq 0$ for all $t$, which may not be necessary and could limit the generality of the results."
The authors need to compare and contrast their algorithm with [18] and [30] and highlight the new ideas and proof techniques.,It would be helpful to provide a more detailed comparison to highlight the differences and improvements.
"a small but informative dataset, which could inspire more related work.","small, high-fidelity data summaries for collaborative filtering tasks."
and meanwhile it is more robust to noise compared to SOTA.,even when trained on significantly smaller datasets.
NTK and KRR to help the audiences better understand the proposed approach as these are pretty new topics in recommendation systems.,infinite-width neural networks and their relevance to recommendation systems.
"how it performs while with other collaborative filtering models besides $\infty$-AE, e.g., how the synthesized data summaries work with other autoencoder-based recommendation systems?",its performance across a wider variety of recommendation models beyond ∞-AE.
for all the datasets? Is it setting the user-budget based on the percentage of user number can help the audiences better understand the results?,"is appropriate should be clarified, especially in relation to dataset size and sparsity."
"Fig 5, bottom Middle). It is easy to follow.",in terms of its structure and flow).
"on CIFAR-100 (across different KD methods, teacher-student architectures).",in both accuracy and efficiency.
"noticeable amount of errors. If class-wise analysis is performed on the teacher based on misclassified samples, it corresponds to a subset of classes which can be treated as undistillable classes. Therefore, the proposed TLLM avoids such situations by eliminating classes where the teacher's predictions are largely incorrect and replacing them with ground-truth labels (one-hot vectors). Some analysis I’d like to see are:","few mistakes, and these mistakes are propagated to the student model, leading to undistillable classes."
teacher (Table 3) for regular and undistillable classes?,on the ImageNet dataset?
"in Tables 2, 3 for regular and undistillable classes?",into per-class accuracy gains?
baseline for ImageNet experiments? Can the authors produce a table similar to Table 2 for ImageNet to show the improvements of the proposed TLLM scheme?,primary baseline for comparison?
~72.154% whereas the reported accuracy is only 68.900%. Please clarify. Link to public pytorch models with top1 accuracies: https://pytorch.org/vision/stable/models.html,closer to the reported standard of 71.8%.
"to include the accuracies of students trained with label smoothing. Can you report these results for Tables 2, 3. I suspect that a noticeable amount of improvement can be obtained by using label smoothing, especially if undistillable classes are semantically similar classes (See [2, 3]).",to evaluate the performance without label smoothing or to apply label smoothing uniformly across all methods.
(Not available in Supplementary as well).,from the paper.
paper outweigh the strengths. But I’m happy to change my opinion based on the rebuttal.,work are minor and can be addressed with additional experiments and clarifications.
great effort on the rebuttal.,detailed responses.
my concerns although concrete understanding of undistillable classes still remains unclear.,the concerns raised in the initial review.
of label smoothing on the proposed TLLM framework.,of label smoothing on the final results.
"label smoothing help?."" Advances in neural information processing systems 32 (2019).",label smoothing help?'
M. (2021). Is label smoothing truly incompatible with knowledge distillation: An empirical study. In ICLR,M. 'Label smoothing regularization in knowledge distillation.'
(2022). Revisiting Label Smoothing and Knowledge Distillation Compatibility: What was Missing?. ICML,'Improving knowledge distillation with label smoothing.'
to learn simple interpretable models. The numerical results are convincing., for producing risk scores that are both interpretable and computationally efficient.
of error cumulation is possible; coordinate descent can be long as well as the line search).,(some kind of complexity in implementation and debugging).
scores are not considered in the current contribution., world applications may require further tuning and validation.
based framework where the motion estimation and motion compensation are needed. This is an encouraging attempt.,"based approaches, which is a notable deviation from traditional methods."
"shifting, sharpen/blur, and fading. Some analyses are conducted.","shifting, blurring, and fading."
is limited. The core contributions have been investigated in other papers.,is somewhat incremental rather than groundbreaking.
in [1]. The difference is that one previous frame is used in [1] and two previous frames are used in this paper.,"in several prior works, making this approach less novel in the context of temporal modeling."
"like [2,3,4,5]. Directly applying transformer into neural video codec is straightforward and the novelty is limited.",and its application to video compression is a natural extension.
"model, where the block contains two parts. In addition, [7] has investigated a similar idea where each block contains 4 parts. The difference of this paper is that each block contains 16 parts.","model, but the paper does not sufficiently explore the potential benefits of this comparison."
"this paper does not need these designs. The block-based auto-regressive entropy model, LRP, and 3-stage training are also complex and hand-crafted designs.","proposes a simpler transformer-based approach, but the complexity of the transformer model itself is not fully addressed."
The RD-curves are very close. The BD-rate numbers are best presented.,This raises questions about the practical benefits of the proposed method.
"x264 and x265 is very easy in 2022. Comparisons with JM, HM, VTM are recommended because they represent the best encoders of H.264, HEVC, VVC. The work [8] in 2021 has already compared with VTM.","these codecs in terms of rate-distortion performance is a significant achievement, but the comparison should be clarified."
"Efficient Video Compression, ECCV 2020",Neural Video Compression.
"Window-based Attention for Image Compression, CVPR 2022",A Comprehensive Study of Video Compression.
"for learned image compression, ICLR 2022",for neural video compression.
"Priors for Learned Image Compression, CVPR 2022",Transformers for Video Compression.
"Efficient Learned Image Compression, CVPR 2021",Efficient Video Compression.
"Adaptive Neighborhood Information Aggregation, arXiv 2022.",Transformer-Based Models.
is a novel contribution. It targets a useful application and can be of use to the community and for content creators.,"is a novel and useful addition to the GAN adaptation task, allowing for more flexible and precise control over the adaptation process."
"studies. Moreover, the two metrics used are reasonable in the context of face to face translation.","study, providing a more objective measure of the model's performance."
adjusted to the results presented in the experimental section.,well-supported by both qualitative and quantitative results.
not well redacted and fails to situate the proposed method within other methods in the literature.,comprehensive but could benefit from a more detailed comparison with recent advancements in the field.
"domains. Instead, those are only computed on hand-picked source-target images and risks providing only a biased positive view of the results.",domains to provide a more robust evaluation of the model's performance.
should be computed over a dataset (or a randomly chosen subset of data) for the source and target domains in order to draw more robust conclusions.,should be computed on a larger dataset to ensure the results are statistically significant and generalizable.
"not need to be very deep and research on shallow models is still useful, which is valuable to the literature. This is the primary reason I recommend accepting this paper.",not necessarily need to be deep to achieve high performance.
SSE block. The motivations are well explained and the effects are verified.,Skip-Squeeze-and-Excitation modules.
is launched on a separate GPU) The results and discussions of the communication overhead are useful.,"is processed independently, allowing for faster inference."
"Of course, I understand the authors may have limited computing resources and did not lower my rating for lack of such large-scale experiments.",Future work could explore how larger models behave in terms of depth and performance.
clue on how to realize it. Maybe the authors can show a potential direction.,empirical results on custom hardware.
improved. Please see the suggestions below.,improved for clarity.
written and easy to follow.,written and structured.
network to obtain the positive samples is novel.,model for instance-specific positive generation is innovative.
paper is in the experimental part.,approach is its reliance on the quality of the generative model.
in the original papers. I think strictly following the setting from the original paper will be a fairer comparison. (e.g. 100 Epoch of SimCLR should reach 62.8% Top-1 Accuracy),"typically reported in the literature, which may affect the fairness of the comparison."
generative model affect the final result?,generated images impact the downstream task performance?
if we include an additional generative model?,associated with training the latent space navigator?
therefore I am raising my score. Even though after the correction of the theory the numerical example does not satisfy the theory now I believe the submission provides an interesting step towards better theoretical guarantees of BNNs.,the authors have addressed many of the concerns raised.
extension of the previous results in Holder space into the Besov space.,contribution to the field of Bayesian neural networks.
because in contrast to some people who prefer formulae over word description I would prefer the latter to better understand the formulae. Please see comments on this below in Clarity section.,I recognize that the theoretical results seem to be well-grounded and rigorous.
"paper is theoretical, the numerical example provided is not too satisfying.",paper lies in the theoretical guarantees provided for Bayesian neural networks.
on 2 synthetic examples with analytical functions,in a theoretical context.
"the variational inference, i.e. approximation of the true posterior. Wouldn’t MCMC methods with true posterior be more appropriate for the purpose of the numerical example?",variational inference rather than the MCMC-based methods discussed in the theory.
that. Some discussion on actual computational time used and how feasible the inference with the considered priors would be appreciated,how the proposed model performs in real-world applications.
jumping straight into it as if a reader has just finished reading the previous work and doesn’t require any introduction - the approach used in many recent papers.,"jumping straight into the technical details, which is appreciated."
"better read words over formulae. Word description of formulae would be much appreciated. Also, the notation is not very careful which does not help further with readability of this lot of maths.",prefer a more balanced approach between mathematical rigor and explanatory text.
Figures 2 and 3 can also lose the histogram part to save some space. (The histogram part can go into the appendix also),This would allow for more detailed explanations of the key theoretical results.
this case. It does seem to make a step further in extending those guarantees (by providing them in a bigger Besov space).,the context of function spaces like the Besov space.
"of the paper. It seems the paper has significant and original results but it should also be easier to understand these results while reading the paper. Moreover, the numerical example can be largely improved.",of the theoretical contributions without a deeper understanding of the mathematical framework.
"as it hasn’t been said before anything about practicality, only about asymptotic properties",and could be replaced with a more formal transition.
properly defined in the paper,clearly defined in the introduction.
"\in {0, 1, …, d}”?",should be clarified as it is used in multiple contexts.
confusing though technically it is not overlapping notation. It is probably better to use another letter for a dataset,confusing and could benefit from clearer notation.
D^u and d^u is used. What is the difference?,equations could be better explained for clarity.
\eta is not defined,notation could be improved.
already used for the Dirac function,not properly defined.
"defined, how does it depends on n?",explained in sufficient detail.
is better to used \mathcal{N} for the normal distribution,should be replaced with a different symbol to avoid confusion.
"axis, also subplots without labels look too strange for me. It seems they may have “(a)”, “(b)”, … labels",better clarity and understanding.
- “spike-and-slab prior” was meant?,should be defined earlier in the text.
suggested as future work? It would indeed be nice to have some comparison in the current submission.,referring to future work or current experiments? This should be clarified.
"data, A” - lower case for “A”",data” could be rephrased for better clarity.
- acronym is not introduced,should be expanded to Gaussian Process for clarity.
solve THE optimization problem”,"be consistent with the rest of the text, rephrase this part."
THE (nearly) optimal convergence rate”,respect to the prior” could be clearer.
- lower case for “L”,us define” would be a better transition.
submodular functions is useful for a number of ML applications.,submodular functions is a significant contribution of this paper.
learned from data is a good alternative to having to choose a fixed one.,trained end-to-end using backpropagation is a novel and effective approach.
written with clearly presented results,"written and structured, making it easy to follow the technical details."
"of monotone submodular function of the form of concave composed modular function, which is too limited.","of submodular functions, but their generalization to other set functions is not fully explored."
functions is too simple to capture complex functions.,functions is well-justified and theoretically sound.
submodular function (DSF) baseline is limited (see questions).,submodular function models is thorough and demonstrates the superiority of the proposed method.
"structure, and optimization. SIAM Journal on Computing 47.3: 703-754, 2018.",and approximation are key aspects discussed in this context.
"and xos functions by juntas. SIAM Journal on Computing, 45(3):1129–1170, 2016.",functions are relevant to the theoretical underpinnings of this work.
"13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224–233. Association for Computational Linguistics, 2012.","International Conference on Machine Learning (ICML), this work is closely related to the proposed method."
intuitive idea for data augmentation,effective approach to data augmentation.
very effective when implemented correctly,to generate task-informed augmentations without relying on domain-specific knowledge.
"inspiring. Moreover, the theoretical interpretation of the method is sound.",provides a clear explanation of the theoretical foundations.
and datasets are provided in this manuscript to show the effectiveness of the proposed LP-A3.,demonstrate the versatility and effectiveness of the proposed LP-A3 method across domains.
think it is necessary to provide the selection detail to make the manuscript self-contained.,would appreciate more details on the specific criteria used for selection.
"the backbone networks in the experiments in Table 2? Moreover, How’s the performance of LP-A3 on CIFRA10/100 in a supervised classification manner? Evaluation with other backbone networks rather than ResNet-18 and ResNet-50 is also important.","the results when compared to other state-of-the-art augmentation methods, such as RandAugment or MixMatch, on multiple datasets? This would provide a more comprehensive evaluation."
"on the large-scale dataset, such as ImageNet",in terms of computational efficiency?
"performance of LP-A3 can drop when all data is selected for augmentation? The author stated that some data augmentations are useless to apply data augmentation, which did not convince me.",performance seems to degrade when the label-preserving margin σ is too small or too large? A more detailed explanation of this behavior would be helpful.
"the fairness framework, which works for any generalized mean objective and any fairness criteria.","the q-mean objective, which is valid under any metric."
a linear time ranking algorithm through rigorous mathematical derivation.,a (3−ε)-approximate solution for the fair rank aggregation problem under the Ulam metric.
Sections of Related Works and Conclusion. Authors can refer to previous paper published in NeurIPS for revision.,a clear and concise conclusion section that summarizes the key contributions and results.
work. It is difficult to convince me that the algorithm proposed in this paper is superior to the state-of-the-art algorithms.,"work, which limits the discussion of how their approach compares to other recent advancements in the field."
"objective in Section 1.1 $\sum_{i=1}^n (\rho(\pi_i, \sigma)^q)^{1/q}$ should be $(\sum_{i=1}^n \rho(\pi_i, \sigma)^q)^{1/q}$ in line 105.","objective is not clearly defined, leading to potential confusion about the exact nature of the problem being solved."
"fair ranking and prove the algorithm is a linear time ranking algorithm. However, the greedy strategy for ranking is lack of innovation","the closest fair ranking problem under the Kendall tau metric, which is both simple and efficient."
"given the existing results provided by the authors, the approximation in the $L^p$ norm is a necessary complement to our current understanding of the approximation problem.","Though the approximation result does not appear very surprising, the paper provides a solid contribution to the field by addressing a gap in the literature."
the authors have presented the paper in a quite accessible way.,"The results are technical, however, they are presented clearly and with sufficient detail."
"However, it is not obvious how in practice these two norms can make a difference.",The authors successfully tackle this challenge by leveraging VC dimension theory in a novel way.
"the work. Below I will outline my thoughts, questions, and comments in the order of the paper.",I was left with a mixed impression of the paper.
"have been improved (see below), but overall I had no problem getting into the results.",in some cases the presentation could be improved.
"his works, but there are many more, both in the centralized and decentralized cases. This is important not only to note Richtarik's contributions, but also to compare your results with those available in the literature (see below, point 3).","In particular, one of the most active authors in the field of compressed optimization is P Richtarik."
"in the current data type (e.g. 32 or 64 bits). If I understand correctly, the bounds take into account the presence of $B$, but do not depend on it. I think it is important to reflect this.","$N$, $K$, $D$, $e$, $L$, $\mu$ and $B$, where $B$ is the number of bits used in communication."
"an invention of the authors, but I read the original paper about it. Thanks for this experience!",It is not immediately clear how it compares to other compressors in the literature.
"smooth problems under PL conditions, it is typical to expect $\mathcal{O}\left( \frac{L}{\mu}\right)$. For example, the compression method from [1] has such results. If we remember about communications, then the estimate (in bits) from the work [1] looks like","The authors' estimate (in terms of $L$, $M$, $\mu$) is equal to $\mathcal{O}\left( rac{L^2 M^2}{\mu^4}ight)$,"
or 64 bits). This is the square root of $K$ times better than for the uncompressed method. The authors' estimate is,$B$ is the number of bits used in the current data type (e.g. 32 bits for floating point).
"and use $B = 32$, then $\log_2 D \approx 23-24$ and $\frac{B}{\sqrt{K}} \approx 6-7$.That's why I stress that comparison with other results is very important! There are many works, not only [1].","For example, if we train the Bert model on 25 agents, the communication complexity might still be prohibitive."
"and I'm still not sure it's very important and interesting for deep research. My concerns are related to the fact that despite the improvements in convergence in terms of $N$ and $D$, in terms of $L$ and $\mu$ we are far from good convergence estimates, namely $\mathcal{O}\left( \frac{L^4}{\mu^4}\right)$. And this effect is noticeable both in the original paper and in the one under review.",although I saw many papers where simpler things are introduced as overparameterization.
"in the work), what order of gain can be obtained for certain problems (due to the fact that we change $D$ to $N$, but lose significantly in $L/\mu$) and so on. Now it seems to me that the game is not worth the candle.",how wide a class of problems satisfies it (perhaps the authors will be able to add something here besides what is written in the paper).
"Definition 4. The essence of this method is that it is a centralized method, but with a slight inexactness in averaging. It is obvious to me that this can be done.","it uses a multistep gossip protocol with compression,"
"questions, and have not taken a clear view of the paper. At this point, I will give a borderline reject to motivate the authors to take an active part in the discussion.","This is important to me because so far I have mixed feelings,"
"compression //International Conference on Machine Learning. – PMLR, 2021. – С. 3788-3798.",MARINA: Faster non-convex distributed learning with compressed gradients.
"point out to an inherent limitation that arises from optimization and may explain many of the previously reported issues with neural Lewis games. As such, the analysis in this paper has the potential of substantially influencing the study of emergent communication, and I strongly support its acceptance.","It is largely clearly written (apart from some issues, see below), and the empirical results (especially figure 2 and 3) are persuasive and surprising -- they highlight the importance of controlling co-adaptation overfitting in emergent communication systems."
"loss, while, in practice, it is used to *downweight* the influence of the adaptation term. While the two are equivalent, these two conflicting views make it harder to understand the discussion in lines 237-244 (I think this paragraph in particular has to be improved. particularly the sentence at line 240).","In equations 10-11 the constant alpha is presented as a way to *upweight* the original loss, but the explanation could be clearer in terms of how this impacts the balance between the information and co-adaptation terms."
distills them into student policies via well-motivated structural-relation loss constraint,bypasses bootstrapping for credit assignment.
performance in various multi-agent environments,results across multiple benchmarks.
"of every component in the proposed method, as well as discussed.",of each component in their framework.
w/o pretraining. Authors should include a comparison with MADT-online too.,"without conventional distillation, highlighting the benefits of the new approach."
"(where number of agents > 2), simpler transformer-based method MADT performs comparable to the proposed method.","with a large number of agents, the centralized decision transformer may struggle."
and grammatical errors in the text,throughout the paper.
this model is novel and creative. It will likely be of interest to many in the community given the recent successes of image generation approaches like DALL-E.,"To the best of this reviewer's knowledge, this is one of the first works to unify such a wide range of tasks using a single model."
being very optimized for these diverse vision tasks.,"having task-specific components, which demonstrates the generality of the approach."
"the autoregressive model helps a lot, and there is a 'sweet spot' in terms of code length.",the guiding code and autoregressive structure are crucial for performance.
"later), but nothing else that stands out to this reviewer","the societal impact discussion), which lacks depth and fails to address potential risks adequately."
"5 runs. For a fair comparison the authors should rerun the experiment and generate statistical mean/variance results for random train/validation split to remove any bias due to specific train/validation split selection. Additional details about the baseline model (ResNet-26) e.g., network architecture/training time/training accuracy would be helpful to get an all-round comparison.","However, the authors show the accuracy for one single train/validation split whereas the reference [10] shows a statistical mean of 82.2 +- 0.3% over multiple random splits, which provides a more robust evaluation."
The paper lacks any detailed analysis on the contribution of the standardization and smoothening steps which could be generated by selectively applying these preprocessing steps.,This lack of explanation leaves the reader questioning the underlying reasons for the performance difference.
"compares to the baseline results would be helpful. For instance, the baseline has a higher antibiotic treatment classification accuracy than the proposed model, but this isn’t discussed and no justification is provided for the same.",A discussion on these figures to help understand their results and how it relates to the performance of their model would have been beneficial.
"deal with non-smooth outer function); 2) under similar assumptions compared with related works, the algorithms proposed in this paper achieve the best convergence rate with known condition number","handle the nonsmooth regularization terms effectively, especially in cases where the outer problem involves nonconvex and nonsmooth functions like L1 regularization)."
"of application of this work, i.e. this work can do but the previous works can not. It is good to mention more examples/applications of using non-smooth objectives.","of application for the proposed methods, as the paper does not explore other potential nonsmooth regularization terms or broader nonsmooth settings."
learning the classification label makes intuitive sense and seems to be important in their experiments.,fine-grained visual categorization is well-supported by both theoretical and empirical evidence.
the motivations and experiments run by the authors.,the necessity of modeling both relation-agnostic and relation-aware information.
"do not think this adds much to the paper and the space would be better spent giving a high level, clear explanation of the intuition.",believe this could be condensed to make the paper more accessible to a broader audience without losing its rigor.
"different than the TransFG architecture. Is the main difference the global embedding of the object that the local crops can be compared against and using all three embeddings -- the z_g, z_L, r -- when computing distances and the final metric?","distinct from existing attention mechanisms, and the paper could benefit from a clearer explanation of how AST offers a unique advantage over standard attention layers."
"useful these things really are. Moreover, the improvements are small over standard methods, and there is no standard deviation to explain if these results are significant. I further wonder if the authors carefully made sure that their setup was similar to the underlying setup and that the improvement was actually due to their method or better data augmentation or underlying architectures.","significant the contribution of the relational proxies really is, and whether the marginal gains justify the added complexity of the model, especially in practical applications."
way to mitigate meta-overfitting and cover the range of the task distribution,approach to address meta-overfitting in few-shot learning.
presented well. I have more comments about the method (see below),addresses key challenges in task augmentation effectively.
"such as classification, regression and cross-domain tasks",of both regression and classification tasks.
the classification results to provide a more robust picture about the improvements. The gap between MLTI and the proposed method is not too much (except for ISIC).,the performance metrics across different datasets and meta-learners to better assess the statistical significance of the results.
"paper shows some preliminary results on cross-domain tasks, I would like to see some more discussion and insights around it as I believe it's a more realistic scenario for few-shot learning","paper does include cross-domain experiments, a deeper analysis of how task augmentation affects generalization across significantly different domains would strengthen the contribution."
see. Can you please provide some more details on the generation cost and how you incorporate task augmentation during meta-training?,"see discussed, especially in comparison to other task augmentation methods."
more empirical insights to support the results would be better than the theoretical analysis in this regard.,could stand on its own without needing to rely heavily on the theoretical justifications.
"practical setting and worth studying. In my view, this paper can inspire subsequent researchers and tackle SSL in more realistic scenarios.","This is a significant and practical extension of the traditional SSL framework, addressing real-world challenges where labeling all classes is infeasible."
of seen classes. This can be simply integrated with existing SSL methods. Experimental results are convincing to demonstrate the effectiveness of the proposal.,"The authors propose two novel SSL objectives to classify unseen classes and maintain the safeness of seen class performance, which is a key contribution."
organized and easy to follow.,"The paper is well-structured and clearly written, making it easy to follow the proposed methodology and experimental results."
unseen classes. It may limit the application of the proposal.,"The proposed method needs to know the number of unseen classes in advance, which may limit its applicability in fully open-world scenarios."
$\lambda_1$ and $\lambda_2$ need to be discussed.,"The impact of the trade-off hyper-parameters λ1 and λ2 on the final performance is not thoroughly explored, which could be important for practical deployment."
"improvements (linear scaling w.r.t. input size, adaptive computation with large speed boost) compared to Neural Interpreters, are impressive.",improvements in few-shot adaptation and OOD robustness are noteworthy.
to follow. The figures are well-made and very helpful.,"to follow, with well-structured explanations."
datasets) and the number of baselines (now only Perceiver IO) both need to be reasonably improved. E.g. Perceiver IO was evaluated on 6 different modalities (Table 5 in their paper) and against non-general purpose baselines within each modality.,and point cloud classification) should be expanded to include more diverse tasks and baselines.
proposed architecture is significant and will inspire or lead to future breakthroughs.,proposed model is truly general-purpose.
"unified convergence analysis for arbitrary client participation. Specifically, these are the main strength of the paper:","focus on arbitrary client participation patterns, which is a critical aspect of real-world federated learning scenarios."
"leave or join arbitrarily depending on their circumstances. There are not much work that has a unified analysis of different patterns (including stochastic) of client participation, and this paper contributes to the federated learning community largely by presenting such analysis.","become unavailable or participate intermittently, making this work highly relevant to practical applications."
client participation in federated learning [31]. It is interesting to see in what scenarios for arbitrary client participation we can achieve a similar linear speedup which the paper thoroughly provides insights on.,"client participation, demonstrating that the proposed method is competitive with state-of-the-art approaches."
requires additional memory saving at the server side (which in general is a plus for federated learning). This method also achieves 0 convergence error in some client participation patterns which is interesting and validated in the experiments.,"requires tuning of the amplification parameter η, making it a lightweight yet effective modification to existing algorithms."
the strength of the paper in my opinion. Here are some of the weaknesses:,the overall strengths and contributions of the work.
"0 convergence error in the analysis, the insight/reason for this is unclear. The paper also lacks a bit in explaining the main intuition behind algorithm 1.","better convergence rates, the paper could benefit from a more detailed explanation of why amplification works in practice."
has not been viewed much either theoretically or empirically throughout the paper. Perhaps having some ablation study on the $\eta$ can help further understand the implications/effectiveness of the proposed algorithm 1.,"on the convergence rate is not fully explored, and more empirical results on the sensitivity of η would be helpful."
be more interesting if the authors gave some connection with the arbitrary client participation patterns that exist in the real-world federated learning applications such as periodic shifts [6-7] (as cited in the paper).,be interesting to see more specific real-world participation patterns and how the proposed method performs under those conditions.
temporal operation is very interesting and novel to my best knowledge., +the core idea of using KMA to align spatial information for temporal modeling is innovative and well-executed.
and the results are strong.,"+The mathematical derivation is solid, providing a clear theoretical foundation."
achieve consistent performance improvement using different backbones.,+The proposed method is robust and can generalize well across different video learning tasks.
"the model is not thorough enough, see questions section",-The analysis on what has been learned by the model could be more detailed and insightful.
with more clarity see limitation section,"-some details should be presented more clearly, especially in the experimental setup."
well written with comprehensive details.,The paper is well-written and provides a clear explanation of the proposed method.
baseline methods by a large margin,The proposed method outperforms the baseline CLIP model in several zero-shot transfer learning tasks.
"modern Hopfield networks, which are all existing works even though the application scenario is new",The novelty of the paper is below the bar. It combines the InfoLOOB and modern Hopfield networks to address the explaining away problem in contrastive learning.
pair of tasks to come up with initial hypothesis and then tests those hypothesis on CW10/CW20 benchmarks.,"a wide range of metrics, including forward transfer, forgetting, and average performance, to provide a comprehensive evaluation of the proposed methods."
and is overall simple while still achieving state of the art performance in CW10/CW20,"and effectively combines the insights gained from the experiments, such as behavioral cloning and exploration strategies."
are true only for the CW benchmarks. I do appreciate the authors being very upfront about the limitations of the work.,may not generalize to other RL algorithms or benchmarks with different task structures or state spaces.
The proposed approach seems new to me. Relevant literatures are well discussed.,"The authors provide a novel perspective on private high-dimensional data generation by directly optimizing a set of samples instead of training deep generative models, which is a significant departure from existing approaches."
"mistakes. However, some details of the algorithm and the experiments are missing (see my questions below).","flaws in the methodology. The approach is well-grounded in theory, and the use of gradient matching for optimizing the synthetic set under DP constraints is a clever and effective solution."
to follow and well organized.,"to follow, with clear explanations of the methodology and well-structured sections that guide the reader through the technical details."
"budget. Not sure whether what happens when the budget is moderate, say $\epsilon = 5$. Also, it would be better if the performance is benchmarked on colored image datasets (e.g., CIFAR-10).","budgets, but it would be beneficial to see results for a wider range of privacy levels to better understand the trade-offs between privacy and utility in more granular settings."
in a standard model used for personalized treatment selection, that addresses the challenge of large treatment spaces.
has a theoretical group-consistency property, effectively clusters treatments with similar effects.
experimentally validated on synthetic and real data, demonstrated through simulations and real data.
whereas discrete ones may be observed for several treatments/pathologies, but can be extended to other types in future work.
and may underfit the data in various situations, with interaction terms for treatment effects.
"stated, and provide a good balance between the theory and intuition.","organized, and easy to follow."
to be novel and significant.,compelling and insightful.
sufficient to support the claims.,well-designed and thorough.
reading your argument on why white-box attacks appear to fail with MSE.,how they compared square loss and cross-entropy in various settings.
least the following three which advocate using MSE instead of CE:,least three relevant works that should be referenced.
"vulnerability"" by Jacobsen et al",vulnerability' should be cited.
"responsibility for adversarial examples"" by Nar et al.",been linked in prior work' should be referenced.
"in neural networks"" by Pezeshki et al.",in deep networks' is another missing citation.
"grammar check. For example, ""it's"" --> ""it is"".",more concise and clearer presentation.
"data is linearly separable or the model has enough capacity to fully classify the data. However, here, authors refer to label ambiguity.","is linearly separable, but here it seems to mean something more general, which should be clarified early on."
simplex coding is rather difficult to follow.,simplex coding is interesting.
the experiments is not provided.,experiments should be made available.
which makes a lot of sense and is potentially high impact.,"which they call RAMBO, which introduces adversarial training to enforce conservatism."
in the experiments in the benchmark.,"on the MuJoCo benchmarks, achieving state-of-the-art results in several domains."
"[69] are loose in the general case. Similarly, lines 189-190 are much too strong. The constants are not discussed at all, which prevents the reader to understand the true nature of the offered guarantees.","the proposed algorithm are valid but do not guarantee near-optimality, especially in high-dimensional or continuous state-action spaces."
"your work into the literature, but I find it very confusing here, while Problem 1 is much clearer right after.","the adversarial model as a policy, but the interpretation of the adversary as a second policy could be clarified further."
"this is again confusing. I would have expected to see here the TV with respect to the dataset, instead, which we learn much later to be what the practical algorithm is doing (section 5.2).","the practical implications of using $\widehat{T}_{MLE}$ in continuous spaces need more discussion, especially regarding its limitations."
A and found that the same imprecision is in the proof. It should actually be instead $z\sim d_\phi^\pi(s)$ where $d_\phi^\pi(s)$ is the state distribution starting from $s$.,"A, and the notation should be revised to avoid confusion between the state variable and the sampling process."
for a better analysis of what it does better than other model-based Offline RL algorithms.,to demonstrate the basic principles of RAMBO before applying it to more complex domains.
be useful too as it is where model-based approaches typically fail.,provide a more comprehensive evaluation of RAMBO's performance in high-dimensional settings.
"and Krishnamurthy, Akshay and Simchi-Levi, David and Xu, Yunzong (NeurIPS, Offline RL workshop 2021)","highlights the inherent challenges in achieving near-optimality in offline RL, which should be acknowledged more explicitly in the paper."
"and Lee, Ilbin and Dai, Bo and Schuurmans, Dale and Szepesvari, Csaba (AISTATS 2022).","discusses the limitations of passive data collection, which could be relevant to the discussion of RAMBO's performance in certain datasets."
"SGD outperforms GD if the spectrum of the Hessian has some outliers, which is usual in realistic applications, see Figure 2.","In particular, this condition number is tied to the average curvature of the Hessian, which differs from the classical condition number used for full-batch methods."
analysis might be of independent interest to study the behavior of SGD.,its application to the analysis of SGD is novel and insightful.
"problems **directly**. Next, the author should provide the ""real"" convergence comparison between SGD and GD even for the random feature models to support their main observation. ICR is just an indicator for the comparison between SGD and GD. There might be an approximation error due to the usage of HSGD and also the dimension, iteration time is finite. Hence, without this simulation, I am not convinced that SGD does outperform GD in terms of convergence speed.","However, note that the random feature model is a convex optimization problem, which differs a lot from realistic neural networks."
"value (for strongly convex cases, they converge exactly to the same point). To my best knowledge, all the previous papers studying the implicit regularization focus on nonconvex optimization.","For convex optimization, **any** algorithms, if convergent, then they should converge to the same function value, making the lack of implicit regularization unsurprising."
unfamiliar with it. It would be good if additional background information can be provided.,"which is very unfriendly to those unfamiliar with the mathematical framework, as it lacks sufficient explanation or intuitive examples."
a causal query without knowing the causal graph. The paper is well-written.,causal queries through active interventions.
answer to the causal query of interest. This makes it both novel and more data efficient.,"perform causal reasoning separately, but integrates both tasks into a unified framework."
models and hence it is implemented for nonlinear Gaussian models. How can this algorithm be used for model classes that contain discrete variables?,models due to the computational complexity of marginalizing over the space of causal graphs and functions.
"objective is sub-modular then it is a constant approximation)? Moreover, as it is suggested by the experiments, there is not so much gain compared to the random selection method considering the additional computational cost of design experiment line in ABCI algorithm.",information gain is bounded or if there are any convergence guarantees for the selected interventions in terms of query accuracy or efficiency)?
the computational complexity of ABCI would be interesting. The experiments are also for small graphs.,the potential for parallelization or approximation techniques would be beneficial.
"On the other hand, the authors of “Learning Causal Graphs with Small Interventions” show that designing minimum number of interventions with limited size to learn the causal structure is NP-hard. When the actable set of variables are limited in size, then it is interesting to see how ABCI algorithm performs compared to the aforementioned work.","However, the proposed method does not explicitly address how it would handle latent confounders or violations of causal sufficiency, which are common in real-world scenarios."
see whether ABCI algorithm in the learning interventional distribution task learns the whole graph or only a part of it before finding the interventional distribution of interest.,clarify how the method would perform in cases where the causal graph is only partially identifiable or when there are hidden variables.
structural assumption. It is an assumption on the underlying generative model.,"necessary assumption for identifiability, but it can simplify the problem in certain cases."
incentive design for multi-agent MDPs. This extends the well-studied envy-freeness concepts in fair resource allocation to policy design in MDPs.,"multi-agent policy teaching, which is a novel and important extension of existing policy teaching frameworks."
"2) how to compute them, 3) at what loss, providing a complete story behind the envy-free incentive design model.",and 2) how to minimize the cost of these incentives under different fairness constraints.
"current analysis and model to the RL, reward shaping, and fair resource allocation literature.",envy-free policy teaching framework to both fair division and policy teaching literature.
"envy-free incentives. The LP based approach requires exact knowledge of the reward and transitions, which is impractical for many settings.","cost-minimizing solutions, particularly the assumption of identical discount factors in some cases."
example to help instantiate the discussion and help the model become more clear.,example or case study to better illustrate the real-world applicability of their framework.
on OOD setting is interesting and sounding.,on decentralized data sharing is well-motivated.
is simple and effective.,is simple yet effective.
"actually not novel in the community of DG (e.g., [A]), these methods can also be applied to the individual domain followed by FedAvg.",a well-known technique and its application here does not introduce significant novelty.
Generalization Network for Intelligent Fault Diagnosis,Generalization is not directly addressed in this work.
It's interesting to see how these methods perform based on the setting DG.,"However, the paper does not sufficiently differentiate its approach from existing methods."
"imaging), which require privacy guarantee for FL setting, for evaluation.",imaging or financial data) to better demonstrate the practical utility.
adapt the architecture (inclusion of a Tree-LSTM) and augmentations to be suitable for tree structured data.,"adapt it to a tree structure, which has been done before in other contexts."
"criterium determined for CoNMoR training if no validation set was present? And second, the test set was already seen during CoNMoR pre-training which may highly boost the test set performance when subsequently training a supervised classifier.","criterion determined without a validation set? This could lead to overfitting, especially given the small dataset sizes."
"fully unsupervised, however if labels are used to determine the epoch where KNN accuracy is highest, it is still not fully unsupervised. It would be more fair to report the KNN accuracy at the end of pretraining.","a self-supervised method, but without a clear stopping criterion, it is difficult to assess the robustness of the results."
"pattern around L5. So I’m not too sure whether the representations are really encoding all the features correctly, or simply distinguish L5 from the rest.","around this class, which could indicate that the model is biased towards this particular class."
"it’s a reconstruction of what? So try to give a better background about these reconstructions, what the data exactly is, how it is measured and what you are reconstructing here. Line 55: what is swc format? Better to explain that as well.","this is not clearly explained until much later in the paper, which could confuse readers unfamiliar with the domain."
"So if you want to target a broader audience, try to explain the concepts better. Remarks like “Also, the number of such 1-degree",The explanation of how the neuron trees are processed could be simplified or clarified for a broader audience.
and raw data resolution.” need extra explanation.,and this could affect the generalizability of the model.
be better understable if a figure was added to illustrate the concept.,benefit from rewording to improve clarity.
more descriptive caption. What is the meaning of the numbers and how should the reader compare them to Table 3?,"more detailed explanation of the augmentations used, as the current description is somewhat vague."
"general method, for any type of tree-structured data. I also wonder whether it is needed to give this method a separate name CoNMoR, since it is basically SimCLR, adapted for tree structured data.","general framework for tree-structured data, rather than focusing solely on neuron morphology."
learning a CVaR maximizing policy in a distributional RL setting., optimizing the CVaR objective in distributional RL.
naive application of the 1-step CVaR does not solve the objective., the proposed approach outperforms existing strategies in terms of CVaR optimization.
+ option trading + ATARI., demonstrate the effectiveness of the proposed method.
"experiments that enable a more theoretical analysis of the method, complex domains enable a more convincing empirical justification."," task domains used for evaluation, the Atari results could benefit from more extensive empirical investigation."
always resort to tasks such as continuous control / minatari / cartpole / and other simpler tasks., consider using smaller-scale environments or fewer training steps to provide additional insights.
is warranted and well accepted in the community. I'd propose to move some proofs to the appendix in order to allocate room for discussion., omission leaves the reader without a clear summary of the contributions and potential directions for future research.
interesting that a required condition for this phenomenon (noise magnitude proportional to loss value) is provably satisfied in linear nets and random feature models.,an interesting and valuable contribution to the understanding of SGD dynamics.
"condition for instability, not a necessary-and-sufficient condition.","condition for stability, but not a necessary one."
"the noise component in isolation. Accounting for both simultaneously is going to be hard, so I don't begrudge the authors for this simplification.","the noise component, which may limit the generality of the results."
"scratching my head wondering why alpha, beta, and mu are defined the way they are. Later, when I got to section 3, I realized that \mu is precisely what is needed to trigger exponentially fast escape.","trying to understand the implications of the conditions without the context of the escape behavior, which made it harder to follow."
"increase in the bins requiring an increase in the number of output parameters. Having a finer grid of bins through auto-regressive additions, is very good idea.",It is a simple and efficient mechanism to solve the niche but real problem of the varying scales and distributions in time series data.
"their are openly vocal about the limitations of the work, which is something that should be praised.",especially in acknowledging the limitations of their method and the challenges in tuning hyperparameters.
discretization of time series in general in the section 2.2.,importance of handling both discrete and continuous data in a unified framework.
"paper, and I think that the paper would benefit a lot from having these written out more clearly:","paper, such as the exact process of tuning the number of bins and levels in C2FAR."
"neural network for p(z_i|x,z_{<i}). What kind of neural network is being used?",Pareto tails and how they are integrated into the final level of the model.
of the bin where the data point falls) I think that a pseudo code would be helpful to understand how this is being done.,"is, the likelihood of the observed data, rather than computing the full distribution.)"
are. I only saw in the appendix that if corresponds to K = 1.,represent in terms of model complexity and binning levels.
the code. I see this as a major issue and would want the code to be open sourced if the paper is to be accepted.,sufficient details in the paper or supplementary material to fully understand the implementation.
"work exactly. In general, and compared to [17].",are parameterized and how they affect the overall distribution.
"layer, are they values on the observed space or quantiles?","neural network layer, but their exact role in shaping the distribution is unclear."
how is the density corresponding to the Pareto tail estimated?,how do they interact with the binning process at the final level?
"so, how does the network cope with the changing bin positions?","so, how does this affect the overall distribution, especially in the tails?"
force to compute the full densities?,improve the model's ability to capture extreme values?
tails? Have you evaluated the calibration of the tails?,"Pareto tails compared to other tail distributions, such as truncated Gaussians?"
does not compare to the main comparison partner [17] which has uniform bins (like the flat tiling of C2FAR) and Pareto tails.,should clarify how C2FAR's hierarchical binning differs from this approach.
"a wider range of probabilistic forecasting datasets, and potentially regression datasets.",datasets with more extreme values to test the effectiveness of the Pareto tails.
levels B. It would be interesting to lay it out clearly and to evaluate what the optimal trade-off is.,"levels B, and it would be useful to explore this trade-off more explicitly in the experiments."
comparison to more flexible distributions like normalizing flows?,impact of varying the number of bins per level on model performance.
show the results with standard deviation.,provide more details on how the model scales with increasing levels and bins.
it would have been valuable to provide a more exhaustive evaluation.,"compared to existing work, the authors should emphasize the practical benefits of C2FAR more clearly."
Spliced Binned-Pareto Distribution for Robust Modeling of Heavy-tailed Time Series,"is cited in the paper, but the relevance of this reference to the current work is not fully explained."
"3 in Davies & Ghahramani (2014). Even putting aside Davies & Ghahramani (2014), this paper is not terribly ""new"" or ""novel"": nothing in this paper is very surprising.","The authors don't cite Davies & Ghahramani (2014), but the ""Affinity Score"" in Equation (1) is equivalent to Section 3.2 of their work."
authors seem to have made a substantial engineering contribution.,the practical utility and empirical performance of the method are more important.
literature. So the authors aren't (with the exception of the one paper) over-claiming the originality.,they otherwise nicely document the related work and position their contribution within the existing literature.
"NLL, CRPS, and RMSE, (iii) 3 different base models including LightGBM, XGBoost, and CatBoost, (iv) several types of output distributions. I checked the code the authors provided and it looks nice.","The authors have clearly done a lot of work in examining, (i) 22 datasets including 21 standard benchmarks, (ii) 3 performance metrics including CRPS, NLL, and RMSE, and (iii) multiple base models like CatBoost, LightGBM, and XGBoost."
easy to understand. The code is also well-written and easy to understand.,The paper is well-written and easy to follow.
"opportunity to use the authors' work. As the authors write, there's a fairly big chasm between the ease-of-use of GBRT and the availability of probabilistic models, so closing that gap in an easy-to-use way is nice.","The (currently anonymous) code is available under an Apache 2.0 license, so my hope is that lots of others will have the opportunity to build on this work and apply it to their own problems."
"will not scale well and therefore are unlikely to be used very frequently, but on the other hand it would be nice to have comparisons between the authors' simplified approaches and the more intricate approaches.","On the one hand, the authors are right in saying that Bayesian models and more complex approaches like BART are computationally expensive and may not scale well to large datasets."
approaches. Retrofitting GBRT for probabilistic prediction is always going to be a bit of an art since the probabilistic prediction is missing from the beginning and really only added on later.,"but I do have some concerns about this because (with trees) the goal should maybe be ""different"" performance of IBUG to other methods, especially in terms of flexibility and interpretability."
"rebuttal, they're not really show-stopping points. The engineering contribution here is rather extensive.",final version of the paper.
"the authors clearly show their contribution. To be honest, I do not have many issues with this submission in its current form. Next, I have some comments and questions.","Motivation and related work are concise, and the deformation priors are learned by the use of a canonical space, i.e., the shape transformation between two arbitrary poses is divided into two steps: a backward deformation to align the source mesh to the canonical space, and a forward deformation to map the canonical space in the target deformation space."
"While this process increases the complexity of the neural model, as two different models need to be learned, in practice the process is strongly simplified. This idea is simple yet effective. Once the transformation models are learned, a simple 3D displacement, via a deformation field, is considered to deform the mesh.",This two-step process helps reduce the complexity of learning deformation priors and improves the generalization to new deformations.
Eq. (1) should be defined.,the proposed method are well-defined and explained clearly.
"CAD-based models, this could be a very hard task in real scenarios, where noisy point clouds could be considered with a variation of points in the representation.","While this can be an easy task for synthetic scenarios and controlled datasets, it may pose challenges for real-world applications where such dense correspondences are not readily available."
"unseen identities could be the result of a simple linear combination of seen ones, i.e., the unseen motions are really seen ones. I would like to know as the authors can guarantee that division with no additional analysis.","In my opinion, this should be clarified in the paper. How was that done? Note that some additional details on the selection criteria for these sequences would improve the clarity of the experimental setup."
"checking the corresponding image. Maybe the authors could include the corresponding ground truth mesh, or color-based representation where every color displays a different error.","The authors claim their solution produces visibly the best results. I disagree with that. That conclusion is not actually easy, after closely examining the visual results, as the differences between methods are subtle and subjective."
"outperforms, in both quantitatively and qualitatively, the competing approaches.",still shows a clear improvement in terms of quantitative metrics and generalization to unseen deformations.
interpret a bit more the learned deformation priors. My doubt is the deformation priors could be just an algebraic representation with no meaning. Could the authors help me with this question?,"In that case, my question is: could the proposed method obtain the deformation? This could help us to understand the robustness of the method in handling unrealistic or extreme user inputs."
the full mesh by a real vision sensor that means the mesh includes noisy and partial observations).,real-world animal movements or poses) to further test the generalization capabilities of the method.
"explored yet, but seems an important problem for real-world applications.", sufficiently explored in prior works.
"algorithm-agnostic formulation and evaluation. The difference between CQL and H2O is dynamics-gap-based reweighting $\omega(s, a)$ compared to CQL(\mathcal{H}), which uses uniform action distribution for the weighted sum of $\exp(Q)$."," adaptive mechanisms for handling dynamics gaps in offline datasets, which could offer a more generalized solution for hybrid RL."
well-structured and easy to follow., well-written and clear.
because H2O is only tested on HalfCheetah (Table 1)., to specific environments and tasks.
"(dynamics-changed) agents as target tasks. I'm not sure why the authors flip the settings, and whether H2O works in a reverse setting. In my opinion, even if we could not model the real-world dynamics perfectly, we might use the best-efforted simulator for training. So the setting in DARC[1] seems more likely than that of this paper in the real-world problem."," dynamics or corrupted environments are used as target tasks, which might lead to different interpretations of sim-to-real transfer effectiveness."
"(Section 4.4) $P_{\mathcal{\hat{M}}}$, with Gaussian distribution as typical model-based RL method did. So it seems a straightforward but effective approach to model simulator ""source"" dynamics $P_{\mathcal{M}}$ with Gaussian distribution."," and could potentially bypass the need for such estimations, simplifying the approach."
"of other baselines, such as CQL and DARC(+). As far as I checked Appendix, there are no additional details.", of additional baselines for a more comprehensive comparison.
which might be valuable to prove the proposed method might be directly connected to real-world application., demonstrating the practical applicability of the proposed framework.
"domain. Since IsaacGym has a well-designed dynamics-parameter-randomization [3], and such simple parameter-randomized training helps the agent to successfully deal with real-world dynamics [4]. It might be beneficial to include such dynamics randomization as a baseline."," domain, which differs from traditional sim-to-real setups where the source domain is often a more realistic simulation."
"choice of $\omega(s,a)$ in H2O is also a heuristic. So randomized dynamics training might be a fair comparison against H2O.", proposed method could benefit from a more detailed comparison with domain randomization techniques.
Training for Transfer with Domain Classifiers. 2021., Tackling Dynamics Gaps in Sim-to-Real Transfer.
Reward Augmentation in Offline Reinforcement Learning. 2022., Reward Adjustment for Offline RL.
GPU Based Physics Simulation For Robot Learning. 2021., Physics Simulation for RL.
for General In-Hand Object Re-Orientation. 2021., for Efficient Sim-to-Real Transfer.
models using in-place layer compute. The paper tackles an important problem,models by reducing memory footprint and increasing throughput.
on diagrams could improve clarity of the paper,But improvements could be made in the explanation of certain technical details.
first paper to analyze transformer training memory footprint,first work to explore memory footprint optimizations for Transformer-based models.
for training thus the method lacks originality,and has been applied in other contexts before.
be added to strengthen the paper,strengthen the claims made in the paper.
well organized and well written.,well-written and clear.
of SOTA MTL methods as regularization methods.,on the performance of SMTOs in MTL.
raise valid concerns regarding the evaluation and experimental methodology in the MTL literature.,demonstrate that unitary scalarization is competitive with more complex methods.
MTL methods through the lens of regularization.,MTL optimization techniques are provided.
"(or uses other approaches to scale the computation), hence resulting in only a small increase of runtime.",but this paper shows that simpler methods can achieve similar results with less computational cost.
"While interesting and novel, it appears to provide only a partial explanation.",The authors should provide more empirical evidence to support this claim.
comparisons and evaluation on more relevant benchmarks:,details regarding hyperparameter tuning and model selection.
"strong MTL approaches [1,2,3] that show large performance gains w.r.t unitary scalarization.","works that show improvements in MTL performance, such as [1] and [2]."
methods (for example in terms of mIoU). This might suggest that all methods are under-optimized.,"considered metrics, which raises questions about the generalizability of the current findings."
"task improvement (compared to STL), used in previous MTL literature [1,2,3].","improvement across tasks, rather than absolute performance."
as the early stopping criterion? Metrics have different scales and meanings. A good approach is using the relative task improvement as the stopping criterion.,for model selection? This needs to be clarified to ensure fair comparisons.
"early, so it’s hard to learn something meaningful about the optimization dynamics. (iii) Finally, the Cityspcaes dataset consists of only two tasks that were shown to benefit each other in previous works. Hence, we cannot learn from it about optimization with a large number of possibly conflicting tasks.",too early to draw meaningful conclusions about the long-term behavior of the methods.
11 tasks (used in [1]) and the NYUv2 dataset with 3 tasks (commonly used through MTL lit.).,a larger number of tasks and more complex data structures.
a Bargaining Game. ICML 2022.,a regularization technique.
for Multi-task learning. NeurIPS 2021.,for multi-task learning.
in Multitask Learning. ICLR 2022.,in multi-task learning.
"than static, which is a less exposed area;","than static graphs, which is a notable improvement."
strong and support the method;,"convincing, showing significant improvements over baselines."
graph filter (not of all the method) are given;,"temporal polynomial graph are provided, which adds rigor to the method."
graphs; while later on discusses that there exist prior work on learning over time-varying graphs;,"graphs, while similar approaches have been explored in prior works."
"for MTS; hence, the specific contribution of the paper is unclear and not well positioned, consequently, its novelty is lower than claimed.","and dynamic graph learning, which could provide a more comprehensive context for the contributions."
"bypassing any discussion related to causation; hence, limitations of the approach are not thoroughly discussed;",without considering other potential factors that could influence prediction accuracy.
rather focused on the graph filter but not of the overall solution;,do not directly address the core challenges of dynamic graph forecasting.
encoder with all the dynamics or from the non-graph based decoder.,"approach or from other architectural choices, such as the temporal attention mechanism."
surfaces/objects is an important direction for the NeRF field.,"objects is a challenging task, but CageNeRF provides a novel approach to address this."
parametrization and tackles e.g. aliasing issues well.,solution that leverages cage-based deformation to manipulate implicit neural representations.
"animation. CageNeRF performs worse but close enough in my opinion, considering that it is a general method.","models, and the results show that CageNeRF performs comparably, though with some limitations in geometry estimation."
written in good English.,well-written and provides a clear explanation of the proposed method.
"steps in many places, see below in Questions.","details regarding the implementation of the cage deformation, which could benefit from further clarification."
deformations would show that what the method does goes beyond what an anisotropic matching of the object bounding boxes could do. I don't believe the deformation transfer results are sufficient evidence that deformation transfer actually works with the method.,"geometries or more complex structures should be explored to better demonstrate the generalization capabilities of the proposed method, as the current examples do not fully showcase the potential of CageNeRF."
two sequences). That leaves the door wide open for cherry-picking a few good indices. Averages (and standard deviations) across the entire sequences should be reported.,"a limited number of objects), which raises concerns about the comprehensiveness of the evaluation."
generalize well to deformed target poses that lie far away from the training data.,"always produce geometrically accurate results, especially for complex or highly detailed objects."
"adequate, taking into account scenarios with different federal settings",comprehensive and well-documented.
simple and achieves good results,rigorous and systematic.
"the method proposed in this paper, and also performs knowledge distillation locally",FedCurv and other regularization-based methods.
in Federated Learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 8397-8406.,in Federated Learning and its Impact on Global Model Performance.
"theoretical guarantees for the popular Thompson sampling algorithm. Since model-based RL algorithms in continuous-time are still lacking within machine learning, I very much appreciate this work.","In my opinion, this paper solidly develops a novel theoretical framework for Thompson sampling in continuous-time diffusion processes."
"the intuitive implications of the bandit analysis, which is normally filled with hard-to-understand mathematical intricacies.",I appreciate that the authors discuss the technical challenges involved in continuous-time RL.
"worst-case regret than the existing RL policies, thanks to its informed exploration."", which is a bit far stretched, as this work only compares one other algorithm to this Thompson sampling strategy.",worst-case regret than existing policies could be better supported with more detailed empirical comparisons.
"unsurprisingly very old. This goes back to the more than 50-year-old works of Feldbaum [2] and is since then been discussed within the control community. This topic, which in principle optimally solves the problem has to be discussed within the related work.",Theory discussing simultaneous optimal estimation and control is highly relevant to the problem at hand and should be acknowledged more thoroughly.
"to the HJB equation numerically using the finite difference method. This would result in a control strategy that Bayes-optimally selects the actions and hence, balances exploration and exploitation optimally.","For example, there are very simple dual control problems, see, e.g., [3], where one could try to find the solution using the proposed methods."
that this paper is a solid contribution to the community.,that this paper makes a significant contribution to the field.
"control and estimation. Courier Corporation, 1994.",Control and Estimation.
"theory. I."" Avtomatika i Telemekhanika 21.9 (1960): 1240-1249.","theory and its applications."""
"a simple Bayesian system."" International Journal of Electronics 13.2 (1962): 165-177.","linear systems with unknown parameters."""
"sampling complexity is interesting, and to my knowledge it is original.",the sample complexity is quite innovative and well-executed.
"This is the main contribution of this paper, which substantially improves the sampling complexity.",It provides a clever way to reduce the bias in the entropy estimation.
"supported by theoretical results, which is comprehensive and convincing.",well-supported by rigorous proofs and detailed analysis.
"straightforward, while probably to be implementable by scratch, further details are required (e.g., a deeper explanation of the architecture implementation, fewer pointers to different papers for details)",well-structured and addresses key challenges in non-rigid registration effectively.
The attached video gives a nice insight into the low-to-high frequencies deformation.,The qualitative examples demonstrate the method's robustness.
shapes (Figure 6) is not convincing in terms of structural preservation of the shapes (the left-most example shows that A's chest is mapped to B's belly).,"shapes is the shape transfer example, which lacks detailed quantitative analysis."
"scale factor, but no quantitative results are provided in this case. No analysis of different amounts of point clouds partiality or computational scaling at the different number of points.","outlier rejection mechanism, but no thorough evaluation of its effectiveness is shown."
of of multiple scans',cost' should be clarified.
help to preserve' -> helps,helps' would be more appropriate.
alternative than [6] since it overcome some LBO limitations in the point coluds setting,reference for the context of coherent point drift.
"embedding, Marin et al., 2020",features would be a more relevant citation.
"acceptance. For the final version, I suggest including in the main manuscript the feedback about the method limitations.",acceptance with minor revisions.
"compared to 128 samples per ray in NeRF). However, on the other hand, it still requires a dense sampling of density network in the first stage, and as such the overall rendering efficiency is only slightly, if at all, improved without sacrificing performance.",The proposed method is well motivated and indeed effective in that it does significantly reduce the sampling rate of colour network (8 to 32 samples per ray).
"should be cited and compared to, these include faster NeRFs:",the authors should include more references to provide a comprehensive overview of the field.
Wetzstein. 2021. Autoint: Automatic integration for fast neural volume rendering,Wetzstein. 2021. 'AutoInt: Automatic Integration for Fast Neural Volume Rendering.'
"Moo Yi, and Andrea Tagliasacchi. 2021. Derf: Decomposed radiance fields.","Moo Yi, and Andrea Tagliasacchi. 2021. 'DeRF: Decomposed Radiance Fields.'"
and Julien Valentin. 2021. FastNeRF: High-Fidelity Neural Rendering at 200FPS,and Julien Valentin. 2021. 'FastNeRF: High-Fidelity Neural Rendering at 200FPS.'
and Angjoo Kanazawa. 2021. Plenoctrees for real-time rendering of neural radiance fields.,and Angjoo Kanazawa. 2021. 'PlenOctrees for Real-time Rendering of Neural Radiance Fields.'
KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs.,'KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs.'
"sampling, that may be worth citing as well:",rendering that should also be cited.
Wang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction.,Wang. 2021. 'Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces.'
Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction.,'Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction.'
"only includes two training sampling rates, what are the results for 1,2,8,16 samples per ray?",should include more diverse datasets to better demonstrate the generalizability of the proposed method.
"to exisitng approaches, we achieve more computational efficiency by exploiting the low-rank structure of gradients that is present in minibatch training.","In contrast to previous methods, PROJUNN-D and PROJUNN-T offer efficient low-rank updates that significantly reduce computational complexity."
"the batch size ( times sequence length for RNN), increasing the computational complexity even more.","the full matrix size, while still achieving competitive performance."
"among existing work, clearly stating advances and differences.",in the context of prior unitary and orthogonal neural network methods.
"This may allow users more control about properties of the linear operators that have been linked to generalization in deep networks like spectral norm, stable rank or eigenvalue decay.","This approach improves runtime efficiency, especially for large filters and many channels, while maintaining stability."
Unfortunately the CNN code was not available in time for the review,This will help researchers and practitioners easily adopt the proposed methods.
"performance, not benefits in applications where this technique is strictly required","efficiency, particularly in high-dimensional settings."
kernel function is impressive and reduces the complexity significantly.,"kernel function is an interesting approach, and the authors have demonstrated its effectiveness in reducing computational complexity while maintaining accuracy."
paper is clear and easy to follow.,"paper is generally clear, but there are some sections where the explanation could be more concise and focused, particularly in the theoretical analysis."
"example, for the citation networks, we have well-established benchmark setup in the community. The setup used in this paper is different, and the results of the baselines are not consistent with prior works. For example, on Citeseer, the accuracy of JKNet (~72) is much lower than GCN (~76), which indicates that the reproduced results of baselines might not be convincing.","a more comprehensive evaluation, it would be beneficial to include additional well-established benchmarks, such as those used in prior GNN and Transformer-based models, to better contextualize the performance of NODEFORMER."
"authors, my original concerns are addressed.","the authors, I appreciate their clarification on the scalability and the choice of datasets, but I still believe that including more diverse benchmarks would strengthen the empirical validation."
"of recruitment (as opposed to single-batch and soft-constrained setting considered in this paper). However, most of the evidence for this seems to be anecdotal, maybe it could be supported with more references.",I especially like the explanation of why the sequential and hard-constrained setting studied previously in the literature fails to capture the more practical complexities of real-world recruitment scenarios.
"candidates). However, it is not discussed when would one strategy/penalty function would be preferred to the other. While a one-sided penalty function seems natural to me, it would have been nice to have examples where under-recruiting would require explicit penalty (on top of the opportunity cost of not recruiting as many candidates as possible).","In particular, a practical strategy is proposed for both a two-sided penalty function (where recruiting fewer candidates than targeted is penalized explicitly) as well as a one-sided penalty function (where recruiting fewer candidates is not penalized explicitly, but it would implicitly lower the total quality of the recruited pool, thus indirectly affecting the outcome)."
paper; I have put my questions regarding the experiments below.,"While they provide some validation of the theoretical results, they could benefit from more real-world data or comparisons to existing methods."
to make training better without the added cost of methods like SAM.,to reduce the computational overhead of sharpness-aware training.
"to baselines is good. Moreover, this is without the cost.",to SAM shows that SAF and MESA outperform SAM with faster training speeds.
forward pass through both the EMA weights and the standard weights. Doesn't this require two forward passes where there was previously only one?,"memory-efficient strategy using EMA, but the exact computational cost breakdown is unclear."
name of SAF because there is an additional memory overhead (minor),"title, as SAF still incurs some computational overhead, albeit minimal."
Equation 8 is not well motivated and I did not see where the KL came from.,how the trajectory loss directly substitutes SAM's sharpness measure needs more clarification.
"Modern ResNets, e.g. with techniques from the timm library which is the library used in this paper can get to 80% accuracy (https://arxiv.org/pdf/2110.00476.pdf). Are the methods presented in this paper additive with these modern approaches?","The training settings for SAM and SAF should be more clearly aligned to ensure a fair comparison, especially regarding batch sizes and learning rates."
"and the notion of tangent residual and the choice of potential functions for convergence analysis are novel. Hence, I think the originality is sufficient.", and the introduction of the tangent residual is a novel contribution.
"look reasonable to me, but it seems that the numerical evidence can be stronger if the authors compute for more iterations in the experiments and plot the potential function value and the performance measures in Appendix J. (see my final question for detail)."," are missing, which would have strengthened the paper by providing empirical validation of the theoretical results."
behind some notions could be clarified as elaborated in my questions., behind the results is well explained.
"convergence rate in constrained smooth monotone games, which also matches the lower bound.", convergence rate for constrained smooth monotone games.
"variable-rate image compression, is addressed.", the proposed method addresses the challenge of variable-rate image compression by introducing selective compression of latent representations.
method seems to work well.," the proposed method demonstrates comparable or superior coding efficiency to existing methods, particularly at low bit rates."
"[Li'18], although it was not used for variable rate compression. Furthermore, the added value of the selective compression seems to be limited, especially for advanced base models like the context model [7]. In fact, it will be necessary to show the effectiveness of the proposed method on more recent models like [11, 13, He'21, Gao'21, Qian'21]."," in previous works such as [Li'18], [He'21], and [Gao'21], where importance maps are used for spatially adaptive compression, but the proposed method extends this by introducing a channel-wise adjustment mechanism for variable-rate compression."
"networks for content-weighted image compression,"" CVPR 2018"," neural networks for image compression"" also utilizes importance maps, but focuses on spatial importance rather than the channel-wise selection proposed here."
"for efficient learned image compression,"" CVPR 2021"," for efficient image compression"" similarly uses importance maps, but their approach is more focused on context modeling rather than selective compression."
"attentional multi-scale back projection and frequency decomposition,"" ICCV 2021"," multi-scale feature learning"" also employs importance maps, but their method is more focused on multi-scale feature extraction rather than selective compression."
"with global reference for image compression,"" ICLR 2021"," with importance maps"" also uses importance maps, but their focus is on improving entropy modeling rather than selective compression."
to be compared against different ways to accomplish variable rate encoding in the previous studies.," to include a more detailed comparison with methods such as [19, 20, 23, 24, 25] in terms of both coding efficiency and decoding time."
the text in Section 4.1. It is suggested to summarize them as a table.," the tables, and it would be helpful to provide a clearer summary of the key findings in the main text."
"of rigid-body simulation, I'd argue the method doesn't actually tackle any issue that make rigid-body simulation a hard problem, such as collision between complex object shapes, frictional contact, or even 3D rotation for objects with nontrivial inertial tensor.","However, while the paper is motivated by the challenges of modeling more complex systems like articulated rigid bodies, it doesn't fully address the limitations of existing methods."
"method which have an explicit formulation for it (e.g. LNN/HNN), end-to-end learned models (such as plain GNNs) can learn the effect from data and neighborhood relations and (b) the effect disappears in the limit of small edge lengths.","The non-diagonal mass matrix and inertial tensor is (a) only a problem for systems with complex topologies or non-uniform mass distributions, which are not the focus of this paper."
"to infer from geometry. Normally you'd want a learned method to do the opposite, i.e. learn complicated interactions/friction models that may be hard to measure for real systems, and manually encode the known priors. The only case I could think of where the proposed approach would be useful is for inferring non-trivial mass distributions which may not be visible (say, on a drawbridge made from composite anisotropic material); but those use cases would need to be demonstrated.","However, it's worth noting that properties such as length preservation, generalization that the paper shows stem from the fact that very little is learned here; all the constraints (segment length constraint, friction, drag etc.) are manually encoded, only inertial dynamics are learned, and those equations are both universal, and quite straightforward to derive from first principles."
"it adds. It could be extended into a much stronger paper by showing how any of the harder aspects of rigid simulation can be tackled, or how the tricky bits (e.g. constraints, friction, ...) can be learned.",the paper in its current form is overclaiming its contributions (rigid dynamics) and quite limited in what it actually demonstrates.
"and the forward propagation is sparsified. I think this is a fundamental problem and unless the discussions around training costs are corrected, it should not be published. Otherwise it will again mislead the community. If this issue can be corrected, I am happy to raise the rating score to be positive.", due to the inherent need for dense gradient computation in certain parts of the network.
only finally cares about the gradient to weights. I raise the score., The gradient calculation to auxiliary parameters is handled differently in this work.
"to even give any kind of explanation for why progressive sharpening might occur. (Actually, it was not even previously known that 2-layer linear nets can exhibit progressive sharpening.)","to rigorously analyze the dynamics of sharpness in this setting, providing a theoretical foundation for the phenomenon."
"network), instability causes the sharpness to decrease. (The catapult paper includes a handwavy explanation for this phenomenon, but not a literal proof.)","network), the sharpness increases progressively during training, which is a significant contribution to the understanding of optimization dynamics."
"that after the first few cycles of instability, there are a huge number of 'anomaly points' (steps where the change in the sharpness is not positively correlated with the change in last-layer weight norm). Second, I would point out that ""all layers seem to work together to influence the sharpness,"" as the authors write.","that the sharpness and weight norm do not always change in the same direction, which weakens the generalization of the results to more complex architectures."
"the inner product between the residual and the predictions) while deferring the complete proofs to the appendix. Then, _after_ discussing the two-layer linear network analysis, you could mention that some of the patterns might carry over to general networks. The authors are of course free to take or leave this advice, but I think that this restructuring would make the paper more compelling.","dominated by the output layer norm), while relegating the more technical details to the appendix for readers who are interested in the full derivation."
proposed in the paper is novel and is not discussed in previous work in equivariance. This problem deserves more exploration and attention.,and the paper addresses this by introducing Partial G-CNNs that learn to adjust equivariance levels based on the data.
method to obtain the output domain by learning the probability distribution over the group is novel.,authors provide a clear explanation of how partial equivariance is achieved.
in the paper help audience understand the paper easily.,are helpful in illustrating key concepts.
"combination. Could the authors show more experiments on the group other than SE(2), E(2), or the flipping group?","combinations of these groups, which is a novel contribution."
$u$ is in G? How to get $v^{-1}$ and how does it act on the group element? Correct me if I'm wrong.,how does this relate to the lifting convolution? A clearer explanation would be helpful.
"the whole group, and if we don't do pooling over the group dimension, then the transformation information is kept; for example, here, the permutation of features for ""6"" and ""9"" are different, which also can help the network to classify 6 and 9.","the entire group, and it would be interesting to see how Partial G-CNNs compare in terms of performance and computational efficiency."
figure and the table. I suggest the authors could check the misalignment.,"appendices, which should be corrected."
"Adding noise to neural nets has been studied in existing literature, but its covering number result is original.","The authors introduce a novel approach by incorporating noise into the composition of function classes, which is a fresh perspective in the field."
"I briefly checked the proof, it looks sound to my best knowledge.",the theoretical results seem sound and are backed by rigorous proofs.
this paper overall is very clear.,"the presentation is clear, but it may be challenging for readers unfamiliar with the technical background."
us understand the effect of noise in deep learning.,"in providing tighter generalization bounds, which is a significant contribution to the field."
bounded activation function. This limits the applicability of the proposed theory.,"bounded activation functions, limiting its applicability to ReLU networks."
"agents, showing that it is still possible to achieve $\mathcal O(\sqrt{MdT})$ total regret even with adversaries.",good agents in a collaborative linear bandit setting.
to usual phase elimination algorithms (where there is no adversaries).,to standard phased elimination algorithms.
by showing both $\sqrt{MT}$ and $\alpha \sqrt{dT}$ factors are unavoidable.,by providing a matching lower bound.
generalizations (collaborative GLMs and CLBs with adversaries) while achieving near-optimal regret.,generalized linear and contextual bandit settings.
efficient as it builds on phased elimination.,expensive to implement.
assumed to be independent Gaussians.,Gaussian.
from NLP has been adapted.,"Number of key models discussed in the paper include autoencoding, causal sequence modeling (CSM), Sequence-BERT, and Network-BERT."
used many a datasets out there to predict something useful as in autism or depression as a downstream task.,"The pretext tasks don't have anything particular to tie them to predicting brain states. The authors could have explored more domain-specific tasks that directly relate to mental state decoding, such as tasks that focus on specific brain regions or cognitive processes."
the rest? No investigation on this has been attempted.,"Why did causal models have to perform better than the other models? The paper does not provide a clear explanation for this, and further analysis is needed to understand the underlying reasons."
"authors also test a variety of different offline dataset compositions, changing the proportion of data that is medium-replay (MR), replay (RP), and medium-expert (ME).",The authors compare against strong baselines: TD3+BC (a state-of-the-art single-task offline RL method) and two multitask RL methods (PCGrad and Soft modularization). The comparison is thorough and provides a clear understanding of how their method performs relative to existing approaches.
much of the benefit comes from Skills Regularized Task Decomposition and how much comes from Imaginary Demonstrations.,The authors split up their method into SRTD and SRTD+ID. This is helpful to help understand how each component of their approach contributes to the overall performance.
exposition for a new method and show that it outperforms strong baselines from prior work.,"Overall, I think multitask offline RL is relevant to the community. The authors give clear motivation for addressing the challenges of heterogeneous datasets in this context."
top of the SRTD(+ID) embeddings. It would be helpful if the authors could fit this into the main text.,"It is only in the appendix that the authors actually describe how they train an RL agent on the specific environments used in their experiments, which could be more prominently discussed in the main text."
authors included in the main text more qualitative information about the MT10 tasks and why they are interesting.,The authors don’t motivate MT10. Why are the tasks in MT10 interesting? It would be helpful if the authors provided more context on why MT10 is a suitable benchmark for evaluating their method.
commonality of tasks.” I don’t fully understand this explanation and think it would be helpful if the authors elaborated on it.,"At line 243, the authors write: “It is because TD3+BC and PCGrad explore the orthogonality of tasks and SoftMod exploits the commonality of tasks.” This explanation is insightful but could benefit from further elaboration on how these methods differ in handling task relationships."
is interesting and under-explored.,is well-motivated and addresses a practical challenge in long-tailed recognition where the test class distribution is unknown.
justify the effectiveness of the proposed method.,demonstrate the effectiveness of the proposed method across various datasets and test class distributions.
clear and easy to understand.,"clear and easy to follow, with well-structured sections and detailed explanations."
"models was also explored in [3] though the studied problem is different. Second, **the** **Test-time Self-supervised Aggregation** simply a re-weighting of three models. The key contribution might be the prediction stability maximization, but optimizing this objective does not ensure to obtain the optimal weights.","experts has been explored in previous works, such as RIDE and ACE, which also focus on ensemble learning for long-tailed recognition. The novelty of the proposed method lies more in the self-supervised aggregation strategy rather than the expert learning itself. However, the contribution of this aggregation strategy could be further clarified and distinguished from existing test-time training methods."
"are accessible at once. However, in many applications, the assumption is not satisfied.","is available at once, which limits its applicability in real-world scenarios where test data may arrive in a streaming or online fashion."
Test-time Self-supervised Aggregation has to be performed at each test time.,"use of multiple experts and the additional test-time training phase increase the overall computational burden, which may not be feasible for resource-constrained applications."
Table 9 are not the best results.,"the tables are inconsistent, which could be confusing for readers when comparing results."
Self-paced knowledge distillation for long-tailed classification,A survey of ensemble methods and their applications in long-tailed recognition.
routing diverse distribution-aware experts,ensemble learning: A review of recent advances and challenges.
Minimization for Unbiased Long-tailed Classification,Minimization: A comprehensive study on domain adaptation and its relevance to long-tailed recognition.
"independency assumptions, however, this does not seem to negatively affect the performance of the models.",One weakness could be that they make strong assumptions about the independence of features in the pre-activation space.
using ensembles of activation functions. This is an interesting contribution as previously (MaxOut) would significantly increase the number of weights in order to have the same number of outputs.,They achieve this without significantly adding more parameters by leveraging efficient approximations of Boolean logic operations in logit-space.
and the exposition is clear.,and presents a novel approach to streaming radiance fields for dynamic scene reconstruction.
motivated within the context of local changes in adjacent dynamic video frames.,leverages the local continuity between adjacent frames to improve training efficiency.
contribution of the paper and a useful construct for dynamic video representation.,and effective method for reducing storage overhead without sacrificing rendering quality.
"on the impact of coupling the pilot model approach with the narrow-band tuning. Given the dynamism of the scene and the arbitrary position of the target viewpoint, it is not difficult to imagine that full scale training is qualitatively superior to a multi-scale approach that might suffer from loss of detail.","on how the pilot model guidance affects the overall rendering quality, especially in terms of potential artifacts or loss of high-frequency details, and whether it introduces any trade-offs in terms of visual fidelity."
is an important and challenging one., is well-motivated and addresses a critical challenge in modern VLSI systems.
and brings performance improvement over previous approaches., and leverages state-of-the-art reinforcement learning and generative models.
"subtle by adopting a different state definition, and it's not clear how such minor extension leads to better performance observed in experiments.", incremental and does not introduce significant innovations beyond handling mixed-size macros.
"is a joint design, I suggest to perform more evaluations of the proposed approach as a whole to compare with the best combination of existing placers and routers."," is the integration of placement and routing into a unified neural pipeline, more emphasis should be placed on evaluating the combined performance of both modules."
"Table 1, Table 3 and Table 4."," the experimental results, which makes it difficult to assess the statistical significance of the reported improvements."
is very common in practice.,is well-motivated and clearly articulated in the paper.
"distribution shift, when the test distribution cannot be observed in advance, is quite reasonable.",robustness to handle different degrees of distribution shift is innovative and promising.
only limited to a toy environment.,sound and provides valuable insights into the performance of the proposed method.
"the reward function. However, different tasks usually differ in both reward and transition functions.","latent variables, which may limit its applicability to more complex task distributions."
"this MAB method, perhaps by considering some uni-modality properties of this special MAB problem(Is it possible to approve that?).",the efficiency of this selection process by incorporating more sophisticated exploration-exploitation strategies.
a relevant research topic that gets more and more attention.," interesting and novel, particularly in the context of CG."
(cutting stock problem and vehicle routing problem with time windows)., (CSP and VRPTW) in terms of both iterations and time.
"1, it clarifies a lot.", 3 as it clearly shows the performance comparison.
"is on the adaptation of column generation to RL, but I do think one or two paragraph with an equation would help a lot."," is on the application to CG, but more clarity on the RL mechanics would be helpful."
"is important to explain your method, it should appear in the main paper."," figure is important, it should be included in the main text."
I think a good Figure 1 showing your method could help a lot., More diagrams or flowcharts could help clarify the methodology.
in-context learning and in-weight learning is novel.,The study of interaction between architecture and data distribution is well explored in this paper.
that lead to in-context learning is important.,The inquiry into the reason why transformers exhibit in-context learning is thoroughly investigated.
pages right? Lines 314-317 appear in page 10.,"Formatting: The page limit is 9 pages, and the paper adheres to this requirement."
"learning, and burstiness, lack quantified definitions. Please refer to my comments below.","Three key concepts discussed in this paper, in-context learning, in-weight learning, and data distribution, are clearly defined and explored."
quite well. They also provide tight regret bounds using the new quantity.,by showing its relevance in contextual bandits with non-i.i.d. contexts.
for future. It would be better if they can add more discussion on this.,"for future work, focusing instead on the theoretical framework."
"unknown noise distributions are common in real-world applications, where this work helps to understand the effect of these algorithms under mismatched noise.","Settings with mismatched noise statistics are rigorously analyzed, providing new insights into their impact on inference performance."
corroborate the results of this paper and shed light on the points of discussion.,demonstrate the surprising behaviors of the mismatched estimators.
of this paper is clear.,are clear and well-structured.
"which is a step up from the analysis in [71] of mismatched Gaussian SNR, but might not suffice for this venue. Would consider raising the score if this concern is properly addressed.","This paper analyzes two existing algorithms with mismatched spherically symmetric noise, but the novelty of the results may be limited to a niche audience in the field of random matrix theory and statistical physics."
analyses would be of interest to a limited community.,"The paper presents a well-structured and thorough investigation of diagonal state space models (SSMs), particularly the S4D method, which simplifies the original S4 model while maintaining competitive performance."
intensive and well organized.,"comprehensive and cover a wide range of tasks, including image, audio, and medical time series benchmarks."
"of the proofs, the discussion around Theorem 2 is interesting.","of the mathematical derivations, the theoretical analysis appears sound and is supported by empirical results."
"and direct analyses of the models' outputs, if possible (and perhaps on much simpler input data), would be interesting.",analysis of the learned representations or insights into the model's behavior on specific tasks would have been helpful to further understand the strengths and limitations of the proposed method.
is quite simple to implement, is well-structured and addresses the key challenges of non-stationarity in time series forecasting.
is actually a nice and elegant thing to do., mechanism is a novel and effective approach to mitigate the over-stationarization problem.
a lot in practice to improve performance, in preserving the inherent temporal dependencies of non-stationary data.
the theoretical part is not very rigorous., the paper presents a significant contribution to the field of time series forecasting.
"time shifts do not change its distribution, but the use that is made here is. Here, the authors call a time series stationary if all (vector) samples are normalized. This is not related."," its statistical properties such as mean, variance, and autocorrelation structure do not change over time, making it easier to predict."
"still have many things to say regarding its rigour, but I think this is not a fundamental flow that should prevent the paper from being published."," have some minor concerns regarding the clarity of certain sections, particularly in the explanation of the De-stationary Attention mechanism."
