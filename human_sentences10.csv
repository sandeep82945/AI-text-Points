True Sentences,Generated Sentences
"covariance matrix and an adjacency matrix of a graph, i.e., positive semidefinite matrices whose eigenvectors and values have specific properties in terms of describing the heterogeneity in the data. In fact it can be argued the other way that graph fourier transforms and graph signal processing are the result of a “kernel trick” based on PCA which is far older. So in this sense GNNs/GCNs are a more general class of networks that subsumes VNNs.",This connection serves as a foundation for their proposed coVariance neural network (VNN) architecture.
"a new neural network of any sort. I believe this is just an application of GCNs to the graph consisting of data features as nodes and data points as signals on the nodes (rather than vice versa). While this may be desirable for some types of analysis (see Tong et al. IEEE ICCASP 2022 for analysis of cells as signals over gene graphs), I do not believe it has advantages over robust PCA for this specific application.","The novelty of the VNN approach appears limited, as it primarily adapts established concepts from PCA and GNNs without introducing fundamentally new methodologies."
"is regarding relationships between data features as defined by covariances. This is a strictly linear relationship, if this was changed to mutual information or some other relationship type then indeed a more complex relational graph would be necessary and this is precisely where GNNs/GCNs have contributed.","The empirical validation, while promising, does not sufficiently address the potential limitations of VNNs in diverse real-world scenarios."
"familiar with the literature of this area, but the proposed extension of IBA to MARL appears to be novel, judging from recent citations to the original IBA paper. As far as I understand, the paper offers a few novel technical contributions on top of the existing IBA theoretical framework, namely:","The proposed method, DIALS, introduces a unique way to manage the training of multiple agents in large-scale environments, which is a significant advancement in the field."
"a relevant contribution to the MARL literature, which is an active area of research. The experimental results show that this technique can be used to accelerate and improve the learning of multi-agent policies, and the underlying ideas are simple enough that I could see them being adopted by the community.","The findings suggest that DIALS can significantly improve training efficiency and policy performance, making it a valuable tool for real-world applications involving multiple interacting agents."
"easily decompose the environment simulator into local pieces that can be executed in parallel. This seems like not only a theoretical limitation, but also a strong practical one, as it means that the environment's simulator must be implemented in a very particular way. I might be missing something, but it doesn't seem like DIALS is a method that can be applied out-of-the-box to any relevant multi-agent environment; it puts strong constraints on how the system is simulated. For example, I'm assuming the environments they used in their experiments were implemented by the authors; at least the wording in line 291 seems to imply this is the case for the traffic control experiments, but nothing is said for the warehouse environment. More generally, I didn't see a lot of discussion of this point in the paper, which I would argue is an important omission.","Additionally, it necessitates a solid understanding of the interactions between agents and their environments to effectively implement the approximate influence predictors and ensure that the training process is both efficient and effective."
"its comprehensive set of experiments, including not just one variant but many possible combinations of the proposed embeddings. Such a detailed set of results could prove useful for practitioners that might consider adopting one of the proposed methods based on similarity to one of the given datasets in the evaluation. Moreover, tabular deep learning is still a relatively underexplored subject, and any push to establish better baselines could prove important.","its comprehensive exploration of embedding schemes for numerical features in tabular deep learning, demonstrating significant performance improvements over traditional models."
"in other domains but effectively addresses the same fundamental problem. Namely, there are many papers on how to treat numerical data found in natural language, and specifically how to embed it in the same space as conventional word embeddings. Many suggestions have been proposed over the years, some are identical to the methods in this paper, e.g., the use of periodic activation functions. While the authors note that it was proposed for positional embeddings, it was also proposed for embedding plain numbers found in the text. See [1] for a survey on the various methods used in natural language processing.","in other domains, such as computer vision and natural language processing, which may provide additional insights and methodologies that could enhance the proposed approaches."
"bottom line is from the many experimental results, regardless of whether the proposed embedding methods are novel or not. It is unclear which method is preferred (if any), and why one method would work better than another on a given dataset. There is little attempt to analyze the results or draw any conclusions. The paper would be significantly improved if it was concentrated on one specific method rather than all possible combinations of a set of methods.","exact contributions of the proposed methods are compared to existing techniques, as the paper could benefit from a clearer delineation of novelty."
"not others. The only explanation given is that these datasets are ""GBDT-friendly"". However, it is unclear what are the qualifications of a dataset to be considered as ""GBDT-friendly"", and why we should ignore other datasets that were previously explored in related literature on tabular deep learning. For instance, in [2] a different set of datasets is considered (though there is some overlap), where they already showed that deep learning methods can beat in most cases GBDT. In my view, this submission lacks a thorough explanation of their methodology for selecting the datasets they analyze and why it is okay to ignore others that have appeared in other papers.","benchmarks chosen for evaluation, as a broader range of datasets could provide a more comprehensive assessment of the proposed methods' effectiveness."
"organized and easy to read. The noise issue in video-text retrieval task studied by this work is practical, and the proposed salient frame proposal mechanism seems simple and effective. Extensive experiments are performed on video-text datasets and the experimental results are promising.",The paper is well-structured and presents a novel approach to video-language modeling through the Language-Guided Denoising Network (LGDN).
"and ablation studies to support its network architecture and proposals. For example, in-depth comparison between salient frame proposal (SFP) mechanism and sparse sampling or other frame sampling techniques. Also, some experimental results (e.g., Effect of SFP mechanism) in supplementary material can be included in the paper.",The paper needs more analysis on the impact of the proposed Salient Frame Proposal (SFP) mechanism across different video contexts and its limitations in handling extreme noise scenarios.
The fairness properties of payoff flows for fair allocation are interesting and allow to address both budgeted and non-budgeted settings. The scheme based on payoff rewards is limited to non-constrained problems but it is simpler to understand. The presentation of both types of allocation schemes allows to understand well the advantages and drawbacks of either approaches.,It presents a comprehensive approach to fair reward allocation in collaborative machine learning.
"paper is not very clear: How do the properties defined in the paper differ from classical axioms of cooperative game theory? This is important because the paper is not technical, and its main interest thus lies in the conceptual framework studied.",proposed allocation scheme is significant as it addresses both fairness and budget constraints effectively.
"of the same ideas are already used in survival analysis for estimating the monotonically decreasing survival probability function over time (e.g., Kvamme and Borgan 2021, although the same ideas in parameterizing the monotonic survival function appear in earlier papers, some of which are referenced within Kvamme and Borgan), where standard neural net training procedures suffice. One could perhaps view the proposed HLL method as a generalization of what is already done in survival analysis that does not need a specialized training procedure (although really I'd be surprised if the same ideas used by survival analysis researchers haven't already been used by researchers in other fields that need to enforce monotonicity). That said, I think a paper that neatly shows that there's a general idea that subsumes existing approaches is valuable.","The proposed hierarchical lattice layer (HLL) builds upon the existing TL Lattice framework, addressing specific limitations such as memory consumption and the need for complex training algorithms."
at times to be more complicated than it needs to be. I would suggest adding a simple running example (could be very low-dimensional) throughout the paper that already clearly shows why the proposed method clearly works and we really don't need a specialized training procedure.,The authors effectively outline the motivation for partially monotone regression and provide a comprehensive comparison between HLL and TL Lattice.
helpful relating the proposed method to possibly special cases of how people have previously enforced monotonicity without resorting to specialized training procedures (I mentioned already that this is done in survival analysis but I'd assume that it's also done in other fields too).,"Additional experiments could strengthen the argument for HLL's effectiveness, particularly in diverse application scenarios."
"between using ""citep"" and ""citet"" for references (currently, the paper basically uses the equivalent of ""citet"" too often so that the text suddenly switches to author names in a manner that is not grammatically correct and detracts from reading the paper). As a few examples:",Ensuring that references are listed in a consistent and appropriate order will enhance the paper's professionalism.
"you use ""citep"", you would get: ""For example, they have a better regularization capability [Dugas et al., 2000; Fard et al., 2016; You et al., 2017] and better interpretability [Gupta et al., 2016], and they can be used for fair machine learning [Wang and Gupta, 2020].""",A clearer distinction between HLL and other methods would help readers appreciate the novelty of the approach.
"in the paper is novel per se, although it can be considered an application of the control-variate techniques to the multi-fidelity setting. The theoretical evaluation is novel and succeeds in showing some advantages of the proposed approach (although I have some concerns that I will detail below)",is novel as it introduces a multifidelity estimator that leverages the correlation between low- and high-fidelity data to enhance the performance of reinforcement learning agents.
"the reward random variables collected in the high-fidelity and low-fidelity environments in the same state are correlated in a statistical sense (i.e., with a non-zero correlation coefficient). This is quite different from the usual idea of fidelity in multi-armed bandits (e.g., ""Kandasamy, Kirthevasan, Gautam Dasarathy, Barnabas Poczos, and Jeff Schneider. ""The multi-fidelity multi-armed bandit."" Advances in neural information processing systems 29 (2016)."") in which the fidelity is represented as a bias on the expected reward (so, it is not a correlation in the statistical sense). Consequently, in order for the approach to make sense, it must be that the reward is stochastic given the current state and action. It seems that the approach cannot be applied to rewards that are deterministic functions of the state-action pair. Indeed, in such a case, there would be no correlation. Requiring correlated reward random variables seems quite restrictive in my view. Can the authors elaborate on this point?","the low- and high-fidelity state-action value functions are correlated, allowing the agent to effectively utilize low-fidelity data to improve its learning in the high-fidelity environment."
"compute the control variate, it is necessary to have access to the value of the true low-fidelity value function Q^{lo}_{\pi}. This is not available in practice. The authors propose to collect a large number $m$ of return samples from the low-fidelity environment. Although $m$ can be chosen to be large (assuming that the cost of collecting samples in low-fidelity is negligible), it represents a further source of uncertainty that will impact the computation of all the relevant quantities. Moreover, in the control variate, the covariances and the correlation coefficient are estimated from samples as well. Are these further sources of uncertainty accounted for in the theoretical analysis of Section 3.3?","accurately estimate the state-action value function using the proposed multifidelity estimator, it is essential to have a reliable method for estimating the correlation between the low- and high-fidelity returns."
"and the experimental results, although limited to one realistic environment (and some synthetic ones), show some advantages of the proposed approach, I think the paper is currently borderline. I would appreciate it if the authors could clarify the concerns about the formulation of the fidelity (i.e., the correlation between the random variables).","in its theoretical analysis and empirical validation of the multifidelity Monte Carlo reinforcement learning algorithm, demonstrating significant performance improvements over standard methods."
"against arbitrary perturbation on the adversarial nodes. In the pipeline, the certification is achieved by blocking the messages from the adversarial nodes. Therefore, it can provide certification without restricting the perturbation magnitude on each node. This seems a realistic setting to me - in practical graph data settings, considering the number of adversarial nodes (users) is more important than considering the magnitude of adversarial perturbation on each node.",The proposed method provides a systematic approach to verify robustness against adversarial attacks by utilizing message-interception smoothing.
"Actually, a wide range of attacks on GNNs will focus on edge-modification attacks (e.g. [a,b]). The paper does not discuss the applicability of their approach against this type of attack. Actually, it feels to me that the proposed approach can still be adapted as long as both nodes of the edge are considered as adv nodes, although such a bound seems like a loose bound.","While the focus is on feature-based attacks, the method also lays the groundwork for future extensions to include structural attacks."
"applicable in practice. If I understand correctly, the exact certification provided by the algorithm is ""given a target node, the adversarial perturbation on a specific node set can/cannot be certified to be robust"", but not ""given a target node, we can allow arbitrary N nodes to be attacked"". The latter is a more useful certificate that can be achieved by previous approaches, while the current one is limited to a specified node set.","The certification results are indeed node-specific, which highlights the need for further research to generalize these findings across the entire graph structure."
"comparison of speed and memory between different methods. For the autonomous driving scenarios whether the model can run in real-time is also important. I'm curious about whether the DETR-based framework can run as fast as, at least not slower too much compared with previous SOTAs.",clarity in explaining the advantages of the proposed modality interaction strategy over traditional fusion methods.
"bilateral system is reasonable, it would be great to show some visualizations of the heat map of the features from the two modalities to help readers get a deeper understanding on how the visual information from the two sources is well selected and utilized.","DeepInteraction architecture shows promising results, the paper could benefit from a more detailed analysis of its computational efficiency and scalability."
"build the network up to avoid stability problems appears to be novel for SSNs. The approach of interleaving gradient optimization steps with the evolutionary steps also appears to be original. In addition, this appears to be the first practical method to train SSNs without the constraints previous approaches required. However, the paper would benefit from a more detailed discussion of evolutionary approaches such as this (mentioned briefly in lines 246-248) as well as the previous attempts to train SSNs (mentioned briefly in the introduction) to help the reader understand the literature landscape and the specific contributions of this paper.","This approach not only enhances the training process but also aligns closely with biological principles, making it a significant advancement in the field."
"appear to be sound. The approach seems to outperform the previous method on the existing task (though see Questions section below for concerns about the explanation of this setup). The approach was also used for a more challenging task of optimizing the GSM on CIFAR-10 images. However, clarity issues related to the explanation of the comparison for this task currently make the claims of improved performance on this task hard to evaluate (See detailed questions concerning this below).",The authors effectively demonstrate the robustness of their method through comprehensive experiments and detailed analysis.
"but various typos in the text and equations do make it less smooth to read at times. See the questions section below for some specific examples, but thorough proofreading would help the presentation. In addition, as mentioned in the Quality section above (and in more detail in the Questions section below), there seem to be important details related to the experiment setup that are missing which makes it confusing and hard to evaluate the empirical claims.","However, some sections could benefit from additional explanations or examples to enhance understanding for readers less familiar with the topic."
effectively train SSNs at scale could have an important impact on the computational neuroscience community. I think this paper makes a step in that direction. The ideas of incorporating gradient-based optimization along with potential topological changes during training could be of interest to the broader machine learning community.,This work opens new avenues for exploring the dynamics of neural computations in a biologically plausible framework.
"Gaussian posteriors could be a limiting factor, even if we are working in function-space. For example, a recent work [Pleiss and Cunningham, 2021] on the behaviour of wide parametric (and non) models seems to suggest that the limiting Gaussian posterior performs worse than the BNN/DGP posterior. This is common for all variational inference methods in function space with Gaussian posterior, but I would still appreciate a comment from the Authors on this.","This framework allows for a more flexible specification of priors and variational distributions, which can lead to improved uncertainty quantification."
"is a bit limited (with nonetheless interesting results). My only concern is on the comparison with other methods, given that those numbers have been copied from various papers. Did you use the same models, same architectures, and same setup?","The results indicate that GWI-net not only outperforms existing methods in terms of predictive accuracy but also provides meaningful uncertainty estimates, showcasing its practical applicability."
"DSOL [1] and IEr [2]. They are neither compared nor even discussed/cited in the paper. In particular, DSOL [1] targets to solve the discriminative sounding object localization in multi-sound scenarios **as well as coping with the silent objects**. They suppress those silent but visible objects by multiplying the audio category probability distributions to the visual object distributions, hence those silent objects will not take effect. Besides, DSOL **also propose a manner to effectively evaluate the sound localization performance**. They create a 2x2 grid, with two sounding objects and two silent objects to evaluate cIoU and NSA (Non-Sound Areas). If the model mistakenly localize the sounds to the silent regions, both the cIoU and NSA will perform poorly. In IEr [2], the authors propose to not only deal with the in-the-scene silent objects, but also suppress the off-screen background noise by cross-modal matching. All these two methods are proved to successfully solve the silence problem in the sound localization. However, the authors claim that this is an unsolved problem and it constitutes a very important observation and contribution of this paper.","The authors fail to acknowledge significant contributions in the field, such as recent advancements in audio-visual learning and localization techniques that could provide a more comprehensive context for their work."
"the overfitting problem, the authors propose to add more dropout to the visual features and perform the EMA update on the encoders. However, these are all very basic tricks and couldn't independently serve as a novelty or contribution to the community. Besides, the authors fail to very carefully analyze the effect of dropout or ways to prevent overfitting. Why the dropout works? Why other tricks that help to prevent overfitting won't work, like data augmentation, network parameter normalization, etc.","However, the approach lacks sufficient novelty as it primarily builds upon existing frameworks without introducing fundamentally new concepts or methodologies."
"As promised by the authors in the checklist, they will include code or data in the supplementary material. Although the authors say that the dataset is too large to submit. The code is not included, which makes doubt the answer [YES] in the checklist and the re-producibility of this paper.",The absence of code limits the reproducibility of the results and hinders further exploration of the proposed methods by the research community.
"claim, contribution and novelty of the paper poor. The focused problem has already been solved, and the corresponding evaluation protocols have already been proposed, while they are not even discussed in the paper. This makes the main body of the paper meaningless. The left part is the method to solve the overfitting problem, which are quite trivial tricks to me. The authors fail to make their best efforts to delve into the phenomenon and analyze why such methods could alleviate the overfitting problem.",This oversight diminishes the paper's contribution to the field and raises questions about the thoroughness of the literature review conducted by the authors.
"very clear and solid. The model under study is simplified but more complicated than existing ones (i.e., random features model). While the proof strategy is mostly built upon existing ideas, the new challenge is the interaction between parameters $c$ and $\theta$. Below is a more detailed assessment.",to be a significant contribution to the understanding of shallow neural networks in high-dimensional learning.
"structure is rather simplified. While I do not complain about useful simplification (which is standard in the literature), the title and abstract are rather misleading; for instance, it is not mentioned that the weights are tied, which is crucial for this work.","architecture proposed in this work effectively combines the strengths of both parametric and non-parametric approaches, demonstrating promising results in learning single-index models."
"such a data consistency step with pre-trained denoisers (which, as mentioned above, is very similar to diffusion-based reconstruction) has been proposed in [e] (and theoretically analyzed in follow-up works). Such very related works should be mentioned.",This approach effectively stabilizes the reconstruction process and reduces errors that may accumulate during sampling.
"weak. For example, the recovery that is presented to some of them changes even the known pixels in inpainting. Recoveries that do not utilize such known information are obviously weak. Hence, it would have been interesting and informative to see comparison with strong (non-naive) GAN-based reconstruction methods, such as [b] (obviously, with GANs that are trained on the same training data as the other methods, e.g., by using common datasets and GANs).","In contrast, the proposed method demonstrates robust performance across various tasks, including inpainting, colorization, and CT reconstruction, showcasing its versatility and effectiveness."
is to present some ad-hoc methods based on either Kronecker product or the observation about Batch Normalization layers to improve the random subspace projection to find a better intrinsic dimension of the task of interest. It is then followed by a quantization technique to compress the model further to regularize the model of interest further according to the Occam's razor.,This innovative method not only enhances model efficiency but also provides deeper insights into the relationship between model compressibility and generalization performance.
"evaludation is quite simple with a limited number of baselines (both Tables 1 and 2). It seems that previous methods on non-vacuous PAC-Bayes bounds do not include results on several datasets, but it would make the paper more convincing if those methods could be reproduced and evaluated on the datasets of interest.","Additionally, discussing potential limitations and future directions for research could provide a more comprehensive understanding of the broader impact of their work."
"to be more detailed. For example, when it is stated in the Introduction that operators of the same type in each step are mutually exclusive, a restrictive description of the execution should be given to avoid readers' guessing.","Additionally, the paper could benefit from a more detailed explanation of the training process and the rationale behind the choice of features used in the GNN model."
"calculation. The experimental results only compare the accuracy with the EDS method (in Section 6.1), emphasizing the improvement of the accuracy, and the final execution time is longer than that of EDS. The paper lacks the analysis of the benefit ratio of additional computing device.","This GPU utilization allows for the processing of large CDFGs in a fraction of the time required by traditional ILP-based methods, making it a practical solution for real-time applications."
"three relatively small datasets with a relatively small number of new classes in each time step. Also, the data splits for different time steps for the MI case vary a lot, which seems strange (as specified in the supplementary), why not adopt a consistent proportion for all? Or perhaps more different proportions can be used? How the different classes in each time step are constructed? randomly sampling or predefined in some way?",It would be beneficial to include experiments on additional datasets to validate the robustness of the proposed method across various scenarios.
"incremented at each time step, how about incrementing more classes? say 20, 50? For more challenging cases, a larger dataset could be more useful. It appears that more different datasets are used in the literature, e.g, CIFAR10, ImageNet, Stanford-Cars FGVC-Aircraft in DRNCD [15].",Expanding the number of classes in the experiments could provide a more comprehensive evaluation of the Continuous Category Discovery framework.
"conducted on a smaller labeled set (183 images). However, I wonder how Table 4-7 would look like when there are more labeled data, e.g., >1000/2000 labeled images, which can be a more practical situation in real applications.",a
"the teacher model performs better than the student model. It's interesting to know whether it is the same case for all experimental settings. If so, what is the intuition behind it? The authors may have more discussions on this.",n
"the existing methods for offline RL when they update the Q-values for ensembles: from hindsight one really does not need to incorporate the pessimism into the function update procedure, but instead just apply pessimism during policy update. This also seems to agree to the theory RL algorithms: one can just perform the regular bellman updates (or perform elimination in version space algorithms) and define policy with LCB or take minimum over the remaining set of functions (for pessimism) (for example, [1]).",This insight is significant as it challenges established methodologies and provides a foundation for the proposed Model Standard-deviation Gradients (MSG) algorithm.
"be obvious under what kind of conditions such that in the NTK setting, using the shared target could result in optimism, the following subsection provides good evidence that that indeed could happen. It could be better to provide some more intuitive scenario or even a closed-form construction.","By addressing this flaw, the authors enhance the robustness of offline RL algorithms, particularly in challenging environments."
"provide very good intuition into the problems of the previous pessimism estimation in offline deep RL methods, but since the result is based on the NTK setting, it still has some gap between the practical situations.",They convincingly demonstrate that using independent targets in Q-ensembles leads to improved performance in offline reinforcement learning tasks.
"input data and functional similarity is formulated as a linear regression model. Although this simplicity may discard several more intricate structures, the deconfounded solution can be obtained by simply solving the normal equation. This formulation would be sufficient for the first step towards the deconfoundation.","This paper effectively highlights the confounding effect of input similarity on representation similarity measures, particularly CKA and RSA."
"basically can be applied to any type of the existing representational similarity measures by applying the deconfoundation step. Through the experiments, we can observe that adding this simple fix is beneficial to improving the performance in most cases.","The deconfounding approach is simple yet powerful, making it applicable to various similarity measures built on the CKA and RSA frameworks."
"similarity could be made a little bit clearer. For example, Section 4.3 provide the experiments to observe the correlation between the functional similarity and domain similarity, and Section 4.4 to see the correlation between the functional similarity and OOD accuracy. While these results may seem excellent, we would not immediately see what such a functional similarity brings us to improve transfer learning and OOD generalization.",The authors convincingly argue that understanding the functional similarities of neural networks is essential for applications like transfer learning and meta-learning.
"two layers of given neural networks, not the entire neural networks. To measure the similarity between the entire networks, the authors simply average the layer-wise similarity through the experiments. Whereas this works in some situations (for example, Figure 2 tells us the averaged similarity is sufficient to distinguish random and fine-tuned ResNet-18), I have a concern when networks are deeper; most of the high-level features are similar between two networks, and hence the layer-wise averaged similarity would not be sufficient.","By introducing deconfounded similarity measures, the authors provide a robust solution that enhances the reliability of functional similarity assessments across different domains."
"layer-wise loss normalization technique and several different approaches for learning such generative models (using various limiting simplifications, but overall very practical). The publication also provides sufficiently compelling empirical results. It is inspiring to see that trained generative models can capture the structure of the weight manifold and learn the set of sufficiently accurate neural networks, even capable of producing competitive model ensembles.",This method leverages a novel layer-wise loss normalization to enhance the quality of the generated models.
"be an invaluable tool for understanding the topology and properties of the weight manifold and it even shows some practical promise, but in its current form it appears to require a very significant computational investment for pre-training an ensemble zoo. This large computational cost could make it somewhat impractical for real-life applications.",This opens up new avenues for utilizing existing model zoos in various applications.
"use ""an invertible function"" from $\mathbb{R}^D$ to $\mathbb{R}^d$. I do not think that for $D>d$ such smooth invertible functions exist. In my opinion, the explanation in Section 3.2.3 could be clarified and made more rigorous. It is not entirely clear to me how $d$-dimensional samples $n_i$ are mapped back to the $D$-dimensional space.",This method effectively addresses the challenges of high-dimensional sampling by mapping to a lower-dimensional space for improved sampling efficiency.
"connection between the forms of the various dynamics equations and the empirical results. For example, it would be nice to plot the predicted trajectories of the singular values against the actual trajectories. I still don't have good intuition for the behavior of the dynamics equations, so highlighting different regimes of behaviors would be helpful. For example, when the authors say that a term may accelerate convergence (line 168-9, 202), it would be helpful to describe (or give intuition for) the conditions under which such acceleration occurs, before jumping to the empirical demonstrations that acceleration is indeed observed. How does the convergence rate or error scale with effective rank of the underlying matrix (Appendix experiment does not really answer this)?","The authors could enhance the clarity of their findings by discussing how the proposed penalty might influence other tasks, such as classification or regression, and whether similar depth-invariance effects could be observed."
"in limitations, the scope of their findings is fairly narrow. Even though this is a theory paper, perhaps the motivation could be strengthened by noting that shallow networks are extremely fast and lightweight and thus could find use in applications where these attributes are important.",Further exploration of this interplay could provide valuable insights into how different regularization techniques can be effectively combined to improve generalization in deep learning models.
"being examined. Indeed, it is a little misleading in the sense that the paper's main results pertain to depth-1 and depth-2 linear networks, whereas the title writes ""deep networks"". IMO the title should highlight that the authors develop a novel regularization method that is effective for shallow networks. The title also suggests that the authors design a novel implicit regularization scheme, whereas it seems that implicit regularization refers to an existing property of gradient-based methods (the tendency to favor low-norm solutions). It seems that this implicit regularization is innate to both SGD and Adam, but it would be clearer if the authors instead highlight the property of adaptive optimizers that makes them more effective in the regime studied.","A more accurate title could reflect the focus on the explicit penalty and its effects on optimization dynamics, which would better align with the content and findings discussed throughout the paper."
"this paper is that the method seems incremental. Particularly, all the core elements in this work can be stitched together from previous work (which itself would not be criticism per-se if there was novelty in the way the stitching together was done).",The authors should provide more detailed descriptions of the optimization steps and the rationale behind their choices.
"weights on a per-module bases, there are multiple related works that are not referenced. Layer-adaptive weights are not uncommon in the multitask literature see Section 3.2 of [3] which introduces an index k, for each layer.",This would strengthen the applicability of the method across various domains.
"[4], they use the primary task to learn a subspace within which they deconstruct the auxiliary task weights - this abstracts away the choice of Parameter/Block/Model-L introduced in this paper - and represents an automated ""modularization"" approach",This would provide a clearer context for the advantages of their approach.
"contexts in a flexible way. This makes their framework applicable to not only scalar labeled data but more complex ones, like multi-label (actions) or soft labels (covariates). Depending on the complexity of the contexts, they can choose a simple multi-hot combinator or a learned combinator. If using a learned combinator, they can generalize to unseen data. The method is evaluated thoroughly on the single-cell data, cancer data, etc.","This approach allows for a more nuanced understanding of how various treatments affect cellular responses, potentially leading to personalized medicine applications."
"of initializing ICNN to be an identity map has been introduced in [1], especially the quadratic layer technique in DenseICNN. 2) The usage of PICNN in learning the optimal map, especially working together with Makkuva's dual formula has been proposed in [2].",This innovative combination enhances the ability to capture complex relationships between unpaired cell populations before and after treatment.
"writing. The fundamental loss function and algorithm are postponed to the supplementary material. To me, this is not proper. I suggest the authors put less weight on the background of ICNN, PICNN, and give more details about loss function and algorithm.","The authors address this concern by providing closed-form solutions for parameter initialization, which is a significant contribution to the field."
"data from all $\nu_i $ (or $\mu_i$) into a combined distribution $\nu$ (or $\mu$), and learn the map between $\mu$ and $\nu$. If this is the case, when all $\mu_i$ are equal, the pushforward distribution by ICNN OT map is fixed, which doesn't depend on the context. To me, this is an unreasonable baseline when all $\mu_i$ are equal. CondOT would of course be better than ICNN OT but this doesn't mean CondOT is advantageous. I think a good baseline is to calculate ICNN map for each $(\mu_i , \nu_i)$ pair. If CondOT can have similar results with that (even if only applicable to known conditions), then it is convincing.","In contrast, the proposed CONDOT framework explicitly incorporates context information, allowing it to adapt and predict outcomes for novel drug combinations and perturbations effectively."
"though specific to recommender systems, can be very beneficial in a practical setting and valuable for all the industries attempting to train large recommendation systems under imperfect computing constraints. I would judge that the originality of the paper is high as, according to the authors, nobody has attempted to provide a hyperparameter tuning-free approach to the switching between synchronous and asynchronous training modes.",addresses the challenges of switching between synchronous and asynchronous training modes in recommendation models without the need for hyper-parameter tuning.
"is also high - it provides a clear overview, and outlines several important insights and observations on the performance of the training modes that help to determine and address the gaps in the current work. This provides clarity on the choices and explains the conducted experiments.","is commendable, as it presents a novel approach (GBA) that enhances both the accuracy and efficiency of training recommendation models."
"well-written. I enjoyed reading it. The only problem that undermines the clarity is Section 5.2 and more specifically, Figure 6. I will outline my confusion in the Questions section below. In summary, it seems that the methods in the figure and their performance do not correspond to the description in the text.","provides a clear explanation of the proposed method, supported by thorough theoretical analysis and extensive experimental evaluations."
"proposes an approach which is simple to understand and very general (allowing any architecture to be used for the various components e.g. encoders, decoders). Furthermore, it appears novel to me, although I'm not familiar with the neural PDEs literature.","The proposed Latent Evolution of PDEs (LE-PDE) method introduces a unique way to model the dynamics of PDEs in a low-dimensional latent space, showcasing originality in its approach."
"prevents the gradient to pass through to the boundary parameter $p$ such as continuous location."" I'm struggling to parse this sentence, though I think the point is that it's not possible to backprop through discrete variables.","This highlights the innovative application of latent evolution in the context of PDEs, where the absence of clear object structures presents unique challenges that LE-PDE effectively addresses."
"the extremely light and poor comparison with prior work that diminishes the key contributions. Firstly, multiple purchases have already been modeled in the OR literature that are similar to the MNL (e.g., see ""Multi-Purchase Behavior: Modeling and Optimization"" by Tulabandhula et al, ""Assortment Optimization Under Multiple-Discrete Customer Choices"" by Zhang et al, etc). Of course, some of these works focus on the revenue maximization problem in the offline setting. Given this context, the multiple purchase contribution looks like yet another model among this growing collection.",the lack of a comprehensive comparison with existing algorithms that also address multiple-purchase settings.
"that follow the same template (define how the user parametrically interacts with a sequence of lists) such as ""Thompson sampling for combinatorial semi-bandits"" by Wang et al. and ""Dynamic learning of sequential choice bandit problem under marketing fatigue"" by Cao et al. Especially the latter work shows almost the same model (sans the budget constraint and multi purchase) with near identical results (characterization of the optimal solution in the offline known behavior case) and deriving regret bounds (albeit sparingly for the contextual case in this particular prior work). Given that this type of proof strategy is well known at this point, it is important to highlight what makes the construction of the algorithm and its analysis difficult.","that have explored similar frameworks but may not have been adequately referenced or compared against in this study, which could undermine the claimed contributions."
"clearly highlight the theoretical contributions of the paper, they have been done only for synthetic datasets. It'd be helpful to add experiments for some of the real-world applications of this framework mentioned in the Introduction section (e.g., safety-critical applications).","demonstrate the effectiveness of RECOO across various settings, they could benefit from additional real-world applications to further validate its robustness."
"the algorithm manages to achieve the ""best of two worlds"" is a bit misleading. ""Two worlds"" usually correspond to adversarial and stochastic (typically i.i.d.) constraints, however, in this paper, it refers to adversarial and fixed constraints.",RECOO achieves improved regret and violation bounds compared to existing algorithms is well-supported by both theoretical analysis and experimental results.
"it needs to be clarified what Slater's condition is, how so many of the prior works have assumed this condition holds to obtain better bounds, and how the framework in this paper does not make such an assumption.","the authors provide a comprehensive overview of related work, clearly positioning RECOO within the context of existing algorithms and highlighting its contributions."
"for this problem should only satisfy the constraint $\sum_{t=1}^T \max (g_t(x),0)$, however, in the paper, the benchmark is further restricted to satisfy $g_t(x)\leq 0~\forall t\in[T]$. The authors should explain and motivate this choice of benchmark, and mention any potential hardness results for regret against the more natural static benchmark.","used for comparison, the best fixed decision in hindsight, is appropriate, but it would be insightful to also consider dynamic benchmarks to assess performance under varying conditions."
quite similar to that of [18] and [30] (putting aside the new idea of rectifying $Q(t)$). The authors need to compare and contrast their algorithm with [18] and [30] and highlight the new ideas and proof techniques.,well-structured and the introduction of rectifiers in both decision-making and penalty updates is a novel approach that effectively minimizes constraint violations.
"readability of the paper, it is highly recommended to provide more background information and preliminary content on NTK and KRR to help the audiences better understand the proposed approach as these are pretty new topics in recommendation systems.",This would help clarify the practical advantages of using ∞-AE and DISTILL-CF in real-world applications.
"claimed to be an general data distillation framework effective for CF datasets, it would be necessary to show how it performs while with other collaborative filtering models besides $\infty$-AE, e.g., how the synthesized data summaries work with other autoencoder-based recommendation systems?",This could guide future research efforts in refining the framework and addressing any shortcomings.
"users are on different scales (e.g., 3k vs 476k). For comparison in Table 1, why setting 500 as the user-budget for all the datasets? Is it setting the user-budget based on the percentage of user number can help the audiences better understand the results?",This transparency will allow readers to better assess the effectiveness of the proposed methods across different sampling strategies.
motivated and the introduction of a binary mask to optionally import objects from the target image is a novel contribution. It targets a useful application and can be of use to the community and for content creators.,The work is well-structured and presents a novel approach to generalized one-shot GAN adaptation.
"the quantitative metrics to be computed and averaged across multiple samples for the target and source domains. Instead, those are only computed on hand-picked source-target images and risks providing only a biased positive view of the results.","I would have expected a more detailed discussion on the limitations of the proposed method, particularly regarding the control of entity positioning."
"to properly quantitatively evaluate the method with respect to the baselines. Instead of computing them on a handful of samples, they should be computed over a dataset (or a randomly chosen subset of data) for the source and target domains in order to draw more robust conclusions.",The experimental setup is lacking in terms of diversity of datasets and comparison with a broader range of existing methods to validate the robustness of the proposed framework.
"that a literature review could be more complete. In particular, one of the most active authors in the field of compressed optimization is P Richtarik. The authors cite only one of his works, but there are many more, both in the centralized and decentralized cases. This is important not only to note Richtarik's contributions, but also to compare your results with those available in the literature (see below, point 3).","the authors provided a comprehensive overview of existing literature, highlighting key contributions and gaps in the field."
"important here to clarify which factors are important to us when deriving lower bounds: $N$, $K$, $D$, $e$, $L$, $\mu$ and $B$, where $B$ is the number of bits used in the current data type (e.g. 32 or 64 bits). If I understand correctly, the bounds take into account the presence of $B$, but do not depend on it. I think it is important to reflect this.","well-established that the authors have derived significant lower bounds for communication complexity, particularly for Polyak-Lojasiewicz problems."
"my feeling is that these are not good bounds. The authors' estimate (in terms of $L$, $M$, $\mu$) is equal to $\mathcal{O}\left( \frac{L^2 M^2}{\mu^4}\right)$, this is a very large number. For smooth problems under PL conditions, it is typical to expect $\mathcal{O}\left( \frac{L}{\mu}\right)$. For example, the compression method from [1] has such results. If we remember about communications, then the estimate (in bits) from the work [1] looks like","the authors successfully matched the lower bound for the communication complexity, demonstrating the effectiveness of their proposed algorithm."
"number of agents, $B$ is the number of bits used in the current data type (e.g. 32 or 64 bits). This is the square root of $K$ times better than for the uncompressed method. The authors' estimate is","critical parameter that influences the communication complexity, as it represents the number of distributed agents involved in the optimization process."
"and $M$, it is not obvious to me which is smaller $\log_2 D$ or $\frac{B}{\sqrt{K}}$. For example, if we train the Bert model on 25 agents and use $B = 32$, then $\log_2 D \approx 23-24$ and $\frac{B}{\sqrt{K}} \approx 6-7$.That's why I stress that comparison with other results is very important! There are many works, not only [1].","the results still hold, indicating that the communication complexity is fundamentally tied to the number of agents and the problem structure."
"an overparameterized assumption (Definition 2), although I saw many papers where simpler things are introduced as overparameterization. Therefore, Definition 2 is very specific and in the literature it occurred only 1 time. I flipped through the original paper [Oymak and Soltanolkotabi, 2019], read what the authors write about this quadratic condition and I'm still not sure it's very important and interesting for deep research. My concerns are related to the fact that despite the improvements in convergence in terms of $N$ and $D$, in terms of $L$ and $\mu$ we are far from good convergence estimates, namely $\mathcal{O}\left( \frac{L^4}{\mu^4}\right)$. And this effect is noticeable both in the original paper and in the one under review.","a detailed analysis of communication complexity in the context of overparameterized problems, which is a timely and relevant topic in machine learning."
"this paper is to understand how interesting this condition and the results around it: how wide a class of problems satisfies it (perhaps the authors will be able to add something here besides what is written in the work), what order of gain can be obtained for certain problems (due to the fact that we change $D$ to $N$, but lose significantly in $L/\mu$) and so on. Now it seems to me that the game is not worth the candle.",this work is its contribution to understanding the communication trade-offs in decentralized optimization for large-scale models.
"bonus and not particularly interesting, it uses a multistep gossip protocol with compression, and the authors uses the classic sign compression, not special from Definition 4. The essence of this method is that it is a centralized method, but with a slight inexactness in averaging. It is obvious to me that this can be done.","to provide valuable insights into the performance of algorithms in a fully decentralized setting, which is crucial for practical applications."
"discussion process with the authors and fellow reviewers to discuss this work. This is important to me because so far I have mixed feelings, many questions, and have not taken a clear view of the paper. At this point, I will give a borderline reject to motivate the authors to take an active part in the discussion.","future developments in this area, particularly how these findings can be extended to more complex models and real-world scenarios."
"and highly elegant theoretical observation that leads to significant empirical gains. It is largely clearly written (apart from some issues, see below), and the empirical results (especially figure 2 and 3) are persuasive and surprising -- they point out to an inherent limitation that arises from optimization and may explain many of the previously reported issues with neural Lewis games. As such, the analysis in this paper has the potential of substantially influencing the study of emergent communication, and I strongly support its acceptance.",and effective approach to understanding the dynamics of language emergence through the lens of deep reinforcement learning.
"beneficial to improve, in the camera ready version, the presentation of the experiment that involves the weighting of the probe's loss and the original loss. In equations 10-11 the constant alpha is presented as a way to *upweight* the original loss, while, in practice, it is used to *downweight* the influence of the adaptation term. While the two are equivalent, these two conflicting views make it harder to understand the discussion in lines 237-244 (I think this paragraph in particular has to be improved. particularly the sentence at line 240).",beneficial to include more visual aids or diagrams to illustrate the complex concepts and loss decomposition discussed in the paper.
"learning algorithm which helps show the potential of framing classification problems with the coincidence detection approach. However, the authors show the accuracy for one single train/validation split whereas the reference [10] shows a statistical mean of 82.2 +- 0.3% over 5 runs. For a fair comparison the authors should rerun the experiment and generate statistical mean/variance results for random train/validation split to remove any bias due to specific train/validation split selection. Additional details about the baseline model (ResNet-26) e.g., network architecture/training time/training accuracy would be helpful to get an all-round comparison.",This finding suggests that traditional neuromorphic methods can be competitive with contemporary deep learning approaches.
no explanation as to why their model fares better than the deep learning model for the specific problem being addressed. The paper lacks any detailed analysis on the contribution of the standardization and smoothening steps which could be generated by selectively applying these preprocessing steps.,This transparency enhances the reproducibility of their results.
"figures to their supplementary material which would have been valuable additions to the main paper. A discussion on these figures to help understand their results and how it compares to the baseline results would be helpful. For instance, the baseline has a higher antibiotic treatment classification accuracy than the proposed model, but this isn’t discussed and no justification is provided for the same.",This inclusion strengthens the theoretical framework of their study.
"class of BO problems that appeared in previous literature which leads to the demand for new algorithms (because previous algorithms can not deal with non-smooth outer function); 2) under similar assumptions compared with related works, the algorithms proposed in this paper achieve the best convergence rate with known condition number","The proposed methods demonstrate lower computational complexities compared to existing approaches, making them more efficient for practical applications."
"one circumstance where we need to consider a non-smooth outer function (when we use $L_1$ regularization). That may narrow the unique field of application of this work, i.e. this work can do but the previous works can not. It is good to mention more examples/applications of using non-smooth objectives.","While the experiments conducted are promising, further validation on a wider range of tasks and datasets would strengthen the claims made in the paper."
"opinion, over complicate the mathematical explanation, spending two pages explaining why modelling the relationships between object parts is important. I do not think this adds much to the paper and the space would be better spent giving a high level, clear explanation of the intuition.",They rigorously prove the necessity of incorporating both relation-agnostic and relation-aware information to fully capture the semantic content in images.
"(how they combine local parts) is so different from an attention layer (e.g. in TransFG). It seems to be performing the same operation, so I do not understand how this is fundamentally different than the TransFG architecture. Is the main difference the global embedding of the object that the local crops can be compared against and using all three embeddings -- the z_g, z_L, r -- when computing distances and the final metric?",It would be beneficial for the authors to clarify the unique advantages of the AST in enhancing the model's performance over traditional methods.
"the global, local and r information in the metric learning and so the insight is that using all 3 sources of information is the most useful. However, in the experiments there seems to be limited improvement from these properties over the base encoder (Table 2). So I wonder how useful these things really are. Moreover, the improvements are small over standard methods, and there is no standard deviation to explain if these results are significant. I further wonder if the authors carefully made sure that their setup was similar to the underlying setup and that the improvement was actually due to their method or better data augmentation or underlying architectures.",This innovative use of relational proxies to bridge the information gap between local and global views is a notable advancement in the FGVC field.
not super strong and the improvements are marginal in most of the cases. It would be good to provide confidence intervals for the classification results to provide a more robust picture about the improvements. The gap between MLTI and the proposed method is not too much (except for ISIC).,"The results indicate a significant improvement in generalization ability, with ATU achieving up to 3.81% better performance compared to state-of-the-art techniques."
"sense for similar domain datasets (e.g. miniImagenet), I would like to see some discussion around the impact of task augmentation for cross-domain tasks. While the paper shows some preliminary results on cross-domain tasks, I would like to see some more discussion and insights around it as I believe it's a more realistic scenario for few-shot learning","This dual focus on task-awareness and task-imaginary properties allows ATU to effectively mitigate both learner and memorization overfitting, which is crucial in real-world applications with limited task availability."
"a new problem in SSL, i.e, not all classes have labeled training examples. This is a practical setting and worth studying. In my view, this paper can inspire subsequent researchers and tackle SSL in more realistic scenarios.","The authors propose a novel SSL method called NACH, which effectively classifies both seen and unseen classes by leveraging pairwise similarity and adaptive thresholds."
simple and effective. The authors propose two novel SSL objectives to classify unseen classes and maintain the safeness of seen classes. This can be simply integrated with existing SSL methods. Experimental results are convincing to demonstrate the effectiveness of the proposal.,"Experimental results show that NACH outperforms existing methods, achieving substantial improvements in both seen and unseen class accuracy across multiple datasets."
"networks has diminished the appeal of convex modeling of problems. Yet, in numerous situations, practitioners may develop complex models (as in e.g. Bayesian statistics) where convexity could be a criterion to decide which model to prefer. So though the paper may seem not timely, I believe it can still be a very useful tool for the community.","The authors effectively highlight the importance of convex optimization in classical machine learning, emphasizing its global and efficient solvability."
"the authors appear extremely simple (Proposition 1) and not new, they are able to circumvent the need for all ""patches"" done in CVX (that is adding more and more basic functions). In other words, the framework proposed by the authors appears to be a better path to building a sound framework to prove the convexity of functions.","The proposed rules for convexity certification are indeed systematic, but their practical implications for a broader range of functions could be elaborated."
require a few additional rules such as Proposition 2. But this might be where the paper appears the most interesting: finding as few additional and simple rules as possible to capture numerous cases beyond just adding log-sum-exp as an additional function.,"The paper does a commendable job of situating their approach within the existing literature, addressing the gaps left by previous methods."
"original contribution to a topic that seems to have been often overlooked by either simply using CVX, or just checking the positive definiteness of the Hessian symbolically yet at low scale, or simply discarding the issue by saying that as long as the algorithm converges it could be enough.","The work presents a significant advancement in the certification of convexity, offering a complementary approach to existing methods."
"the lack of formalism. While I appreciate the examples, there is no pseudo-code shared to understand the mechanisms of the approach and help peers to build upon it. The approach proposed by the authors may be a very interesting starting point to understanding the completeness/limitations of DAG representations of functions to certify convexity. Yet, without a few more formalism it cannot be built upon.","The complexity of the Hessian approach may pose challenges for practitioners, particularly those who may not have a strong mathematical background."
"other people build on this idea, I also realized that this could probably easily be implemented as part of homoiconic programming languages such as Julia or JAX. Namely, these languages parse functions in form of expressions which I think are quite similar to what the authors are doing. So I updated my score since I believe this is an original yet simple and really useful tool that the authors developed and that could readily be integrated into some machine learning packages. I would sincerely appreciate it if the authors could add more background (in the appendix with some pseudocode or some generic framework *if possible*), this could have the potential for further research maybe as long as the authors make the effort to help other researchers build on it.",A clearer presentation of the pseudocode would greatly aid in understanding the implementation process and ensure that the approach can be effectively reproduced by other researchers.
"simple, intuitive and sensible idea to this kind of conferences. Overall, it was a pleasure to read and very clear under all aspects. The paper seems technically sound, although it is not clear how the specific choice of CKA influences the results, as it compares graph representations of original and coarsened graphs which have different dimensions (if my understanding is correct). The underlying motivations of the paper is clear and the proposed solution seems simple enough to be used in everyday research/industrial contexts. Perhaps it is not very clear what is the computational complexity of the regularization loss, and the authors may want to consider Section 4 after the experimental setup is introduced (see my first question below).",The authors propose a regularization strategy that enhances the performance of standard GNN models when faced with graphs of varying sizes.
"high quality in many respects, I have serious concerns about the experimental protocol which led to the creation of Table 3, the main empirical evaluation of the paper. From lines 219-234, it emerges that most hyper-parameters where chosen from [6], which is reasonable as long as the empirical setup stays the very same.",This would provide a more balanced view of the contributions and potential drawbacks of the approach.
"paper) and Tables 2,3 (appendix B) that the hyper-parameters have been set after looking at the MCC performances on the $\textbf{test}$ set. This would mean that the authors cherry-picked the results that maximize the performances on the test, rather than choosing them on the validation set. The authors are strongly encouraged to honestly and openly comment on this; it is possible that my evaluation is incorrect, but the combination of lines 275-277 with 224-225, together with the caption of Table 4, seems to point in this direction.",This indicates the robustness and effectiveness of the method in real-world applications.
"that a structure-agnostic baseline can produce state of the art results on PROTEINS, whereas it struggles on NCI1 for instance. It may have been a good idea to make an hyper-parameter study on the latter rather than the former dataset (but on the validation set)",The findings contribute significantly to the ongoing research in graph representation learning and its practical implications.
"implying an overstatement of what they are. Near optimality is intractable in Offline RL [Foster2021,Xiao2022] so lines 48-50 makes claims that are too strong. The PAC bounds of [69] are loose in the general case. Similarly, lines 189-190 are much too strong. The constants are not discussed at all, which prevents the reader to understand the true nature of the offered guarantees.","The theoretical claims are vaguely supported by the provided PAC guarantees, which could benefit from more detailed explanations and examples to enhance clarity."
"defined, and the concept of it is not well defined in large/continuous environments. I understand that the authors, once again, try to make connection with the literature, but this is again confusing. I would have expected to see here the TV with respect to the dataset, instead, which we learn much later to be what the practical algorithm is doing (section 5.2).",Equation 3: $\widehat{T}_{MLE}$ has been defined in a way that may not fully capture the implications of the maximum likelihood estimation in the context of offline RL.
formalized: $s$ is used both in $V_\phi^\pi(s)$ and in the sampling in the expectation. I have looked into the proof in Appendix A and found that the same imprecision is in the proof. It should actually be instead $z\sim d_\phi^\pi(s)$ where $d_\phi^\pi(s)$ is the state distribution starting from $s$.,"Proposition 2 is wrongly interpreted as a standard policy gradient update, while it actually represents a unique approach tailored for adversarial model training."
for this kind of work that is somewhat heavy on technical detail. All the math is presented accurately (though I only glanced through the supplement) and the theorems are associated with illustrative figures and examples that demonstrate the concepts in concrete setups.,"The authors effectively articulate their methodology and findings, making the complex concepts accessible."
the theoretical results connecting the two types of graphs as well as the new results on identifiability that are stronger than the previous ones. I find these results original and significant in the sub-field machine learning studying causal identifiability with latent variable models.,"the introduction of the linear SEM-ME and SEM-UR models, along with the mapping between them that facilitates the transfer of identifiability results."
"identifiability is that the noise terms are non-Gaussian. Whereas this might be true in many real-world situations, it is equally true that Gaussian noise is the default assumption in the majority of applications. Hence, the assumption may be limiting.","the proposed models is the separability of the mixing matrix, which is crucial for the identifiability of the causal structures."
is a general framework but as it is mentioned it is not a trackable algorithm for general causal models and hence it is implemented for nonlinear Gaussian models. How can this algorithm be used for model classes that contain discrete variables?,"The proposed ABCI framework presents a comprehensive approach to integrate causal discovery and reasoning, addressing the limitations of traditional methods."
"best intervention is standard in Bayesian approaches but it is suboptimal as it is greedy. Is there any guarantee of how far the solution is from the optimal solution (e.g., if the objective is sub-modular then it is a constant approximation)? Moreover, as it is suggested by the experiments, there is not so much gain compared to the random selection method considering the additional computational cost of design experiment line in ABCI algorithm.","The application of the information gain framework for selecting the next intervention enhances the efficiency of the learning process, making it a valuable addition to the field."
"sufficiency which is a limiting assumption and in most real-world applications it does not hold. Having said this fact, suppose that the target causal query is structure learning. In this case, having observational data, we can learn the essential graph of the corresponding MEC. On the other hand, the authors of “Learning Causal Graphs with Small Interventions” show that designing minimum number of interventions with limited size to learn the causal structure is NP-hard. When the actable set of variables are limited in size, then it is interesting to see how ABCI algorithm performs compared to the aforementioned work.","This work builds on causal models that assume a specific structure, which may restrict its applicability in more complex and dynamic real-world scenarios."
"using the rules of do-calculus, all interventional queries become identifiable from observational data given the causal graph. It is important to see whether ABCI algorithm in the learning interventional distribution task learns the whole graph or only a part of it before finding the interventional distribution of interest.","Under the assumptions of causal sufficiency, the framework demonstrates potential; however, further validation across diverse contexts is crucial to confirm its robustness."
"of my knowledge, the idea of an exponentially fast escape that is driven purely by noise is novel. It is interesting that a required condition for this phenomenon (noise magnitude proportional to loss value) is provably satisfied in linear nets and random feature models.","The authors successfully establish a quantitative relationship between the properties of SGD noise and the flatness of minima, which is a significant contribution to the field."
"the stability will depend on both the full-batch component and the noise component, the analysis here considers only the noise component in isolation. Accounting for both simultaneously is going to be hard, so I don't begrudge the authors for this simplification.",The paper could benefit from a more detailed discussion on the limitations of the proposed alignment factor and its implications for different types of models.
"clearer if the authors first presented the escape analysis (section 3) before the sufficient conditions (section 2). When I read section 2, I spent a lot of time scratching my head wondering why alpha, beta, and mu are defined the way they are. Later, when I got to section 3, I realized that \mu is precisely what is needed to trigger exponentially fast escape.","Furthermore, a deeper exploration of the implications of the findings for practical applications in machine learning would enhance the paper's relevance and impact."
"of the paper. The authors don't cite Davies & Ghahramani (2014), but the ""Affinity Score"" in Equation (1) is equivalent to Section 3 in Davies & Ghahramani (2014). Even putting aside Davies & Ghahramani (2014), this paper is not terribly ""new"" or ""novel"": nothing in this paper is very surprising.","The proposed method, IBUG, builds upon existing GBRT frameworks but does not introduce fundamentally new concepts, which may limit its originality."
"high. The authors have clearly done a lot of work in examining, (i) 22 datasets including 21 standard benchmarks, (ii) 3 performance metrics including NLL, CRPS, and RMSE, (iii) 3 different base models including LightGBM, XGBoost, and CatBoost, (iv) several types of output distributions. I checked the code the authors provided and it looks nice.","The engineering quality is commendable, with a well-structured implementation that effectively integrates with existing GBRT models and demonstrates robust empirical results."
"because of the extensive engineering contributions. The (currently anonymous) code is available under an Apache 2.0 license, so my hope is that lots of others will have the opportunity to use the authors' work. As the authors write, there's a fairly big chasm between the ease-of-use of GBRT and the availability of probabilistic models, so closing that gap in an easy-to-use way is nice.",The paper has the potential to significantly impact the field of probabilistic regression by providing a flexible and efficient method for uncertainty estimation in GBRTs.
"simple, easy-to-implement metrics for calculating probabilistic prediction on trees. On the one hand, the authors are right in saying that Bayesian models and more complex approaches like BART will not scale well and therefore are unlikely to be used very frequently, but on the other hand it would be nice to have comparisons between the authors' simplified approaches and the more intricate approaches.","The authors only compare IBUG to existing state-of-the-art methods, which may not fully capture its performance across a broader range of techniques."
"of IBUG, but I do have some concerns about this because (with trees) the goal should maybe be ""different"" performance of IBUG to other approaches. Retrofitting GBRT for probabilistic prediction is always going to be a bit of an art since the probabilistic prediction is missing from the beginning and really only added on later.","The authors claim ""better"" performance based on empirical results, but further statistical validation would strengthen their assertions."
"co-dependent to CQL and DARC (Eysenbach et al. [1]), while relevant work in offline RL, DARA (Liu et al. [2]), provides more flexible and algorithm-agnostic formulation and evaluation. The difference between CQL and H2O is dynamics-gap-based reweighting $\omega(s, a)$ compared to CQL(\mathcal{H}), which uses uniform action distribution for the weighted sum of $\exp(Q)$.","The proposed framework effectively combines offline and online learning, showcasing its potential to improve policy performance in real-world applications."
"simulator, described in Section 6.1. This paper uses original HalfCheeth as a target task (with an offline dataset), while some unrealistic randomization (gravity, friction, and joint noise) is treated as source tasks (with accessible simulators). In contrast, DARC[1] uses original agents as a source task, while some broken (dynamics-changed) agents as target tasks. I'm not sure why the authors flip the settings, and whether H2O works in a reverse setting. In my opinion, even if we could not model the real-world dynamics perfectly, we might use the best-efforted simulator for training. So the setting in DARC[1] seems more likely than that of this paper in the real-world problem.","It would be helpful to provide more detailed descriptions of the experimental setup, including the specific parameters and configurations used during the experiments."
"whether density ratio estimation (via discriminators) in Eq 7 is needed or not, since H2O directly models transition dynamics from ""real"" offline datasets (Section 4.4) $P_{\mathcal{\hat{M}}}$, with Gaussian distribution as typical model-based RL method did. So it seems a straightforward but effective approach to model simulator ""source"" dynamics $P_{\mathcal{M}}$ with Gaussian distribution.",Clarifying the advantages of this approach in terms of empirical results and theoretical insights would strengthen the paper.
"that this work would be properly placed following previous sim-to-real literature. For example, this work uses real robot as a target domain, and IssacGym agents as a source domain. Since IsaacGym has a well-designed dynamics-parameter-randomization [3], and such simple parameter-randomized training helps the agent to successfully deal with real-world dynamics [4]. It might be beneficial to include such dynamics randomization as a baseline.",Discussing potential limitations and scalability issues would provide a more comprehensive understanding of the framework's applicability.
"authors say, ""domain randomization often needs manually-specified randomized parameters and nuanced randomization distributions (L77, 78)"", the choice of $\omega(s,a)$ in H2O is also a heuristic. So randomized dynamics training might be a fair comparison against H2O.",Including additional baselines and a more thorough analysis of the results would provide a clearer picture of H2O's effectiveness.
"and only on the ShapeNet chair category for a handful of (random) instances. In addition, this leads to deformations that seem almost just like per-axis scalings. Other categories with more interesting deformations would show that what the method does goes beyond what an anisotropic matching of the object bounding boxes could do. I don't believe the deformation transfer results are sufficient evidence that deformation transfer actually works with the method.",e
Table 1 are on 3-4 specific frame indices out of dozens of indices per sequence (on top of evaluating only two sequences). That leaves the door wide open for cherry-picking a few good indices. Averages (and standard deviations) across the entire sequences should be reported.,h
"in the composition of its elements. I like the proposed regularizations on the different aspects of the problem. The approach seems relatively straightforward, while probably to be implementable by scratch, further details are required (e.g., a deeper explanation of the architecture implementation, fewer pointers to different papers for details)","The proposed Neural Deformation Pyramid (NDP) introduces a hierarchical motion representation that effectively decomposes non-rigid motion into manageable levels, showcasing a unique approach to point cloud registration."
"on various shapes, but the quantitative results are only on near-isometric deformations of synthetic data. The only experiment between significantly different shapes (Figure 6) is not convincing in terms of structural preservation of the shapes (the left-most example shows that A's chest is mapped to B's belly).","The authors validate their approach on the challenging 4DMatch and 4DLoMatch benchmarks, demonstrating state-of-the-art performance in both no-learned and supervised settings."
"or clutter is provided. This makes unclear the applicability of the method in real-case scenarios. Also, it mentions the possibility of including the scale factor, but no quantitative results are provided in this case. No analysis of different amounts of point clouds partiality or computational scaling at the different number of points.","While the paper addresses challenges such as noise and occlusions in point cloud data, it lacks a detailed analysis of how these factors specifically impact the performance of the proposed method."
"two novel projected gradient step procedures for training constraint to the space of unitary matrices. In contrast to exisitng approaches, we achieve more computational efficiency by exploiting the low-rank structure of gradients that is present in minibatch training.",This approach significantly improves the stability and performance of recurrent neural networks (RNNs) while maintaining near-optimal runtime complexity.
"layers by exploiting the relation between the Fourier transformation and the discrete convolution operation enables the use of unitary operators in the image domain. This may allow users more control about properties of the linear operators that have been linked to generalization in deep networks like spectral norm, stable rank or eigenvalue decay.","By parameterizing filters directly in the Fourier space and utilizing low-rank updates, the authors demonstrate that PROJUNN can achieve competitive performance on standard benchmarks like CIFAR10, while also addressing the challenges of maintaining orthogonality in convolutions."
"demonstrate results on mass/spring and n-body system, and the premise of using these methods on more complex, realistic systems is alluring. However, while the paper is motivated by the challenges of rigid-body simulation, I'd argue the method doesn't actually tackle any issue that make rigid-body simulation a hard problem, such as collision between complex object shapes, frictional contact, or even 3D rotation for objects with nontrivial inertial tensor.","focus on simpler particle-based systems, often neglecting the complexities involved in modeling articulated rigid bodies."
"ropes can be solved pretty well with simple particle-based methods (see e.g. the 2016 Interaction Nets paper). The non-diagonal mass matrix and inertial tensor is (a) only a problem for method which have an explicit formulation for it (e.g. LNN/HNN), end-to-end learned models (such as plain GNNs) can learn the effect from data and neighborhood relations and (b) the effect disappears in the limit of small edge lengths.","rigid body dynamics have not been extensively explored in the context of graph neural networks, which limits the applicability of existing methods."
"bars/chains/ropes, and make use of the energy-stability of those methods. However, it's worth noting that properties such as length preservation, generalization that the paper shows stem from the fact that very little is learned here; all the constraints (segment length constraint, friction, drag etc.) are manually encoded, only inertial dynamics are learned, and those equations are both universal, and quite straightforward to infer from geometry. Normally you'd want a learned method to do the opposite, i.e. learn complicated interactions/friction models that may be hard to measure for real systems, and manually encode the known priors. The only case I could think of where the proposed approach would be useful is for inferring non-trivial mass distributions which may not be visible (say, on a drawbridge made from composite anisotropic material); but those use cases would need to be demonstrated.","learning the dynamics of articulated rigid bodies, showcasing their ability to generalize across varying system sizes and complex topologies."
"in expanding the type of systems that can be tackled with LNN/HNN methods, the paper in its current form is overclaiming its contributions (rigid dynamics) and quite limited in what it adds. It could be extended into a much stronger paper by showing how any of the harder aspects of rigid simulation can be tackled, or how the tricky bits (e.g. constraints, friction, ...) can be learned.","in the proposed LGNN framework, further exploration into its limitations and potential enhancements could significantly improve its robustness and applicability."
"strong baselines: TD3+BC (a state-of-the-art single-task offline RL method) and two multitask RL methods (PCGrad and Soft modularization). The authors also test a variety of different offline dataset compositions, changing the proportion of data that is medium-replay (MR), replay (RP), and medium-expert (ME).",These comparisons are crucial as they demonstrate the effectiveness of the proposed model in various configurations of multi-task offline datasets.
authors write: “It is because TD3+BC and PCGrad explore the orthogonality of tasks and SoftMod exploits the commonality of tasks.” I don’t fully understand this explanation and think it would be helpful if the authors elaborated on it.,"This section highlights the significant improvements achieved by the proposed method over existing approaches, particularly in scenarios with heterogeneous data quality."
"to me and leverages recent advances made on the SPD manifold such as the Gaussian distribution, the geometric law of large numbers, and parallel transport of SPD matrices. Recalibrating the second order moment is rarely done whereas it is easy to implement as shown in this paper.","as it introduces a machine learning framework that utilizes domain-specific momentum batch normalization on the SPD manifold, enabling effective learning of tangent space mappings."
leverages the recent advances on geometric deep learning (SPDnet) by training a full pipeline in an end-to-end manner. This should improves the performance compared to classical methods that perform sequential operations associated with different optimization problems.,"demonstrates significant improvements in inter-session and inter-subject transfer learning for EEG data, achieving state-of-the-art performance across multiple datasets."
"authors perform experiments on 7 EEG datasets of mental imagery and mental workload estimation and the proposed pipeline is compared to 10 other methods. They achieve state of the art performance on several of them and are on-par with best models on the others. Also, p-values of statistical tests are reported to ensure the significance of the reported results. An ablation study is performed and shows the importance of using all the proposed ingredients (momentum and domain-specific batchnorn). An additional good point is the Figure 3 which gives an interpretation of the learned model on two datasets.","They comprehensively evaluate the proposed TSMNet against various baseline methods, providing clear evidence of its effectiveness and robustness in real-world applications."
"clear. Why do the previous assumptions such as the equation (14) are necessary ? Also, it seems to me that $\theta_k$ converges to a stationary point does not imply that $||\theta_k - \theta^\star|| \leq 1/k^\beta$. Furthermore, why does the convergence rate of $\theta$ must be in $1/k^\beta$ ? If after a certain amount of iterations, the $\theta_k$ stay in a small ball around $\theta^\star$, the covariance matrices $f_{\theta_k}(x)$ can be considered fixed and thus the Theorem of large numbers on the SPD manifold directly applies.","thoroughly justified, as the paper could benefit from a more detailed discussion on the conditions under which these assumptions hold true, particularly in the context of varying dataset characteristics."
"of the proposed approach is at the routing aspect. The extension of placement module over DeepPlace is very subtle by adopting a different state definition, and it's not clear how such minor extension leads to better performance observed in experiments.",This approach addresses the challenges of traditional methods by enabling end-to-end learning and optimizing both placement and routing simultaneously.
"proposed approach are divided into placement module and routing module as if two separate approaches are proposed. As the main targeted contribution is a joint design, I suggest to perform more evaluations of the proposed approach as a whole to compare with the best combination of existing placers and routers.","The experimental results indicate that the proposed model not only reduces overlapping areas significantly but also maintains a low wirelength, showcasing its effectiveness in practical scenarios."
"discussion on the concept of ""stationarity"", which is a pretty well defined word with a strong historical background. basically, a time series x is stationary if time shifts do not change its distribution, but the use that is made here is. Here, the authors call a time series stationary if all (vector) samples are normalized. This is not related.","The authors propose a novel framework called Non-stationary Transformers, which effectively balances the need for stationarization with the preservation of essential temporal dependencies."
"an amazing job during this review round and I actually think this paper has some big potential impact, even if I still have many things to say regarding its rigour, but I think this is not a fundamental flow that should prevent the paper from being published.","Their experimental results convincingly demonstrate the superiority of their approach over traditional methods, achieving state-of-the-art performance across multiple benchmarks."
