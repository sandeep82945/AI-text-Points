True Sentences,Generated Sentences,Precision,Recall,F1 Score,Edit Distance
of the proposed method by comparing with existing methods.,of the proposed adaptive multi-distribution knowledge distillation (amdkd) scheme.,0.82945806,0.8957897,0.86134875,51
is overall well presented.,shows promising results in enhancing cross-distribution generalization.,0.8603504,0.89170766,0.87574846,57
"policies, architectures, hyper-parameters, etc. it is thus doubtful if the proposed method is generalizable enough when applied in practice.",the student model with adaptive strategies and leveraging multiple teacher models for knowledge distillation.,0.86101353,0.85366374,0.8573229,106
"some special implications. if it is, there should be some explanations about it; otherwise, it could be presented in a better way, or maybe just leave it unfilled.",been clarified whether these results indicate optimal solutions or if there are any specific conditions under which these methods were evaluated.,0.840298,0.8386953,0.83949596,119
variance with respect to the hyper-parameters.,of the proposed method compared to traditional heuristics on larger problem sizes?,0.84604406,0.8582326,0.8520947,62
be argued the other way that graph fourier transforms and graph signal processing are the result of a “kernel trick” based on pca which is far older. so in this sense gnns/gcns are a more general class of networks that subsumes vnns.,this observation highlights the potential for leveraging graph-based techniques to analyze covariance structures effectively.,0.86826515,0.82852346,0.84792894,176
have been robust forms of pca that have been developed including variations of principal component pursuit.,the note that this results in a stable pca is interesting although over the years there have been various approaches to address pca's instability.,0.8709264,0.85873705,0.8647888,106
"than vice versa). while this may be desirable for some types of analysis (see tong et al. ieee iccasp 2022 for analysis of cells as signals over gene graphs), i do not believe it has advantages over robust pca for this specific application.",i believe this is just an application of gcns to the graph consisting of data features as nodes and data points as signals on the nodes (rather than introducing fundamentally new concepts).,0.8477445,0.83425665,0.84094656,170
of gcns/gnns to feature covariance matrices and situations where that could be useful.,this would clarify the contribution and situate it within the existing literature on graph-based learning.,0.85841733,0.8429866,0.850632,79
to mutual information or some other relationship type then indeed a more complex relational graph would be necessary and this is precisely where gnns/gcns have contributed.,"this is a strictly linear relationship, if this was changed to accommodate non-linear interactions, it could enhance the model's applicability.",0.875217,0.8381226,0.8562683,123
again i don't see much advantage in using a neural network for this kind of linear operation.,this raises questions about the novelty of the proposed approach in the context of existing dimensionality reduction techniques.,0.88506866,0.87294847,0.8789668,94
wildlife conservation. the technical work required to extend the problem setting in this way is significant since one must now account for how workers with different (arm/state-specific) expertise may interact (and influence arms' transitions) over time.,"the authors effectively highlight the importance of addressing the heterogeneity of workers in intervention planning, which is a significant advancement in the field.",0.87997353,0.8557828,0.8677096,178
"of information is available should be explicitly stated and justified by the availability/""learnability"" of this type of information within the domain(s) of interest. this will be particularly important in settings where there is heteroskedastic uncertainty about workers' intervention effects.","while it is conventional to assume each arm's (worker-agnostic) transition matrix is known by the decision-maker in the planning setting, the assumption that this additional level of detail is also observable may limit the applicability of the proposed method in real-world scenarios where such information is often uncertain or incomplete.",0.85169595,0.8526836,0.8521895,250
and assumptions before introducing the new solution,"by formalizing continual learning through the lens of feature extraction, which allows for a clearer understanding of the challenges and requirements in this domain.",0.85749096,0.8876195,0.8722951,131
learning process. a direction which is heavily unexplored in the cl community due to the difficulty of the topic.,"learning process, particularly focusing on the limitations and guarantees of algorithms in the context of linear feature mappings.",0.8737408,0.8731027,0.8734216,83
their findings do not apply to all of them. it is particular important to be clear about it when providing negative results such the lower bound in sec. 5.,the distinctions between these settings are crucial for understanding the implications of their results and the applicability of their proposed algorithm.,0.8726571,0.85552156,0.8640043,115
"be wrong otherwise. generally speaking, it would be more intuitive to see it expressed as a difference with the performance measured in the past on an observed task.","be beneficial to explore scenarios where this assumption does not hold, as it could lead to a more comprehensive understanding of the limitations of their approach.",0.87169015,0.8586097,0.86510044,127
"this work is theoretical and the aim is not to outperform some continual learning baselines, but it would be beneficial for the community to understand how big is the gap between theory and practice.","the paper lies in its theoretical insights, yet a more detailed discussion on how these insights could translate into practical applications would enhance the paper's relevance.",0.8715404,0.86820185,0.869868,144
going to have. it would be interesting if the authors could discuss which realistic assumptions on the environments could provide a more favorable situation.,as the theoretical results need to be contextualized within real-world applications to fully appreciate their significance.,0.8584443,0.86169326,0.86006564,115
"support their argument, and provided time complexity.",provide a comprehensive overview of the correlation clustering problem and its implications in the context of differential privacy.,0.852208,0.86950266,0.8607685,102
"recent work, especially using noised agreement have novelty.","the correlation clustering problem, presenting a novel algorithm that achieves both multiplicative and additive approximation guarantees.",0.8397329,0.852883,0.8462569,104
authors need to say the sum of the “absolute value” of negative edges to prevent confusion.,"authors should clarify the definitions of the terms used, as they may lead to confusion regarding the privacy guarantees.",0.88413405,0.864655,0.87428606,77
"not independent, the privacy budget is accumulated by composition theorem. this implies the privacy budget is higher than the authors argued, exactly |g|/2 times higher than expected(the sensitivity of d is 2, as mentioned in line 227, and the sensitivity of d(v) is 1).","sensitive to the addition or removal of edges, and thus the noise added should be carefully calibrated to ensure that the overall privacy budget is maintained.",0.85449874,0.8176271,0.8356564,199
"submission. for example, [calm+21], which appears to have motivated the authors is accepted by icml.",version of the paper to include the most recent and relevant citations to enhance the credibility of their work.,0.8489427,0.84329844,0.8461111,86
"consider adopting one of the proposed methods based on similarity to one of the given datasets in the evaluation. moreover, tabular deep learning is still a relatively underexplored subject, and any push to establish better baselines could prove important.",such a detailed set of results could prove useful for practitioners that might be looking to implement these techniques in their own models.,0.8689374,0.83820486,0.8532945,189
"over the years, some are identical to the methods in this paper, e.g., the use of periodic activation functions. while the authors note that it was proposed for positional embeddings, it was also proposed for embedding plain numbers found in the text. see [1] for a survey on the various methods used in natural language processing.","namely, there are many papers on how to treat numerical data found in natural language, and specifically how to embed it in the same space as conventional word embeddings.",0.8862566,0.86461675,0.875303,240
better than another on a given dataset. there is little attempt to analyze the results or draw any conclusions. the paper would be significantly improved if it was concentrated on one specific method rather than all possible combinations of a set of methods.,incorporating these perspectives could enhance the depth of the analysis and potentially lead to more robust embedding techniques.,0.8677435,0.835823,0.8514842,189
"in [2] a different set of datasets is considered (though there is some overlap), where they already showed that deep learning methods can beat in most cases gbdt. in my view, this submission lacks a thorough explanation of their methodology for selecting the datasets they analyze and why it is okay to ignore others that have appeared in other papers.","it is unclear which method is preferred (if any), and why one method would work better than another in specific contexts.",0.8608575,0.841099,0.8508636,276
"nlp: a survey and a vision, naacl 2021.",the only explanation given is that these datasets are 'gbdt-friendly'.,0.83890635,0.8309785,0.83492357,57
"learning models for tabular data, neurips 2021.","for instance, the paper could benefit from discussing the relevance of datasets like those used in thawani et al. and gorishniy et al., which have been pivotal in advancing the field.",0.80516785,0.8517659,0.8278116,154
"clear and easy to follow manner, high clarity.",the paper is written in a straightforward and accessible style.,0.89303154,0.8789154,0.88591725,49
although extended a previous uniform sampling method.,the approach is innovative and presents a fresh perspective on the problem.,0.88769877,0.88476145,0.88622767,54
"the ""posterior collapse"" in terms of theoretical analysis.",the work provides valuable insights into mitigating posterior collapse in autoregressive vaes.,0.8468771,0.8668414,0.856743,66
the claims and proposed method in this paper,the work presents a comprehensive experimental study that validates the proposed approach.,0.86818683,0.8588876,0.8635122,66
i am not confident that the improvements over the baseline are significant.,"the significance of the work is somewhat ambiguous, especially regarding its potential impact on future research.",0.8780674,0.8917812,0.8848711,86
\lambda. it is not clear why the authors chose \lambda=1 as their choice.,the ablation study lacks a thorough examination of how different parameter settings influence the results.,0.8671045,0.85121477,0.85908616,81
"over uniform sampling on the yahoo dataset, the paper does not discuss it.","in table 1, the lack of improvement in kl and mi metrics suggests that the method may not be as effective as claimed.",0.8516843,0.86952955,0.8605144,86
"parts. first, the discrimination can propagate from input to output (theorem 1). second, the iterated mean aggregation feature imputation algorithm can exaggerate the input discrimination (theorem 2).",the propagation effect can break into two main pathways: the structural biases inherent in the graph and the imputed feature values that reflect these biases.,0.8609114,0.8453934,0.8530819,147
across groups in each iteration. the algorithm can provably satisfy epsilon-fairness.,this algorithm aims to ensure that the imputed features maintain a low discrimination risk while still achieving acceptable reconstruction error.,0.85655326,0.8565122,0.85653275,106
"between utility and fairness, which is expected.","the results indicate that while the proposed method improves fairness, it does so at the cost of some reconstruction accuracy.",0.84563035,0.88427764,0.86452234,96
intuition behind the fair graph feature imputation algorithm.,full picture of how these imputation methods interact with various graph structures and their implications for fairness.,0.8849118,0.91219133,0.89834446,79
"and an improved hybrid method, as well as some understanding of why beam search approaches are worse. the paper is well written overall, though the explanation/motivation of the proposal distribution could be made clearer.","they also provide a framework, baselines, and experimental results that demonstrate the effectiveness of their proposed methods.",0.8664317,0.8542198,0.86028236,158
know they usually assume some parametric form for the underlying probabilistic model / hazard function / etc. that is easier to work with. for neural models there is less structure to work with.,"however, as far as i can tell, the specific focus on conditional probability queries in neural sequence models is indeed a novel contribution.",0.84657216,0.849805,0.84818554,140
"combines the two for mutual benefits, using importance sampling to estimate the gap between the beam search lower bound and the true probability.",this hybrid method systematically outperforms the individual approaches across various datasets and query types.,0.8660361,0.83742833,0.8514919,112
"interactions, usage records) and models (lstms, gpt2 (which one?) ).",which adds to the robustness of their findings and demonstrates the applicability of their methods in different contexts.,0.8308574,0.803034,0.81670886,91
improves existing regret bounds that have been compared.,demonstrates significant improvements in convergence rates.,0.8930449,0.8739391,0.8833887,43
"ingredients, e.g., contraction property of mwu mapping.",insights that simplify the convergence analysis.,0.89016175,0.85435057,0.8718886,43
a new decentralized multi-agent online learning algorithm.,efficient and practical for decentralized settings.,0.87886345,0.887661,0.8832403,44
and all claims are justified via proofs.,and presents the concepts clearly.,0.89876187,0.8768351,0.8876631,29
implementation which is important for online multi-agent game learning.,approach that enhances scalability in multi-agent systems.,0.9161858,0.9031946,0.9096438,46
of time average of history. this goes beyond the standard metric in learning games.,"of relying on the entire sequence, which is more efficient.",0.88304865,0.867413,0.875161,60
the existing methods. this is an important advance in learning normal form games.,"previous methods, offering a better rate of convergence.",0.8879776,0.88405955,0.8860142,57
the technical steps are easy to follow.,the authors present their findings in a structured and coherent manner.,0.87583137,0.8745644,0.8751974,52
propose a new metric for measuring the quality of attacks/defenses.,authors also highlight the critical need for robust evaluation methods in the graph domain.,0.8856188,0.8758074,0.88068575,70
"include global/local attacks under evasion/poisoning setting, can break several representative defense gnn models.",demonstrate the inadequacy of existing defenses against stronger adversarial strategies.,0.8766792,0.84182984,0.8589012,84
proposed robustness unit test set is useful for evaluating new defense models.,the proposed methodology provides a solid foundation for future research in adversarial robustness.,0.900164,0.8970101,0.8985843,69
"millions of nodes) is very important, as it may reveal more realistic lessons and guidelines. note that there are a few scalable defense models such as [2].","considering the scale of real gnn applications (e.g., [1]) for attacking, evaluating attacks/defenses on large graphs (with",0.82535744,0.83803254,0.83164674,114
"networks for web-scale recommender systems"", kdd'18. \","networks for semi-supervised classification"".",0.89416414,0.8469128,0.86989725,33
"graph neural networks at scale”, neurips'21.","graph neural networks against adversarial attacks"".",0.8829391,0.8521358,0.86726403,24
on well-known competitive benchmarks.,the results demonstrate the effectiveness of the proposed querypose framework.,0.8725488,0.87652504,0.87453246,63
explaining what each of the several modules does. figure 2 is particularly helpful.,of clearly explaining the architecture and the rationale behind each component.,0.897619,0.8807764,0.88911796,66
it would be very hard to reproduce the reported results using the description provided in the paper.,it will allow other researchers to replicate and build upon their work.,0.89113677,0.88147444,0.88627934,67
complex. there are several modules involved.,innovative in its approach to multi-person pose estimation.,0.8469958,0.88699263,0.8665329,46
method is. both training and inference times need to be provided in the updated manuscript.,method performs in practical applications without this crucial data.,0.8613373,0.86622113,0.8637724,67
classifiers is an intriguing idea. doing this with a self-supervised backbone is also carefully considered.,is a crucial aspect that enhances the committee's ability to focus on challenging bias-conflicting samples.,0.85239697,0.84942204,0.8509069,88
be dominated by bias-guiding samples especially in long-tailed datasets,"capture the full diversity of bias-conflicting samples, potentially limiting the effectiveness of the committee.",0.87308276,0.88897216,0.8809559,73
bias-conflicting samples. higher enrichment can possibly be achieved due to other factors as well?,"the nuanced differences between bias-guiding and bias-conflicting samples, which is essential for understanding its effectiveness.",0.8616806,0.85717005,0.8594194,100
especially for demonstrating its motivation in the introduction session.,it presents a clear and structured approach to multi-modal 3d object detection.,0.86695224,0.87343466,0.8701813,61
both comparing with previous methods and ablation studies.,demonstrating the effectiveness of the proposed method.,0.8769838,0.8695948,0.8732737,45
"is also important. i'm curious about whether the detr-based framework can run as fast as, at least not slower too much compared with previous sotas.",it is crucial to evaluate the computational efficiency and resource requirements of the proposed approach.,0.8922558,0.8372998,0.8639047,108
the two modalities to help readers get a deeper understanding on how the visual information from the two sources is well selected and utilized.,the multi-modal interaction to better illustrate how the model leverages information from both modalities.,0.898345,0.8921088,0.89521605,95
upon the state of the art.,over previous methods for achieving differential privacy in non-convex empirical risk minimization.,0.83222115,0.84842867,0.84024674,82
relevance in differential privacy and machine learning.,"importance in the context of machine learning, especially given the increasing concerns around data privacy.",0.88047814,0.9226543,0.9010729,79
"wonder whether the shuffling is necessary at all: i suspect this can be circumvented using analyses of incremental sgd, such as nedic, bertsekas (incremental subgradient methods for nondifferentiable optimization, siopt 2001).","would suggest clarifying the role of shuffling in the context of privacy guarantees, as it may confuse readers.",0.86443317,0.80467725,0.83348554,175
"be marginal, compared to existing literature.",significantly enhance the utility bounds while maintaining privacy.,0.85387236,0.85631424,0.8550916,51
to understand the out of sample performance of the algorithm.,to explore the trade-offs between computational efficiency and privacy guarantees.,0.89780116,0.89097774,0.89437646,59
"this fact in its presentation. the main difference between the current submission and past work is that in the current paper this is analyzed for multiple passes on the data, which adds another layer of complexity into the analysis.","this prior work adequately, which could mislead readers about the novelty of the approach.",0.8844561,0.8510457,0.8674294,176
some small inconsistencies which make reading difficult.,grammatical errors that detract from the overall clarity and professionalism of the presentation.,0.8868902,0.9064691,0.89657277,76
"modeling, and the authors extends pix2seq model to learn 4 specific tasks in coco datasets.","modeling, aiming to create a unified interface for diverse vision tasks.",0.8865696,0.8541682,0.87006736,57
training objective and model architecture design.,architecture and training process for multiple vision tasks.,0.8832777,0.8841406,0.88370895,45
(although the text prompt is pre-defined). this helps create a friendly user interface to access the model.,"for various vision tasks, enhancing flexibility and scalability.",0.87668455,0.8646582,0.8706298,85
"research line unifying different computer vision tasks. the proposed model shows great potential for the ambitious goal of the all-in-one design; however, i also have several concerns about this paper, and they are listed below.",direction in the development of generalist vision models that leverage unified frameworks.,0.880592,0.8453122,0.86259145,170
"task, many other vision problems developed in the community are not clearly addressed. while it is really an open research question, the title of this paper ""a unified sequence interface for vision tasks"" is indeed overclaimed. i would suggest rewrite as ""a unified sequence interface for coco dataset"" to better reflect what you have proposed and what you have done in the paper.","task, the generalizability of this approach to a broader range of vision tasks is still uncertain.",0.88373005,0.85031813,0.8667022,304
"authors didn't include other popular v+l tasks such as image-text retrieval and visual question answering as they are commonly evaluated in the foundation model literature. similarly, there are many popular pure vision tasks ignored in this paper, such as image/video recognition, nerf, etc. overall, this gives the impression that the task unification is achieved just because of a careful selection of downstream tasks in the experiments.","authors chose to include image captioning as a core task, which may skew the evaluation of their unified approach.",0.89014184,0.8438646,0.8663857,352
"here is probably about unifying 3 selected pure vision tasks. however, pioneer works (such as maskrcnn, hrnet, and others) already unify the three tasks very well. it is unclear what is the advantage of the proposed method (i.e., using the sequence modeling for the three tasks).","is the integration of multiple vision tasks into a single framework, yet a direct comparison with existing v+l models would strengthen their claims.",0.85835624,0.8417142,0.8499538,201
critical to the selected 3 tasks. the motivation of using sequence modeling for the three tasks is somewhat weak.,"crucial for tasks like object detection and keypoint estimation, which may limit the effectiveness of the proposed method.",0.88479054,0.8834095,0.8840995,90
"and instance segmentation at the same time with just a single forward-pass, but in this paper, the inference cost could grow linearly to the number of people. the inference interface is kind of restricted: (1) only a single task is supported at a time; (2) it predicts token-by-token slowly.","and keypoint detection more efficiently, highlighting a potential drawback of the proposed method.",0.8833185,0.8217041,0.851398,233
"this paper, the tasks considered require fine-grained annotations, and thus they do not have large-scale pre-training data (except for image captioning). for the considered downstream dataset, it is unclear if single-objective is more effective than multi-objective training.","the context of multi-task learning for vision, the complexity of balancing multiple loss functions may hinder the training process.",0.87621856,0.85257566,0.8642354,208
"paper integrating a few known components together to achieve an expected ""okay"" performance. considering the technical novelty, the contribution of this paper is relatively weak.",overview rather than a comprehensive study that addresses the nuances of each task.,0.87055755,0.8424125,0.85625374,132
"current paper content, it is difficult to reach a conclusion at this point.","current limitations in efficiency and task generalization, its practical applicability remains to be fully realized.",0.85445094,0.8820627,0.8680373,83
between performance and efficiency on gpu-like devices for semantic segmentation task.,between performance and efficiency on gpu-like devices.,0.9799602,0.9448335,0.96207637,31
global context for improving semantic segmentation by utilizing attention deeply without lost of efficiency.,global context for improving semantic segmentation without loss of efficiency.,0.97031736,0.9350103,0.95233667,31
"in addition, it provides a new perspective for practice on real-time semantic segmentation task.",this demonstrates its effectiveness across multiple datasets.,0.8929391,0.8639945,0.87822837,65
design. this paper is incremental compared with previous work ddrnet. the novelty of this paper is limited.,"architectures, but it is specifically tailored to enhance the performance in real-time segmentation tasks.",0.854686,0.85195386,0.85331774,87
"1, the miou score and fps improvement are both limited, not obvious enough.","2, the gains in miou are modest compared to existing state-of-the-art methods.",0.8990514,0.9048956,0.90196407,59
"the method is simple and not novel enough. in addition, why not apply this method to other vision tasks, like classification, object detection.",the generalization of the model to other datasets beyond those tested may require further validation.,0.8763815,0.8643834,0.8703411,98
"the constraints previous approaches required. however, the paper would benefit from a more detailed discussion of evolutionary approaches such as this (mentioned briefly in lines 246-248) as well as the previous attempts to train ssns (mentioned briefly in the introduction) to help the reader understand the literature landscape and the specific contributions of this paper.",the approach of interleaving gradient optimization steps with the evolutionary steps also appears to be original.,0.8527608,0.8198118,0.83596176,296
"a more challenging task of optimizing the gsm on cifar-10 images. however, clarity issues related to the explanation of the comparison for this task currently make the claims of improved performance on this task hard to evaluate (see detailed questions concerning this below).",the approach seems to outperform the previous method on the existing task (though see questions section below for concerns about the explanation of this setup).,0.893026,0.86921144,0.88095784,192
"addition, as mentioned in the quality section above (and in more detail in the questions section below), there seem to be important details related to the experiment setup that are missing which makes it confusing and hard to evaluate the empirical claims.","see the questions section below for some specific examples, but thorough proofreading would help the presentation.",0.8779464,0.8611063,0.8694448,183
step in that direction. the ideas of incorporating gradient-based optimization along with potential topological changes during training could be of interest to the broader machine learning community.,i think this paper makes a substantial contribution to the understanding of neural circuit mechanisms and their computational capabilities.,0.8658879,0.85442764,0.86011964,144
"of noun phrases), some external resources will be required (for example, an already trained constituent parser) which we may not assume to be readily available for a wide variety of languages. therefore, it seems to me that the proposed theory has a limited scope.","the authors could elaborate on the challenges and potential strategies for implementing data augmentation in textual data, as the complexities differ significantly from image data.",0.86122894,0.8409784,0.8509832,194
"vc dimension in classical pac learning. in the proposed theory, different modes of realizability entail different characterizations of learnability.","the classical pac learning framework, which typically offers clearer boundaries and definitions for these concepts.",0.8879193,0.8608181,0.87415874,96
"a 1-inclusion-graph predictor), i.e. to calculate the label on any new test point, the entire training set will be used (as it is, for example, in the k nearest neighbors algorithm), which calls into question the computational efficiency of learning.",the optimal learning algorithm is again conditioned by the realizability mode.,0.8607216,0.83388865,0.8470927,202
it has been a good transformer based method for high-quality image restoration applications.,and demonstrates significant improvements over existing methods.,0.8946043,0.8816186,0.88806397,71
"like rectangle-window self-attention, axial-shift operation, and locality complementary module.",which aids in understanding the overall framework.,0.8775668,0.8482568,0.862663,72
and demonstrate the effects of each proposed component.,and provide valuable insights into the contributions of each component.,0.9141139,0.9344637,0.9241768,43
"the proposed method cat. the visual differences between the proposed cat and other methods are very obvious, and further show the effectiveness of the proposed cat. similar observations happen for image jpeg compression artifact reduction.","the proposed cat models, highlighting their effectiveness in various image restoration tasks.",0.90248525,0.8749558,0.88850737,170
and organization are pretty good.,is coherent and well-structured.,0.8738875,0.88058555,0.87722373,27
"for reproduction, which further shows the solidness of the work.",which enhances the reproducibility of their results.,0.88651466,0.88936913,0.88793963,51
"w, sl should be given in the caption.","w, and other parameters should be explicitly defined for clarity.",0.8694805,0.8972895,0.8831662,42
model size and flops than the related work swinir. it would be much better if the authors provide comparisons with swinir using similar parameter number and flops.,"model size compared to some traditional methods, which may impact deployment in resource-constrained environments.",0.85927796,0.83647156,0.8477214,115
"cat-a. for jpeg compression artifact reduction, the authors only show cat. it is not very clear about its model size and flops.","cat-a, but the differences in their configurations and performance should be elaborated further.",0.8918721,0.86250126,0.87694085,90
"well-structured, and easy to follow.",the paper is well-structured and presents a novel approach to anomaly detection.,0.89208424,0.90204275,0.89703584,51
"on anomaly detection tasks, which has highly practical significance.","this paper explores the impact of noise data on unsupervised anomaly detection methods, highlighting the challenges posed by noisy training sets.",0.85604703,0.894727,0.8749597,106
experiments to explain insights and analyze the impact of noise on ad tasks.,"the proposed task setup is reasonable, and the authors have conducted fundamental experiments to validate their approach, demonstrating the effectiveness of their method.",0.8648274,0.8854449,0.87501466,127
effectiveness of the proposed method is verified.,"from the experimental results, the proposed method shows significant improvements over existing techniques, particularly in noisy environments.",0.8686776,0.89159214,0.87998575,106
"should be $w_{(i)}$; line 268, lof achieve -> achieved.","there are some grammatical errors: line 156, $w_(i)$ should be corrected to $w_i$ for consistency in notation.",0.8662141,0.874136,0.87015706,86
"symbol of sample variance $\sum_{h,w}$ is confusing in eq(2) and (3).","the mathematical notation should be improved. for example, the definitions of variables could be clearer to enhance readability.",0.8577123,0.7950946,0.82521725,101
should be given more explanations.,"the critical threshold τ is set to a constant value, which may limit the adaptability of the method to varying noise levels.",0.82879806,0.84576714,0.83719665,104
"are actually new. but since i can't point to a previous reference that proves these results, i recommend accepting the paper.",do not lead to significant advancements in the field of algorithmic linear algebra.,0.86628234,0.84930485,0.85770965,91
"groundbreaking; still, they are nontrivial observations that yield great results.","easily applicable to all types of distance metrics, which may limit their generalizability.",0.8502476,0.85482574,0.85253054,71
seems like a scientifically interesting research topic to me to find the linguistic differences between natural and emergent languages.,it also contributes to the understanding of the relationship between emergent and natural languages.,0.9196449,0.88874996,0.90393347,88
the background is explained with good attention to details.,the methodology is clearly outlined.,0.92884636,0.8925116,0.9103165,42
writing is quite good.,is generally well-structured and easy to follow.,0.86226636,0.8945662,0.8781193,38
"not fundamentally change the field of nlp, it is a nice focused contribution.","suggest that has may not apply meaningfully to emergent languages, they provide valuable insights.",0.8490235,0.8631547,0.85603076,72
languages in multiple settings in addition to lewis's signaling game.,language properties in more diverse settings.,0.8982429,0.8498359,0.8733691,42
not think this takes away from the merit of the paper.,appreciate the focused approach to a specific aspect of emergent languages.,0.85642195,0.8690727,0.86270094,53
related work needed to understand the proposed approach.,the challenges associated with quantizing transformer models.,0.8927902,0.8915976,0.8921935,49
with related work on the glue benchmark.,of their method against previous state-of-the-art approaches.,0.8820627,0.87612873,0.8790857,46
model has competitive performance compared to previous sota.,model can be achieved through the proposed multi-distillation and elastic binarization techniques.,0.849767,0.86183435,0.85575813,72
and selection of hyper-parameters could affect the model performance.,affects the overall performance of the binarized models in different layers.,0.8844509,0.90051264,0.89240944,59
their contributions. their proof for $d=1$ provides a good initiation for the general proof.,"the authors clearly explain the problem, outlining the critical separation threshold for efficient learning of spherical gaussian mixtures.",0.8521874,0.84639114,0.84927934,101
"problem and, to my knowledge, the contribution seem significant.",the authors build upon previous work to address the challenges in low-dimensional regimes.,0.84965485,0.84778345,0.84871817,67
"if possible, it would be very interesting to see numerical performance of this algorithm, or a discussion on why this is not feasible.","this algorithm is efficient and operates under conditions of smaller separation than previously established, making significant contributions to the field.",0.8709829,0.86614513,0.8685573,114
interesting application of lottery tickets.,empirical validation of the effectiveness of sparse networks in low data regimes.,0.8767369,0.89329296,0.88493747,62
"authors consider also the case where lottery tickets are used in combination of other techniques for limited-data regimes, such as data augmentation.",results demonstrate that sparse winning tickets significantly outperform dense networks in data-limited scenarios.,0.8938557,0.8730265,0.8833183,101
written and easy to follow.,structured and clearly written.,0.925663,0.91201866,0.9187902,20
"an empirical study, it is important to provide also an analysis that tries to explain the performance shown in the experiments and identifies the properties of the lottery tickets that are useful in this context.","focused on understanding the properties of sparse networks, it provides valuable insights into their generalization capabilities.",0.8769578,0.8693705,0.8731476,156
"all the experiments using resnet-18, which is a quite simple architecture. i am curious to see if the same results could be obtained using also other architectures.","experiments primarily on resnet architectures, which may limit the generalizability of their findings.",0.8855647,0.87040263,0.8779182,108
"sparse networks for data-limited regimes in the context of image classification, there are some results in nlp.","the specific application of iterative magnitude pruning in low data regimes, the concept of sparsity improving performance in such settings is well-established.",0.8760057,0.88479686,0.8803794,112
it easy to justify and understand the underlying reasons that explain the design.,this makes it a significant contribution to the field of time series forecasting.,0.8674769,0.8588338,0.86313367,62
improvement relative to the specified benchmark.,advancement in the accuracy and interpretability of time series predictions.,0.8489064,0.86197615,0.8553914,55
"by the ground truth. hence, there is a high probability that they are caused by numerical instabilities in the numerical algorithms.","these sudden breaks of continuity are not reflected or explained in the analysis, which raises concerns about the robustness of the model.",0.8701352,0.872916,0.8715234,110
be versions of the same pipeline capable of reaching even better performances.,be potential for further optimization and improvement in the model's performance.,0.89789104,0.88775116,0.89279234,51
surface in terms of the bias-variance tradeoff parlance.,the authors effectively highlight the challenges posed by the multimodal nature of the hyperparameter posterior and propose innovative solutions to navigate this complexity.,0.83818185,0.852824,0.8454395,136
a relatively understudied area in literature.,the use of mcmc sampling to approximate the hyperparameter posterior is a commendable approach that allows for a more comprehensive exploration of model uncertainty.,0.82780576,0.8553101,0.84133315,136
m is the number of samples - how exactly can this method scale? perhaps by interleaving fbgp at intervals with traditional ml-ii optimisation - a more sophisticated strategy would be required.,"the computational burden increases significantly with larger datasets, potentially limiting the practical applicability of the proposed methods in real-world scenarios where data is abundant.",0.86314535,0.8406339,0.85174096,155
the integration of model compression into existing pac-bayes bound to make the bound non-vacuous.,its focus on model compressibility as a key factor in understanding generalization in neural networks.,0.8786305,0.851599,0.86490357,71
better intrinsic dimension of the task of interest. it is then followed by a quantization technique to compress the model further to regularize the model of interest further according to the occam's razor.,more efficient and scalable way to train neural networks in lower-dimensional spaces.,0.8716282,0.82332367,0.8467877,161
"running time significantly, seems as an engineering technique to optimize [38].","training efficiency, may require further empirical validation against a broader range of architectures.",0.8640077,0.83909535,0.8513694,77
"bounds do not include results on several datasets, but it would make the paper more convincing if those methods could be reproduced and evaluated on the datasets of interest.",bounds could be more comprehensively compared to strengthen the claims of superiority made in this work.,0.87561786,0.86432433,0.86993444,125
"of this article, and the writing logic is clear, the motivation and optimization point of this paper is clearly clarified.",the example effectively demonstrates how the neuroschedule algorithm improves scheduling efficiency by utilizing gnns.,0.8610752,0.85666436,0.8588642,102
"convenient for calculation when performing operations, thereby reducing the complexity of the overall calculation.","suitable for the scheduling process, allowing for better resource utilization.",0.9107389,0.9053185,0.9080206,77
"is predicted through the gnn model, thereby reducing the execution time of the entire process.","is effectively learned through the proposed model, enhancing the scheduling outcomes.",0.9168489,0.8957825,0.90619326,61
"same type in each step are mutually exclusive, a restrictive description of the execution should be given to avoid readers' guessing.","cdfg are represented as nodes, a more thorough explanation of how these representations impact scheduling would be beneficial.",0.8775023,0.8612063,0.86927795,102
"accuracy, and the final execution time is longer than that of eds. the paper lacks the analysis of the benefit ratio of additional computing device.",runtime efficiency and scalability of the proposed method over traditional approaches.,0.8699533,0.84385,0.8567028,105
"into account many variants, domain changes, etc.",a variety of graph structures into account.,0.8804944,0.89058024,0.8855086,39
results on scattering hold,frameworks are considered.,0.88434815,0.840273,0.8617474,23
"convincing, especially for graph regression",well-designed and provide insightful results.,0.85781276,0.84247994,0.8500772,38
"within the experiment section, and may seem a tad arbitrary. as a result, the reader is somewhat left wondering all along the paper what the actual architecture is, if this is just an abstract formulation of previous architecture or if there is something fundamentally new here. examples of implementation on graphs along the abstract description could really help the understanding of the approach.",which may lead to difficulties in understanding the practical implications of the proposed methods.,0.89044636,0.83664316,0.8627067,326
"not tested in experiments (changing graphs, higher-order tensors...)",the core contributions may be obscured by the complexity of the framework.,0.8600662,0.8408384,0.8503436,63
a minimal examples satisfying all of them is not given,the practical applicability of these results may be limited in real-world scenarios.,0.857113,0.8400899,0.8485161,64
related works on graph partition and fair division are properly cited. i do wonder if there are more closely related works that combine graph partition and fair division together?,the approach of integrating graphical structures into fair division is novel and contributes to the existing literature.,0.8751969,0.85478234,0.8648691,135
theorems / lemmas are neatly presented and proved.,the algorithms presented are rigorously analyzed and provide clear approximation guarantees.,0.88127196,0.8422499,0.8613192,63
written and organized. some minor comments:,structured and the arguments are presented in a logical manner.,0.85979164,0.86667925,0.86322165,43
constant fraction” -> ensuring a constant fraction,the fairness of allocations is a central theme throughout the paper.,0.86056423,0.8205551,0.84008354,50
amount of” -> a significant amount of,sacrifices in social welfare are acknowledged and discussed.,0.83182603,0.7875153,0.8090644,48
and studies” -> widely accepted and studied,fairness concepts are effectively utilized in the proposed algorithms.,0.8588433,0.8247712,0.84146255,52
case” -> for the two-agent case,"cases, the results are particularly insightful.",0.8646409,0.8320179,0.8480157,35
page 9 line 302: “second smallest ” -> the second smallest,the correction enhances the grammatical accuracy of the text.,0.85940385,0.8309934,0.8449599,52
interesting and could be building blocks for future works in this line.,important as they highlight the challenges and potential solutions in fair allocation problems.,0.8626573,0.86026204,0.861458,68
"area of ml, namely solving np-hard co problems.","in various fields such as logistics, drug discovery, and semiconductor design.",0.84826005,0.84373623,0.845992,60
the underlying symmetries of co problems in addition to learning to find near-optimal solutions.,symmetricities that enhance the performance of neural combinatorial optimization.,0.8713047,0.859877,0.86555314,69
solvers and is therefore complimentary to a broad variety of prior work.,"methods, allowing for improved performance without extensive architectural changes.",0.8737558,0.86121464,0.8674399,65
in multiple areas (see below).,"in some sections, particularly in the explanation of the loss functions and their implications.",0.8435226,0.8479676,0.8457393,75
are significantly smaller than those of prior works.,demonstrate the effectiveness of the proposed method in practical scenarios.,0.8605051,0.859371,0.8599377,55
has multiple shortcomings and inconsistencies (see below).,provides a comprehensive evaluation of the proposed method across multiple combinatorial optimization problems.,0.83835304,0.83229154,0.83531135,88
sufficiently place this work in the context of the current literature.,fully capture the breadth of existing research in neural combinatorial optimization and symmetric learning.,0.87410676,0.8978816,0.8858347,79
well and is easy to follow,the paper is written in a clear and structured manner.,0.87899226,0.8701151,0.87453115,44
"and effective, which can reduce the issue of noisy pseudo-labels",the proposed method with the assistant model is straightforward and efficient.,0.86128575,0.8687067,0.86498034,62
student model can be a generally useful tool for other tasks,the proposed feature transmission from the assistant model to the student model is a novel approach.,0.8747518,0.8851544,0.87992233,76
performance improvements on two datasets,experiments show good performance across multiple benchmarks.,0.89747286,0.8886963,0.89306307,45
are not explained very well,some hyperparameters and technical details require more clarification.,0.84656334,0.85445774,0.85049224,56
\tau value in eq. (6)?,what is the primary contribution of this research?,0.8462059,0.8196602,0.8327215,41
"pseudo-labeling, i.e., is \gamma a constant or varied and how?",how do the authors determine the threshold for generating pseudo labels?,0.87158704,0.84262526,0.8568615,59
"like when there are more labeled data, e.g., >1000/2000 labeled images, which can be a more practical situation in real applications.","table 4-7 are conducted on a smaller labeled set (183 images). however, i am curious about the results in table 4-7 with a larger labeled dataset.",0.8444071,0.853088,0.8487253,110
"same case for all experimental settings. if so, what is the intuition behind it? the authors may have more discussions on this.","in table 6, the teacher model performs better than the student model. it's intriguing to know whether this is due to the training strategy or the architecture of the models.",0.8609687,0.85720277,0.8590816,123
"semi-supervised semantic segmentation, bmvc'18",adversarial learning for semi-supervised tasks is an important research direction.,0.85588354,0.83865404,0.84718114,58
"with high-and low-level consistency, pami'19",semi-supervised semantic segmentation is an expanding area of research.,0.83420575,0.83100915,0.83260435,58
"segmentation with self-correcting networks, cvpr'20",semi-supervised semantic image segmentation has recently attracted significant interest.,0.8452257,0.83546937,0.8403192,68
"learning from a class-wise memory bank, iccv'21",semi-supervised semantic segmentation with pixel-level contrastive learning is a developing trend.,0.8494896,0.83630574,0.84284616,80
all components of a model is interesting.,the idea of collaboratively pruning all components in vits is innovative and addresses a significant gap in the current literature on model compression.,0.85691965,0.9135871,0.8843465,118
when compared to sufficient state-of-the-art methods.,"the performance gain is impressive, particularly the accuracy improvements while achieving substantial reductions in flops.",0.8552048,0.8622078,0.858692,96
well-written and easy to follow.,"this paper is a valuable contribution to the field of deep learning, particularly in the context of optimizing vision transformers.",0.8501849,0.8506826,0.85043365,109
"for the proposed method during pruning, when compared to state-of-the-art network pruning methods.","it's not clear how much computation cost and data is needed to effectively implement the proposed pruning method in practical scenarios, which could limit its applicability.",0.8543068,0.881192,0.86754113,133
"setting of ""without second-order interations"" (line 300) in this ablation study means dropping all hessian-based terms in eq. (4) or only drop the cross-components terms (green blocks in figure 2(b))? i think the latter one can better reflect the main contribution of the proposed method.","the main novelty of this work is the interaction of different components during pruning, so the ablation study on this design is important. in my understanding, line 297-305 and table 7 aim to give such ablation studies, while it's not clear whether the results convincingly demonstrate the effectiveness of considering interactions over traditional methods.",0.85430944,0.8383813,0.84627044,247
"to follow, and the technical part seems correct.",the authors effectively communicate their findings and contributions.,0.8635467,0.8612122,0.86237794,49
"update. this also seems to agree to the theory rl algorithms: one can just perform the regular bellman updates (or perform elimination in version space algorithms) and define policy with lcb or take minimum over the remaining set of functions (for pessimism) (for example, [1]).",this insight is crucial as it challenges the conventional wisdom in the field and suggests a more efficient approach to policy evaluation.,0.8628963,0.81347734,0.8374584,207
the following subsection provides good evidence that that indeed could happen. it could be better to provide some more intuitive scenario or even a closed-form construction.,the authors provide a thorough analysis that clarifies these conditions and their implications.,0.88398063,0.85524833,0.8693772,123
"of shared target updating methods (such as shared-lcb ens., shared-min deep ens, and with a different number of ensembles)",these experiments effectively demonstrate the advantages of the proposed msg algorithm over traditional methods.,0.87410045,0.81823057,0.8452432,95
be fine-tuned for the final presentation of the results. c) the experiments are performed on extensive benchmarks.,this thoroughness in experimentation strengthens the validity of the results presented.,0.8993598,0.8671713,0.8829723,79
"methods, but since the result is based on the ntk setting, it still has some gap between the practical situations.",they lay a solid foundation for understanding the limitations of existing approaches.,0.8822639,0.8552278,0.8685355,85
"different tasks, which likely undermines the empirical merits of the proposed algorithm.",this highlights the algorithm's adaptability across different settings.,0.88018656,0.89028084,0.8852049,68
"learning."" advances in neural information processing systems 34 (2021): 6683-6694.",it provides additional context and supports the claims made in the paper.,0.8527665,0.7785325,0.8139605,65
pareto set to help optimize the qehvi criterion;,this integration allows for a more comprehensive exploration of trade-offs among competing objectives.,0.8672508,0.8202403,0.8430907,79
practitioners. the method to build it with backpropagation is original.,"it effectively maps trade-off preferences to corresponding pareto solutions, enhancing decision-making.",0.84814775,0.8624649,0.85524637,76
terms of speed and sample efficiency.,they show that the psl method outperforms existing mobo approaches in various benchmark and real-world scenarios.,0.84289956,0.8694415,0.85596484,93
have been proposed in the multi-objective literature.,this allows for capturing complex structures and relationships within the pareto front.,0.85368675,0.8618348,0.8577414,66
pareto front is not shown.,"the method shows robustness in handling such scenarios, providing valid solutions.",0.85081613,0.81786305,0.8340142,65
"be clarified, see questions below.",further research could focus on enhancing the performance in high-dimensional optimization problems.,0.8508686,0.829444,0.8400197,82
"know, the originality is good.","can tell, the paper presents a novel approach to domain generalization.",0.8794106,0.8884868,0.88392544,53
is theoretically and technically sound.,shows promise in addressing the limitations of existing domain generalization techniques.,0.86246455,0.87523043,0.8688006,71
"the proposed optimizer (sdg) significantly improves the performances of coral, fish, and vrex on several datasets.",this highlights the trade-off between empirical risk and generalization performance.,0.8829359,0.8529619,0.8676901,85
experiments with more penalty-based dg methods will make the effectiveness of sdg stronger.,it would benefit from more comprehensive comparisons against a wider range of baseline methods.,0.8704585,0.8738731,0.87216246,76
"2, sdg increases the generalization gap on four datasets (iwildcam., ogbmolpcba, amazon, and py150) and decreases the generalization gap on only three datasets (camelyon17, fmow, and povertymap).","in table 1, while improvements are noted, the statistical significance and practical implications of these gains need further clarification.",0.85630274,0.7950338,0.8245317,147
fish for fmow is already strong is far-fetched with the worst accuracy of 34.6.,proposed method is consistent across different models and benchmarks is somewhat vague and requires more detailed analysis.,0.85045016,0.86680055,0.8585475,94
"tasks” many interpretations would be possible and the authors do not comment on this and i don’t think their data are sufficiently clear on this on their own. nonetheless, the authors are thorough. for example, they investigate the convergence of their batch algorithm, find a working clustering for the umap embedding and check the necessity of the parts of their model.","the authors demonstrate that by carefully tuning the architecture and hyperparameters, they can achieve competitive performance in unsupervised learning, which raises questions about the generalizability of their findings.",0.853841,0.83965635,0.8466892,264
"more plausible than back propagation, but other aspects like the subspace clustering for example are not questioned regarding back propagation at all.","they emphasize that stacked unsupervised learning is inspired by biological processes, yet the reliance on meta-learning and hyperparameter tuning may detract from this claim.",0.85683894,0.84976625,0.85328794,124
"then is consistent with the architecture search theme, this seems odd. is there an explanation why deeper features are no longer better?","while choosing 3 layers appears optimal for this dataset, it prompts further investigation into the diminishing returns of adding more layers.",0.87321275,0.8626609,0.8679047,105
"of unsupervised learning still apply. practically for example, the method still requires that supervised data is available for meta-learning. i think some discussion of this is necessary.","the authors should clarify how their approach maintains biological relevance despite the supervised tuning process, as this could influence the interpretation of their results.",0.86583126,0.8540079,0.85987896,134
"structures, the deconfounded solution can be obtained by simply solving the normal equation. this formulation would be sufficient for the first step towards the deconfoundation.","while this simplicity may discard several more intricate relationships, it provides a clear and interpretable framework for understanding the confounding effects.",0.85388696,0.8508713,0.8523765,139
"step. through the experiments, we can observe that adding this simple fix is beneficial to improving the performance in most cases.","this versatility enhances its applicability across various domains and tasks, making it a valuable tool for researchers.",0.8723473,0.8656265,0.8689739,103
"to see the correlation between the functional similarity and ood accuracy. while these results may seem excellent, we would not immediately see what such a functional similarity brings us to improve transfer learning and ood generalization.","for example, section 4.3 provides experiments to observe the correlation between functional similarity and domain similarity, and section 4.4 further emphasizes the importance of understanding these relationships in the context of out-of-distribution generalization.",0.86784256,0.87322605,0.870526,184
"example, figure 2 tells us the averaged similarity is sufficient to distinguish random and fine-tuned resnet-18), i have a concern when networks are deeper; most of the high-level features are similar between two networks, and hence the layer-wise averaged similarity would not be sufficient.","to measure the similarity between the entire networks, the authors simply average the layer-wise similarity through the experiments.",0.90081894,0.85308987,0.876305,197
"generative models to sample from this manifold appears very interesting and intriguing, at least conceptually.",a generative model to sample from this manifold is innovative and addresses a significant gap in the current literature.,0.91762567,0.9162165,0.91692054,59
"results. it is inspiring to see that trained generative models can capture the structure of the weight manifold and learn the set of sufficiently accurate neural networks, even capable of producing competitive model ensembles.","evidence demonstrating the advantages of the proposed methods over existing baselines, particularly in terms of model performance and diversity.",0.8690205,0.8584813,0.8637187,166
its current form it appears to require a very significant computational investment for pre-training an ensemble zoo. this large computational cost could make it somewhat impractical for real-life applications.,"certain scenarios, the reliance on specific architectures may limit its generalizability across diverse model types.",0.865891,0.86050475,0.86318946,157
explanation in section 3.2.3 could be clarified and made more rigorous. it is not entirely clear to me how $d$-dimensional samples $n_i$ are mapped back to the $d$-dimensional space.,assumption of the existence of such functions may need further justification or alternative approaches to ensure the validity of the proposed sampling methods.,0.864256,0.80812603,0.83524907,141
written and easy to parse,structured and presents a clear methodology.,0.87902915,0.8598185,0.8693177,30
stated and situated in the context of previous work,clearly articulated and address a significant gap in auxiliary learning.,0.87135255,0.867432,0.86938787,52
baselines over multiple seeds with calculated standard deviations.,tasks demonstrates the effectiveness of the proposed method.,0.88210434,0.84789026,0.864659,50
together from previous work (which itself would not be criticism per-se if there was novelty in the way the stitching together was done).,"the contributions, while valuable, do not significantly advance the state-of-the-art in auxiliary learning.",0.8669649,0.8297185,0.8479328,97
"bi-level optimization has been done in [1, 2].",module-level importance is a concept that has been explored in previous works.,0.90079945,0.8703439,0.8853098,47
"are not uncommon in the multitask literature see section 3.2 of [3] which introduces an index k, for each layer.",these include methods that dynamically adjust loss weights based on task relevance.,0.86964345,0.84376276,0.85650766,81
"task weights - this abstracts away the choice of parameter/block/model-l introduced in this paper - and represents an automated ""modularization"" approach","losses, which is a relevant approach that could have been discussed.",0.86750937,0.8293433,0.8479971,120
end-task aware training as an alternative : https://arxiv.org/abs/2109.07437,the necessity of pre-training in auxiliary learning contexts.,0.8814826,0.7817982,0.8286533,57
multi-task optimization in massively multilingual models. https://arxiv.org/abs/2010.05874,the robustness of models against negative transfer from auxiliary tasks.,0.8647364,0.7816197,0.82108,74
"good, the bad and the neutral. https://arxiv.org/abs/2108.11346",importance of decomposing auxiliary task updates for better performance.,0.8428514,0.75767565,0.7979971,62
"the authors, i've updated my score to accept","the authors, the paper has significantly improved in clarity and depth.",0.8653078,0.8557068,0.86048055,43
"constraints. i would judge that the originality of the paper is high as, according to the authors, nobody has attempted to provide a hyperparameter tuning-free approach to the switching between synchronous and asynchronous training modes.","conditions, making it a timely contribution to the field.",0.8684927,0.83131063,0.84949505,197
the training modes that help to determine and address the gaps in the current work. this provides clarity on the choices and explains the conducted experiments.,"various training modes, particularly in the context of gradient staleness and its impact on model accuracy.",0.8683358,0.87393725,0.8711274,109
"my confusion in the questions section below. in summary, it seems that the methods in the figure and their performance do not correspond to the description in the text.","my concerns regarding the presentation of data in this section, as it could benefit from clearer labeling and explanation.",0.87903774,0.8770007,0.878018,119
"properties in the industrial setting. i believe that this impact might translate to better resource utilization and, therefore, cost savings.","training processes, particularly in environments with variable resource availability.",0.8889338,0.8709332,0.87984145,105
"infer the interaction between the agent, allies, entities and memory.","it proposes a new memory module, atm which can improve the handling of partial observability.",0.8628662,0.86507183,0.8639676,69
outperforms gru-based one which is usually used.,"it evaluates for smac, atm-based agent demonstrates superior learning efficiency.",0.86028504,0.8402983,0.8501742,58
and memory types and the number of memory slots.,"it reports the ablation studies for eba, confirming its importance in the overall architecture.",0.8529964,0.8538494,0.85342276,73
"example, in figure 2 (a) and (b), atm-based models outperform gru-based models in steps, but when comparing in wall-clock time, atm-based model could be much slower than recurrent module-based models.","as shown in [1], the agent with transformer requires more computations, even though it can benefit for sample efficiency [1,2] or interactions between entities like this paper. for atm, the design mitigates the computational burden while maintaining performance.",0.84535,0.84876615,0.8470547,201
"transformer-based agents [2,3] get the advantages from attending the past directly, while it cannot do that through atm architecture.","the memory following the working memory mechanism is interesting, but it cannot attend the past explicitly. other architectures could potentially offer more direct access to historical information.",0.8885812,0.86666864,0.87748814,142
"reinforcement learning using actor-learner distillation."" arxiv preprint arxiv:2104.01655 (2021).","[1] parisotto, emilio, and ruslan salakhutdinov. ""efficient transformers in reinforcement learning.""",0.7898753,0.767872,0.77871823,83
"reinforcement learning."" international conference on machine learning. pmlr, 2020.","[2] parisotto, emilio, et al. ""stabilizing transformers for reinforcement learning.""",0.8097077,0.8531811,0.8308762,70
"with transformer world models."" arxiv preprint arxiv:2202.09481 (2022).","[3] chen, chang, et al. ""transdreamer: reinforcement learning with transformers.""",0.8147975,0.7636318,0.7883854,65
excellent job motivating the problem and giving intuitions for their algorithm.,excellent job in presenting their findings.,0.9324771,0.89506394,0.91338754,46
compares and contrasts related works to this paper.,highlights the gaps that the current research addresses.,0.8667798,0.85856223,0.8626514,46
seems novel and changes the proof techniques for bounding the constraint violation.,is a novel approach that enhances the algorithm's performance.,0.9101913,0.8847557,0.89729327,58
"be helpful to add experiments for some of the real-world applications of this framework mentioned in the introduction section (e.g., safety-critical applications).",be beneficial to include real-world datasets to validate the algorithm's effectiveness in practical scenarios.,0.91447973,0.87583256,0.894739,107
"usually correspond to adversarial and stochastic (typically i.i.d.) constraints, however, in this paper, it refers to adversarial and fixed constraints.",should be more explicitly defined to avoid confusion regarding the specific trade-offs being addressed.,0.8449156,0.8243651,0.83451384,110
"assumed this condition holds to obtain better bounds, and how the framework in this paper does not make such an assumption.","utilized it, and the implications of its presence or absence in the context of the proposed algorithm.",0.88143426,0.86569893,0.8734957,91
times in the paper before it is finally defined on page 6.,"times throughout the paper, but its significance should be elaborated for clarity.",0.88265276,0.8789293,0.8807871,49
"$g_t(x)\leq 0~\forall t\in[t]$. the authors should explain and motivate this choice of benchmark, and mention any potential hardness results for regret against the more natural static benchmark.","the conditions of the best fixed decision in hindsight, which may not be necessary for all applications.",0.84935856,0.79063153,0.81894356,143
the authors need to compare and contrast their algorithm with [18] and [30] and highlight the new ideas and proof techniques.,a more detailed comparison with these algorithms would strengthen the discussion of the contributions of recoo.,0.87833554,0.86374664,0.87098,82
early convolutions etc.). masked convolutions were introduced in the pixelrnn paper (https://arxiv.org/pdf/1601.06759.pdf). the strength of this paper is in its novel combination of existing ideas to produce a very simple hybrid framework that effectively combines the strength of convolutions and transformers.,"the idea of hybrid models already exists in multiple pieces of literature (coatnet, container, uniformer), but the authors effectively leverage these concepts to enhance the masked auto-encoding paradigm.",0.86427706,0.82078487,0.84196967,236
the mask to larger resolutions to avoid the requirement of keeping all tokens in stage 3.,this approach not only improves computational efficiency but also mitigates the pretraining-finetuning discrepancy.,0.8302219,0.86563754,0.8475599,83
nice way to generate a feature pyramid with local context via convolutions and global context using transformers.,it is a promising direction for improving object detection and segmentation tasks.,0.8778343,0.8532132,0.86534864,78
be nice to see whether the proposed scheme continues to outperform existing masking techniques for larger models. there is also limited runtime comparison with existing techniques.,"it covers a broad set of vision tasks but it does not cover scale, which could limit the generalizability of the findings.",0.85312414,0.84718376,0.8501436,123
"experiments comparing random masking, regular convolutions, multi-scale decoders etc.",these results provide valuable insights into the contributions of different components of the mcmae framework.,0.85764533,0.84626436,0.85191685,81
"a nice flow, and explains the concepts with ease.","the structure is logical, and the figures effectively illustrate the proposed methods and results.",0.89712554,0.9114715,0.9042416,70
generates hierarchical representations from an image and outperforms a number of existing techniques.,"this framework demonstrates significant improvements over existing methods, making it a valuable contribution to the field of self-supervised learning.",0.8677833,0.8778362,0.8727808,107
"fig 5, bottom middle). it is easy to follow.",the methodology and results sections are well-structured and easy to follow.,0.8748652,0.8590537,0.86688733,50
"on cifar-100 (across different kd methods, teacher-student architectures).",in the overall accuracy of the student models.,0.8666877,0.83813816,0.8521739,51
"noticeable amount of errors. if class-wise analysis is performed on the teacher based on misclassified samples, it corresponds to a subset of classes which can be treated as undistillable classes. therefore, the proposed tllm avoids such situations by eliminating classes where the teacher's predictions are largely incorrect and replacing them with ground-truth labels (one-hot vectors). some analysis i’d like to see are:","suboptimal contribution to the learning process of the student model, particularly for classes that are already challenging.",0.8488102,0.8211747,0.83476377,342
teacher (table 3) for regular and undistillable classes?,on the imagenet dataset?,0.8741985,0.83176327,0.8524531,43
"in tables 2, 3 for regular and undistillable classes?",in terms of per-class accuracy and overall performance metrics?,0.8592291,0.82398975,0.8412405,44
baseline for imagenet experiments? can the authors produce a table similar to table 2 for imagenet to show the improvements of the proposed tllm scheme?,sole baseline for comparison in the experiments?,0.87765914,0.8354449,0.85603195,119
~72.154% whereas the reported accuracy is only 68.900%. please clarify. link to public pytorch models with top1 accuracies: https://pytorch.org/vision/stable/models.html,"higher than what is reported, considering its established performance in the literature.",0.8748747,0.8110556,0.84175724,132
"to include the accuracies of students trained with label smoothing. can you report these results for tables 2, 3. i suspect that a noticeable amount of improvement can be obtained by using label smoothing, especially if undistillable classes are semantically similar classes (see [2, 3]).",to evaluate the performance of the student model without label smoothing against the baseline models.,0.9033241,0.8430672,0.87215614,217
(not available in supplementary as well).,"from the experimental section, which limits the generalizability of the findings.",0.833527,0.8191486,0.8262752,62
paper outweigh the strengths. but i’m happy to change my opinion based on the rebuttal.,study could be addressed by including more diverse datasets and distillation methods.,0.85062325,0.8496418,0.8501323,71
great effort on the rebuttal.,clarifications and additional experiments provided.,0.8394888,0.8575829,0.84843934,40
my concerns although concrete understanding of undistillable classes still remains unclear.,the concerns raised in the previous review.,0.9055704,0.8454059,0.87445444,67
of label smoothing on the proposed tllm framework.,of the proposed tllm on various teacher-student configurations.,0.91434896,0.91220796,0.9132772,46
"label smoothing help?."" advances in neural information processing systems 32 (2019).","label smoothing help?""",0.93142676,0.8385378,0.8825448,62
m. (2021). is label smoothing truly incompatible with knowledge distillation: an empirical study. in iclr,"a. ""label smoothing for knowledge distillation.""",0.92399913,0.8619859,0.8919159,62
(2022). revisiting label smoothing and knowledge distillation compatibility: what was missing?. icml,"""understanding the effects of label smoothing in neural networks.""",0.88411355,0.8483533,0.86586434,79
to learn simple interpretable models. the numerical results are convincing.,that effectively combines speed and optimization capabilities for generating risk scores.,0.8705601,0.8801528,0.8753302,69
of error cumulation is possible; coordinate descent can be long as well as the line search).,such as potential difficulties in ensuring consistency and coherence across the different stages of the process.,0.8599647,0.84044623,0.8500935,85
scores are not considered in the current contribution.,world applications may require additional validation to ensure the robustness of the generated risk scores.,0.86324394,0.86780417,0.86551803,79
based framework where the motion estimation and motion compensation are needed. this is an encouraging attempt.,which has been a dominant approach in neural video compression.,0.8692544,0.8648693,0.86705637,85
"shifting, sharpen/blur, and fading. some analyses are conducted.","shifting, blurring, and fading effects to evaluate the model's performance.",0.91081274,0.90463275,0.9077122,45
is limited. the core contributions have been investigated in other papers.,is somewhat limited as it builds upon existing concepts without introducing fundamentally new mechanisms.,0.8663728,0.8877355,0.8769241,73
in [1]. the difference is that one previous frame is used in [1] and two previous frames are used in this paper.,"in various forms in prior works, indicating that this approach is not entirely novel.",0.8539554,0.83775437,0.84577733,80
"like [2,3,4,5]. directly applying transformer into neural video codec is straightforward and the novelty is limited.",but this paper extends its application to video compression with promising results.,0.8920154,0.8329981,0.86149716,85
"model, where the block contains two parts. in addition, [7] has investigated a similar idea where each block contains 4 parts. the difference of this paper is that each block contains 16 parts.",which suggests that the proposed method may not be as innovative as claimed.,0.85215914,0.8334812,0.8427167,150
"this paper does not need these designs. the block-based auto-regressive entropy model, lrp, and 3-stage training are also complex and hand-crafted designs.",argues that their transformer-based approach simplifies the process significantly.,0.8806918,0.8510271,0.86560535,115
the rd-curves are very close. the bd-rate numbers are best presented.,this raises questions about the practical advantages of the proposed method.,0.8857099,0.8389943,0.8617194,52
"x264 and x265 is very easy in 2022. comparisons with jm, hm, vtm are recommended because they represent the best encoders of h.264, hevc, vvc. the work [8] in 2021 has already compared with vtm.",these codecs in specific scenarios does not necessarily imply overall superiority.,0.84710103,0.80524105,0.8256408,155
"efficient video compression, eccv 2020",video compression is a relevant topic that this paper addresses.,0.85902065,0.8530328,0.8560163,53
"window-based attention for image compression, cvpr 2022",which highlights the importance of implementation nuances in compression algorithms.,0.8716922,0.8470534,0.8591962,59
"for learned image compression, iclr 2022",is a related work that explores similar concepts in a different context.,0.84240884,0.8110142,0.8264134,55
"priors for learned image compression, cvpr 2022","approaches have been explored in the literature, providing a backdrop for this study.",0.8357879,0.8189796,0.8272984,64
"efficient learned image compression, cvpr 2021",video compression is another relevant framework that shares similarities with the proposed method.,0.855274,0.84454536,0.84987587,75
"adaptive neighborhood information aggregation, arxiv 2022.",transformers has been a significant area of research that informs this paper's methodology.,0.84415704,0.8096542,0.82654566,73
"not need to be very deep and research on shallow models is still useful, which is valuable to the literature. this is the primary reason i recommend accepting this paper.",the results show that cnns do not necessarily need to be deep to achieve competitive performance.,0.8660168,0.8574573,0.86171585,126
sse block. the motivations are well explained and the effects are verified.,skip-squeeze-and-excitation modules.,0.82858944,0.8451736,0.8367994,60
is launched on a separate gpu) the results and discussions of the communication overhead are useful.,"(for the multi-gpu version, each stream is launched on a separate gpu, enhancing parallel processing efficiency.)",0.88265526,0.87796897,0.88030595,90
"of course, i understand the authors may have limited computing resources and did not lower my rating for lack of such large-scale experiments.",it is not sure if heavy-weight models show different depth vs. performance trade-offs.,0.8507327,0.84513474,0.84792453,108
clue on how to realize it. maybe the authors can show a potential direction.,experimental results on how parnet performs with such hardware.,0.8709446,0.88093144,0.8759095,59
improved. please see the suggestions below.,improved for clarity and conciseness.,0.8056777,0.8653099,0.83442974,29
written and easy to follow.,structured and presents a novel approach to contrastive learning.,0.8617661,0.8572886,0.8595215,48
network to obtain the positive samples is novel.,model to create instance-specific optimal positive pairs is innovative and promising.,0.88265383,0.92189443,0.9018475,56
paper is in the experimental part.,approach is the reliance on the quality of the generative model used.,0.8764471,0.8920285,0.88416916,48
in the original papers. i think strictly following the setting from the original paper will be a fairer comparison. (e.g. 100 epoch of simclr should reach 62.8% top-1 accuracy),"achieved with higher resolution datasets, which raises questions about the generalizability of the results.",0.8577883,0.8302603,0.8437999,139
generative model affect the final result?,generated images from the generative model affect the performance of the contrastive learning task?,0.8838297,0.920236,0.9016655,63
if we include an additional generative model?,associated with the proposed cop-gen method compared to traditional methods?,0.8590665,0.8757478,0.867327,56
intuitive idea for data augmentation,"a novel, simple, and efficient method for data augmentation is proposed.",0.8917129,0.93733114,0.9139531,44
very effective when implemented correctly,the simple idea is to create augmentations that maintain label information while reducing nuisance.,0.8435601,0.86047643,0.8519343,80
"inspiring. moreover, the theoretical interpretation of the method is sound.","the paper is easy to follow, well-written, and provides a coherent methodology.",0.8889755,0.89322746,0.8910964,60
and datasets are provided in this manuscript to show the effectiveness of the proposed lp-a3.,"results across various tasks, including semi-supervised learning, noisy-label learning, and medical image classification, highlight the effectiveness of lp-a3.",0.8664827,0.9044709,0.8850694,115
think it is necessary to provide the selection detail to make the manuscript self-contained.,how does the tcs select the hard positive data in algorithm 1? i seek further clarification on this mechanism.,0.84996235,0.85124916,0.8506053,83
"the backbone networks in the experiments in table 2? moreover, how’s the performance of lp-a3 on cifra10/100 in a supervised classification manner? evaluation with other backbone networks rather than resnet-18 and resnet-50 is also important.","the experimental results in the manuscript are insufficient to demonstrate the effectiveness of lp-a3. in semi-supervised learning tasks, only one augmentation method, infomin, is tested on the stl-10 dataset for comparison. what are the results when comparing lp-a3 with additional augmentation methods on this dataset?",0.8678648,0.8617203,0.8647816,225
"on the large-scale dataset, such as imagenet",how’s the performance of lp-a3 in comparison to other state-of-the-art methods?,0.83244246,0.8520939,0.84215355,60
"performance of lp-a3 can drop when all data is selected for augmentation? the author stated that some data augmentations are useless to apply data augmentation, which did not convince me.","i’m curious about the results in fig.5. as lp-a3 is designed to generate the augmented x’ while preserving the minimum sufficient information of the label, why does the performance fluctuate with different label preserving margins?",0.8558823,0.86533403,0.86058223,171
"given the existing results provided by the authors, the approximation in the $l^p$ norm is a necessary complement to our current understanding of the approximation problem.",the authors provide a comprehensive analysis of the lower bounds on approximation errors in this context.,0.89042956,0.86010957,0.87500703,119
the authors have presented the paper in a quite accessible way.,the clarity of the presentation helps in understanding the complex proofs involved.,0.8810575,0.8920672,0.8865282,54
"however, it is not obvious how in practice these two norms can make a difference.",the paper successfully addresses this transition and offers valuable insights into the implications for neural network architectures.,0.8750447,0.879765,0.87739855,99
different formats for different stages of the revive framework including implicit knowledge retrieval and the answer generation stage. the experiments verify the value of introducing the regional features.,"the authors address this issue by introducing the regional descriptions in their proposed method, revive.",0.8594198,0.8497306,0.85454774,145
"the performance of implicit knowledge retrieval? also, the left qualitative example in figure 3 confuses me as none of the knowledge retrieved mentions the right answer ``battery'' and the regional tags introduce additional misleading objects say the desktop.","concretely, as the regional descriptions are injected into different stages of the framework, how do these descriptions influence the overall accuracy and knowledge retrieval capabilities of the model?",0.8614646,0.8438348,0.8525586,190
regional features to augment the prompts for better ok-vqa results.,regional visual representations to enhance knowledge-based vqa.,0.8941647,0.8925965,0.89337987,47
distills them into student policies via well-motivated structural-relation loss constraint,effectively utilizes self-attention mechanisms to enhance cooperative behavior among agents.,0.8614794,0.8317526,0.846355,75
performance in various multi-agent environments,"results across various benchmarks, demonstrating its effectiveness in offline multi-agent reinforcement learning.",0.8812562,0.9319019,0.90587175,80
"of every component in the proposed method, as well as discussed.","of each component in their framework, validating the necessity of the proposed structural relation distillation.",0.873984,0.8991467,0.8863868,73
w/o pretraining. authors should include a comparison with madt-online too.,"the conventional distillation approach, which highlights the advantages of the new method.",0.87001866,0.84320664,0.8564028,69
"(where number of agents > 2), simpler transformer-based method madt performs comparable to the proposed method.",the reliance on a centralized decision transformer may limit performance as the number of agents increases.,0.87243634,0.85124147,0.8617086,73
and grammatical errors in the text,and grammatical errors throughout the paper detract from the overall clarity.,0.90727854,0.95090526,0.92857975,46
this model is novel and creative. it will likely be of interest to many in the community given the recent successes of image generation approaches like dall-e.,"to the best of this reviewer's knowledge, this is one of the first attempts to unify such diverse tasks under a single framework.",0.8378078,0.86688983,0.8521008,117
being very optimized for these diverse vision tasks.,requiring task-specific components.,0.9024471,0.87481034,0.88841385,41
"the autoregressive model helps a lot, and there is a 'sweet spot' in terms of code length.",the autoregressive structure of the language model is crucial for achieving high performance.,0.92127174,0.9080046,0.91459,60
"later), but nothing else that stands out to this reviewer",the need for more thorough exploration of potential misuse and ethical considerations in deploying such a general-purpose model.,0.8372002,0.80405545,0.82029325,99
"5 runs. for a fair comparison the authors should rerun the experiment and generate statistical mean/variance results for random train/validation split to remove any bias due to specific train/validation split selection. additional details about the baseline model (resnet-26) e.g., network architecture/training time/training accuracy would be helpful to get an all-round comparison.","however, the authors show the accuracy for one single train/validation split whereas the reference [10] shows a statistical mean of 82.2 +- 0.3% over multiple runs.",0.861508,0.8476058,0.85450035,276
the paper lacks any detailed analysis on the contribution of the standardization and smoothening steps which could be generated by selectively applying these preprocessing steps.,this lack of analysis limits the understanding of the advantages of their approach in this context.,0.9013564,0.8615855,0.8810224,119
"compares to the baseline results would be helpful. for instance, the baseline has a higher antibiotic treatment classification accuracy than the proposed model, but this isn’t discussed and no justification is provided for the same.",a discussion on these figures to help understand their results and how it relates to their findings would enhance the clarity and impact of the paper.,0.88678956,0.8705832,0.8786117,161
"deal with non-smooth outer function); 2) under similar assumptions compared with related works, the algorithms proposed in this paper achieve the best convergence rate with known condition number",the proposed methods effectively address the limitations of existing algorithms by accommodating nonconvex and possibly nonsmooth outer functions.,0.8808021,0.8494233,0.8648282,141
"of application of this work, i.e. this work can do but the previous works can not. it is good to mention more examples/applications of using non-smooth objectives.",expanding the discussion to include other potential applications of nonsmooth regularization could enhance the relevance and impact of the research.,0.86101264,0.8513122,0.85613495,128
way to mitigate meta-overfitting and cover the range of the task distribution,the problem is realistic as task augmentation is a good approach to improve the performance of meta-learning.,0.8730041,0.87563884,0.8743195,79
presented well. i have more comments about the method (see below),the method though not very intuitive is well-motivated and effectively tackles the issues of task diversity and overfitting.,0.84592915,0.85310256,0.8495007,95
"such as classification, regression and cross-domain tasks",preliminary results are covering multiple aspects of the method's performance and its applicability.,0.8361239,0.8490744,0.84254944,77
the classification results to provide a more robust picture about the improvements. the gap between mlti and the proposed method is not too much (except for isic).,"the empirical results are not particularly strong, and the improvements are marginal in most cases. it would be beneficial to provide confidence intervals for the reported metrics to better assess the variability of the results.",0.8811687,0.8624073,0.8716871,157
"paper shows some preliminary results on cross-domain tasks, i would like to see some more discussion and insights around it as i believe it's a more realistic scenario for few-shot learning","while task augmentation intuitively makes sense for similar domain datasets (e.g. miniimagenet), i would like to see some discussion around the impact of task augmentation for cross-domain tasks. while the results indicate improvements, a deeper understanding of the limitations and potential drawbacks in cross-domain scenarios would enhance the paper.",0.8769156,0.89591014,0.8863111,241
see. can you please provide some more details on the generation cost and how you incorporate task augmentation during meta-training?,"the paper does not provide information on the computational cost for the method, which i would like to see included to assess its practicality.",0.8740928,0.8682703,0.87117183,97
more empirical insights to support the results would be better than the theoretical analysis in this regard.,the theoretical results seem somewhat forced. i believe the methodological contribution is quite strong and could be emphasized more effectively without an over-reliance on theoretical justifications.,0.86327565,0.8761958,0.86968774,145
"improvements (linear scaling w.r.t. input size, adaptive computation with large speed boost) compared to neural interpreters, are impressive.",the proposed architecture demonstrates significant advancements in modular neural networks.,0.89264745,0.84772325,0.8696055,105
to follow. the figures are well-made and very helpful.,the authors effectively communicate their ideas.,0.8826161,0.8702047,0.8763665,42
datasets) and the number of baselines (now only perceiver io) both need to be reasonably improved. e.g. perceiver io was evaluated on 6 different modalities (table 5 in their paper) and against non-general purpose baselines within each modality.,a more comprehensive evaluation across various domains would strengthen the claims made.,0.86654735,0.8274052,0.84652406,191
proposed architecture is significant and will inspire or lead to future breakthroughs.,this limitation raises questions about the generalizability of the results.,0.87057996,0.8566216,0.8635444,64
temporal operation is very interesting and novel to my best knowledge.,the core idea of using kma to align spatial information for temporal modeling is innovative and effectively addresses the limitations of existing methods.,0.8523205,0.8758085,0.8639049,112
and the results are strong.,"the mathematical derivation is solid, providing a clear foundation for the proposed approach.",0.8707198,0.902902,0.88651896,76
achieve consistent performance improvement using different backbones.,"the proposed method is robust and can be easily integrated into various video learning frameworks, demonstrating its versatility.",0.8700534,0.89058006,0.88019705,99
"the model is not thorough enough, see questions section",the analysis on what has been learned by the model through the alignment process could be expanded to provide deeper insights into its effectiveness.,0.85197705,0.8570063,0.8544842,110
with more clarity see limitation section,some details should be presented regarding the computational complexity of the proposed method compared to existing approaches.,0.84060186,0.8348107,0.83769625,101
well written with comprehensive details.,the paper is well-structured and presents a comprehensive analysis of the proposed method.,0.87198174,0.8998882,0.88571525,62
baseline methods by a large margin,the proposed method outperforms the existing clip model in various zero-shot transfer learning tasks.,0.84327257,0.866779,0.8548643,80
"modern hopfield networks, which are all existing works even though the application scenario is new","the novelty of the paper is below the bar. it combines the infoloob and modern hopfield networks to address the saturation problem of infonce, but the contributions may not be sufficiently innovative compared to existing literature.",0.8446185,0.8615987,0.85302407,175
pair of tasks to come up with initial hypothesis and then tests those hypothesis on cw10/cw20 benchmarks.,i like the fact that it uses a comprehensive set of experiments to evaluate the impact of different components on transfer.,0.8641145,0.8470459,0.8554951,92
and is overall simple while still achieving state of the art performance in cw10/cw20,it effectively integrates the insights gained from the experiments to enhance performance.,0.8771936,0.85018414,0.8634777,70
are true only for the cw benchmarks. i do appreciate the authors being very upfront about the limitations of the work.,"hence, there’s a possibility that these findings may not generalize to other reinforcement learning algorithms or different task environments.",0.84797883,0.8560808,0.85201055,102
the proposed approach seems new to me. relevant literatures are well discussed.,the authors effectively highlight the challenges of data sharing in sensitive domains and the limitations of existing differentially private generative models.,0.8441695,0.86832994,0.8560793,120
"mistakes. however, some details of the algorithm and the experiments are missing (see my questions below).","flaws in the methodology, as the approach of directly optimizing a small set of samples for downstream tasks appears sound and innovative.",0.86587936,0.8547394,0.86027336,103
to follow and well organized.,"to follow, with a clear structure and well-defined objectives that facilitate understanding of the proposed method.",0.86107284,0.89837384,0.87932795,88
"budget. not sure whether what happens when the budget is moderate, say $\epsilon = 5$. also, it would be better if the performance is benchmarked on colored image datasets (e.g., cifar-10).","budgets, which may limit the generalizability of their findings; exploring a wider range of privacy levels could provide a more comprehensive evaluation of the method's effectiveness.",0.849939,0.8257408,0.8376652,148
"results nicely complement each other. in addition, i believe that the framework the authors suggested might be used to analyze other optimization methods.",the different theoretical results and the empirical findings presented in the paper support the proposed multiscale perturbed gradient descent (mpgd) framework.,0.8395489,0.8791561,0.85889614,123
"is probably due to space constraints, this makes it hard to evaluate how general or reasonable the assumptions are and the limitations of the results.","while i understand this may be due to space constraints, providing a brief overview or summary of these assumptions in the main text would enhance the paper's accessibility and clarity.",0.8719959,0.8898619,0.88083833,120
in a standard model used for personalized treatment selection,which enhances the ability to identify and utilize similar treatment effects among various options.,0.8678967,0.85742015,0.8626266,69
has a theoretical group-consistency property,is effective in simultaneously clustering treatments and estimating the optimal individualized treatment rule.,0.8438834,0.86025786,0.85199195,91
experimentally validated on synthetic and real data,demonstrated through both theoretical guarantees and empirical results.,0.871238,0.8380697,0.85433203,53
whereas discrete ones may be observed for several treatments/pathologies,allowing for a more nuanced understanding of treatment effects across different patient characteristics.,0.87519246,0.86139596,0.86823934,75
and may underfit the data in various situations,which facilitates the interpretation of treatment effects and their interactions with covariates.,0.8780283,0.86920667,0.8735952,71
"sgd outperforms gd if the spectrum of the hessian has some outliers, which is usual in realistic applications, see figure 2.","this finding highlights the unique efficiency of sgd in high-dimensional settings, particularly when compared to full-batch methods.",0.87568706,0.8514023,0.8633739,107
analysis might be of independent interest to study the behavior of sgd.,introduction provides a useful framework for understanding the asymptotic behavior of sgd.,0.89379704,0.9059774,0.899846,55
"problems **directly**. next, the author should provide the ""real"" convergence comparison between sgd and gd even for the random feature models to support their main observation. icr is just an indicator for the comparison between sgd and gd. there might be an approximation error due to the usage of hsgd and also the dimension, iteration time is finite. hence, without this simulation, i am not convinced that sgd does outperform gd in terms of convergence speed.","however, note that the random feature model is a convex optimization problem, which differs a lot from realistic neural networks. hence, small icr on the random feature model cannot be used to show that sgd outperforms gd on realistic optimization problems, as the dynamics and challenges in non-convex settings are significantly different.",0.86397326,0.8599596,0.8619617,331
"value (for strongly convex cases, they converge exactly to the same point). to my best knowledge, all the previous papers studying the implicit regularization focus on nonconvex optimization.","for convex optimization, any algorithms, if convergent, then they should converge to the same function, which diminishes the significance of the claim regarding implicit regularization.",0.8895079,0.87039226,0.8798463,143
unfamiliar with it. it would be good if additional background information can be provided.,"who may not have a strong mathematical background, making it challenging to fully grasp the implications of the results presented.",0.8671972,0.86281157,0.8649988,95
