True Sentences,Generated Sentences,Precision,Recall,F1 Score,Edit Distance
verify the effectiveness of the proposed method by comparing with existing methods.,the experiments conducted on tsp and cvrp datasets demonstrate the effectiveness of the proposed amdkd method.,0.869946,0.91038924,0.8897083,79
"has quite a few task-specific designs, e.g., training policies, architectures, hyper-parameters, etc. it is thus doubtful if the proposed method is generalizable enough when applied in practice.",is well-structured and effectively utilizes knowledge distillation to enhance cross-distribution generalization.,0.8555823,0.8412175,0.8483391,148
"result numbers are 0.00%. i am not sure if it has some special implications. if it is, there should be some explanations about it; otherwise, it could be presented in a better way, or maybe just leave it unfilled.","results indicate that these traditional solvers achieve optimal solutions, highlighting the competitive performance of the proposed method against established benchmarks.",0.8576715,0.8317405,0.844507,154
values have specific properties in terms of describing the heterogeneity in the data. in fact it can be argued the other way that graph fourier transforms and graph signal processing are the result of a “kernel trick” based on pca which is far older. so in this sense gnns/gcns are a more general class of networks that subsumes vnns.,this connection is well-established and provides a solid foundation for their work.,0.8606695,0.8095052,0.8343036,278
is interesting although over the years there have been robust forms of pca that have been developed including variations of principal component pursuit.,"however, the stability they claim is primarily a consequence of the inherent properties of covariance matrices rather than a novel aspect of their proposed architecture.",0.8499496,0.8471954,0.8485703,129
"the graph consisting of data features as nodes and data points as signals on the nodes (rather than vice versa). while this may be desirable for some types of analysis (see tong et al. ieee iccasp 2022 for analysis of cells as signals over gene graphs), i do not believe it has advantages over robust pca for this specific application.",it appears to be an adaptation of existing graph convolutional networks (gcns) rather than a fundamentally new architecture.,0.84501743,0.81277835,0.8285844,258
simply talk about an application of gcns/gnns to feature covariance matrices and situations where that could be useful.,a clearer distinction between the proposed method and existing gcns would enhance the paper's impact.,0.88260806,0.86757565,0.8750273,91
"this is a strictly linear relationship, if this was changed to mutual information or some other relationship type then indeed a more complex relational graph would be necessary and this is precisely where gnns/gcns have contributed.",the authors could elaborate on how their method captures these relationships more effectively than traditional methods.,0.85272443,0.82571447,0.83900213,167
to low-rank approximations done via svd and again i don't see much advantage in using a neural network for this kind of linear operation.,the novelty of the covariance filter needs to be better articulated to justify its introduction in the context of gnns.,0.8690167,0.8520132,0.86043096,106
"assume full-support, which is a strong assumption in real-world applications. this paper takes a close look on the problem and discusses new efficient estimators, that based on mild assumptions, such as full-support not in the combinatorial action spaces, but in the feature space, which also shows better empirical performance.","most prior works have primarily focused on scenarios where the full support assumption holds, making this study particularly relevant.",0.8696183,0.8477694,0.8585549,246
"the combinatorial sense of full support sense in the support over the action features, which draw a nice connection in the slate literature.",the proposed estimators provide a novel approach to address the challenges posed by deficient support.,0.8802135,0.8524791,0.86612433,98
"highly rely on the discrete features of the action, and the linear assumption of the reward, which seems a reasonable assumption for slate, but seems not the case for the reward for each individual action over features.","for the pi estimator, it seems that its performance may heavily depend on the assumptions of full support on side information and linearity, which might not always hold in practice.",0.86864465,0.8591792,0.863886,157
"to define the weighting function). it would be great if the authors add more discussion along this line. this is indeed match the experiment result, as we do see better performance of the similarity estimator as the support divergence increases.",this reliance on similarity could lead to challenges in scenarios where the action features do not adequately capture the underlying reward structure.,0.8606318,0.8403734,0.85038203,176
"ok if this paper only considers the weighting based approach. however, the proposed similarity estimator shares a lot of similarity with the model-based approach, then it is great if the paper could add more baselines based on the model based approach.","including additional baselines, such as other state-of-the-art ope methods, would strengthen the empirical evaluation.",0.86366194,0.8529777,0.85828656,192
"to add more results in terms of bias, variance to further understand the trade-off of the estimators.",this could provide insights into how the choice of similarity affects the performance of the proposed estimators.,0.902576,0.8868017,0.8946194,72
"of complexity in adversarial decision making, which may facilitate further study in bandits/rl etc. the upper and lower bound is new and non-trivial.",of the statistical complexity of adversarial decision making using the convexified decision-estimation coefficient.,0.8820638,0.8595996,0.87068677,100
"the relationships between complexities in adversarial/stochastic settings, and connections between different complexity notions are well explained.",it presents complex ideas clearly and systematically.,0.88160324,0.8426947,0.86170995,112
that i am not very clear about. please see the questions section.,where the notation could be clarified for better understanding.,0.8678191,0.8577353,0.86274767,48
"of iba to marl appears to be novel, judging from recent citations to the original iba paper. as far as i understand, the paper offers a few novel technical contributions on top of the existing iba theoretical framework, namely:",the introduction of distributed influence-augmented local simulators (dials) represents a significant advancement in the field.,0.8458582,0.84229076,0.84407073,175
"for non-stationarity due to other agents, with the retraining frequency as a hyperparameter controlling addressing non-stationarity vs stability.",this allows for a more efficient training process while maintaining the integrity of the learning signals.,0.87819177,0.8414265,0.85941607,102
the above choice by essentially implying that one does not need exact influences to obtain optimal policies.,this theorem supports the practicality of the dials approach in real-world applications.,0.87140685,0.85088915,0.8610258,79
"area of research. the experimental results show that this technique can be used to accelerate and improve the learning of multi-agent policies, and the underlying ideas are simple enough that i could see them being adopted by the community.","the findings have implications for various applications, including traffic management and robotics.",0.88652384,0.8574387,0.87173873,184
"follow naturally from the iba framework. i haven't checked the derivations, but the resulting claims are not surprising and appear correct.",the theoretical insights and empirical results presented in the paper reinforce the validity of the approach.,0.87279695,0.85511065,0.8638632,104
"the environment's simulator must be implemented in a very particular way. i might be missing something, but it doesn't seem like dials is a method that can be applied out-of-the-box to any relevant multi-agent environment; it puts strong constraints on how the system is simulated. for example, i'm assuming the environments they used in their experiments were implemented by the authors; at least the wording in line 291 seems to imply this is the case for the traffic control experiments, but nothing is said for the warehouse environment. more generally, i didn't see a lot of discussion of this point in the paper, which i would argue is an important omission.","this seems like not only a theoretical limitation, but also a strong practical one, as it means that environments that do not have a clear local structure may not benefit from this approach.",0.86029166,0.83465767,0.8472808,526
and one baseline being considered. some of the results are not necessarily intuitive and not a lot of discussion is offered (see my last main question below).,expanding the evaluation to include a wider variety of environments would strengthen the claims made in the paper.,0.86056966,0.83436406,0.8472643,116
"costs and effectiveness) is well-motivated by many practical domains, such as healthcare, manufacturing, and wildlife conservation. the technical work required to extend the problem setting in this way is significant since one must now account for how workers with different (arm/state-specific) expertise may interact (and influence arms' transitions) over time.",h,0.7977144,0.78061295,0.78907096,362
"is known by the decision-maker in the planning setting, the assumption that this additional level of information is available should be explicitly stated and justified by the availability/""learnability"" of this type of information within the domain(s) of interest. this will be particularly important in settings where there is heteroskedastic uncertainty about workers' intervention effects.",r,0.81080085,0.77687794,0.793477,391
defines the setting and assumptions before introducing the new solution,"the paper rigorously establishes a theoretical framework for continual learning, particularly emphasizing the feature extraction paradigm.",0.85322404,0.8635624,0.8583621,103
of the theoretical aspects involved in the continual learning process. a direction which is heavily unexplored in the cl community due to the difficulty of the topic.,"the work in this paper attempts to enhance our understanding of the limitations and capabilities of continual learning algorithms, especially regarding linear feature mappings.",0.8533908,0.85737836,0.85538,137
"correctly point out, there are several different settings in continual learning and their findings do not apply to all of them. it is particular important to be clear about it when providing negative results such the lower bound in sec. 5.","the authors generically refer to continual learning in various sections of the paper, but they clarify the specific challenges and assumptions that underpin their proposed methodology.",0.86446804,0.8496281,0.85698384,166
"get a good accuracy on the current task (l103) but it would be wrong otherwise. generally speaking, it would be more intuitive to see it expressed as a difference with the performance measured in the past on an observed task.","the definition of forgetting on l102 is valid due to the assumption that the learner will retain a fixed model size while adapting to new environments, which is a crucial aspect of their framework.",0.8511261,0.84196115,0.8465188,166
"in sec. 6 do not help. it is clear that the main contribution of this work is theoretical and the aim is not to outperform some continual learning baselines, but it would be beneficial for the community to understand how big is the gap between theory and practice.","it is challenging to grasp the potential practical implications of the findings in the paper, as the results are predominantly theoretical, potentially limiting their real-world applicability.",0.8767459,0.8616488,0.86913174,192
clear how much of a practical impact that is going to have. it would be interesting if the authors could discuss which realistic assumptions on the environments could provide a more favorable situation.,"sec 5 provides a lower bound on the error, but it lacks comprehensiveness to fully address the complexities associated with continual learning in non-linear feature settings.",0.85324806,0.8523358,0.85279167,148
"video-text retrieval task studied by this work is practical, and the proposed salient frame proposal mechanism seems simple and effective. extensive experiments are performed on video-text datasets and the experimental results are promising.",the paper effectively addresses the noise issue in video-language modeling by proposing a language-guided denoising network (lgdn) that filters out irrelevant frames.,0.85173416,0.864844,0.858239,173
"for example, in-depth comparison between salient frame proposal (sfp) mechanism and sparse sampling or other frame sampling techniques. also, some experimental results (e.g., effect of sfp mechanism) in supplementary material can be included in the paper.","while the authors provide some ablation studies, a deeper analysis of the impact of each component on overall performance would strengthen the validation of their proposed architecture.",0.8628706,0.8298099,0.84601736,189
"written in a clear and easy to follow manner, high clarity.",the paper is well-structured and presents a novel approach to mitigating posterior collapse in autoregressive vaes.,0.83526945,0.85293806,0.8440113,86
"novel and original, although extended a previous uniform sampling method.","the approach is innovative, utilizing an adversarial dropout mechanism to enhance latent variable learning.",0.8783053,0.87363845,0.87596565,81
"some new insights to the ""posterior collapse"" in terms of theoretical analysis.",the work provides a theoretical foundation for understanding the effects of dropout on information flow in vaes.,0.8796544,0.88040555,0.8800298,80
experimentation study which support the claims and proposed method in this paper,the work presents empirical results demonstrating improved performance on standard text benchmarks compared to existing methods.,0.8546988,0.85557675,0.8551375,98
"not completely clear to me, i am not confident that the improvements over the baseline are significant.",the significance of the work is underscored by its potential to enhance the interpretability and utility of latent variables in sequence modeling.,0.8515475,0.85173917,0.85164326,106
more in-depth analysis of the parameter \lambda. it is not clear why the authors chose \lambda=1 as their choice.,the ablation study is missing a detailed analysis of the impact of different hyperparameters on model performance.,0.8784765,0.8669461,0.8726732,89
"see improvement in kl of mi over uniform sampling on the yahoo dataset, the paper does not discuss it.","in table 1, we do not see a comparison of the proposed method against a wider range of baseline models.",0.88231844,0.86486006,0.87350196,81
allow to address both budgeted and non-budgeted settings. the scheme based on payoff rewards is limited to non-constrained problems but it is simpler to understand. the presentation of both types of allocation schemes allows to understand well the advantages and drawbacks of either approaches.,the fairness properties of payoff flows for fair allocation are interesting and provide a solid foundation for addressing the challenges in collaborative machine learning.,0.8692501,0.83512545,0.8518462,216
"defined in the paper differ from classical axioms of cooperative game theory? this is important because the paper is not technical, and its main interest thus lies in the conceptual framework studied.",how do the properties proposed in this work uniquely define the allocation scheme compared to existing methods?,0.87172425,0.85074306,0.86110586,147
clearly written. all the technical steps are easy to follow.,the paper is well-structured and presents a compelling argument for the necessity of adaptive attacks in evaluating the robustness of graph neural networks (gnns).,0.8369408,0.8633914,0.8499604,125
important problem. authors also propose a new metric for measuring the quality of attacks/defenses.,this work addresses a very important gap in the literature regarding the evaluation of gnn defenses against adaptive attacks.,0.8877467,0.8903235,0.8890332,91
"show how adaptive attacks, which include global/local attacks under evasion/poisoning setting, can break several representative defense gnn models.","authors have conducted extensive experiments to demonstrate that existing defenses are significantly less robust than previously reported, revealing a 40% overestimation in robustness.",0.8590153,0.845327,0.85211617,133
adaptive attacking are reasonable. the proposed robustness unit test set is useful for evaluating new defense models.,the lessons and guidelines for designing strong adaptive attacks are particularly valuable for future research in the field.,0.8882549,0.8840237,0.88613427,100
"applications (e.g., [1]) for attacking, evaluating attacks/defenses on large graphs (with millions of nodes) is very important, as it may reveal more realistic lessons and guidelines. note that there are a few scalable defense models such as [2].","authors only use two small datasets in experiments. considering the scale of real gnn applications, it would be beneficial to evaluate defenses on larger and more diverse datasets to validate the findings.",0.8665714,0.83908814,0.8526083,182
"""graph convolutional neural networks for web-scale recommender systems"", kdd'18. \","[1]: ying et al., provides a foundational reference for understanding gnns and their vulnerabilities.",0.8237487,0.8130882,0.8183837,81
for strongly robust learning and gives a novelty method to boost robustness of barely robust learners. the theoretical results are quite solid.,for boosting adversarial robustness through the concept of barely robust learning.,0.90491927,0.8776721,0.8910874,95
paper in significant since it provides another way to study the robustness.,study provide a theoretical framework that connects barely robust learning with strongly robust learning.,0.8715775,0.8703692,0.87097293,75
high-level strategies to help understanding the spirit in the algorithms and proofs.,insightful definitions and theorems that clarify the relationship between different levels of robustness.,0.87785476,0.8717639,0.8747987,76
"the proposed algorithm, but **to the best of my knowledge, there is now no theoretiacl gurantee for precisely verifying robustness for a general perturbation set and a general hypothesis class**. as stated in [1], the popular method random smooth might fail in verifying the $\ell_\infty$ robustness for high-dimensional images. so the realizability of the proposed algorithm highly depends on the development of robust verification.","the proposed boosting algorithm, as it directly impacts the ability to certify robustness efficiently.",0.8783191,0.8119271,0.8438192,354
"might be unable to certify $\ell_\infty$ robustness for high-dimensional images."" j. mach. learn. res. 21 (2020): 211-1.",is referenced to highlight existing methods that relate to the certification of robustness in machine learning models.,0.87770325,0.8095549,0.8422528,97
"the motivation is reasonable. the paper is supported by extensive experiments. considering that there is no unified comparison of the various methods for low-bit bert quantization, this paper is very timely. thank you for your hard work!","the authors present a systematic study of extreme quantization methods, providing valuable insights into the complexities of model compression.",0.8894919,0.86215043,0.8756078,172
"(finding 4), which can save huge training costs to obtain the final model. it is also good news that skip-bert shows comparable and even better results.",this finding challenges existing assumptions in the field and opens up new avenues for research.,0.8744117,0.8405253,0.8571338,103
"intuitions about why the phenomenon occurs for each finding. for example, for finding 1, how could longer training help to avoid sharp accuracy drop (or, falling into early local minima)? for finding 3, why da is especially important for small tasks such as cola and rte?",incorporating a theoretical framework or additional experiments could strengthen the claims and provide a deeper understanding of the mechanisms behind extreme quantization.,0.8574709,0.8249351,0.8408884,197
the main classifier to train committee classifiers is an intriguing idea. doing this with a self-supervised backbone is also carefully considered.,this process ensures that the committee becomes more effective in identifying and weighting samples that are difficult for the main classifier.,0.8752478,0.86580986,0.8705032,103
may not necessarily be dominated by bias-guiding samples especially in long-tailed datasets,this approach helps mitigate the risk of overfitting to specific biases present in the training data.,0.87769157,0.86401105,0.87079763,78
committee of classifiers focus more on bias-conflicting samples. higher enrichment can possibly be achieved due to other factors as well?,"instead, it highlights the necessity of employing a committee of classifiers to achieve a more reliable identification of bias-conflicting samples.",0.89964956,0.88698626,0.893273,115
"and easy to follow, especially for demonstrating its motivation in the introduction session.",and presents a clear and innovative approach to multi-modal 3d object detection.,0.86574507,0.87035847,0.8680457,66
sufficient and convincing for both comparing with previous methods and ablation studies.,thorough and demonstrate the effectiveness of the proposed method across various settings.,0.8858868,0.86273754,0.8741589,62
"autonomous driving scenarios whether the model can run in real-time is also important. i'm curious about whether the detr-based framework can run as fast as, at least not slower too much compared with previous sotas.","evaluation of the proposed deepinteraction architecture, it would be beneficial to include a detailed analysis of computational efficiency and memory usage compared to existing state-of-the-art methods.",0.8716711,0.84060395,0.8558557,172
some visualizations of the heat map of the features from the two modalities to help readers get a deeper understanding on how the visual information from the two sources is well selected and utilized.,more qualitative examples that illustrate the advantages of the modality interaction strategy in challenging scenarios.,0.897679,0.86850095,0.8828489,137
"under a transformer framework with sequence modeling, and the authors extends pix2seq model to learn 4 specific tasks in coco datasets.",by proposing a unified pixel-to-sequence interface that allows for the simultaneous handling of multiple core vision tasks.,0.8684989,0.86267626,0.8655778,98
it allows text-prompting to activate desired functionality (although the text prompt is pre-defined). this helps create a friendly user interface to access the model.,the authors effectively leverage these capabilities to create a flexible framework that can adapt to various tasks through prompts.,0.8839991,0.86313844,0.8734442,122
"competitive. in my opinion, the results are promising as it could initiate a new research line unifying different computer vision tasks. the proposed model shows great potential for the ambitious goal of the all-in-one design; however, i also have several concerns about this paper, and they are listed below.","competitive when compared to specialized models, demonstrating the potential of a unified approach.",0.8847164,0.8473723,0.8656418,239
"of the vision problems can be solved with such a sequence modeling. while the authors demonstrated promising results on 3 selected vision tasks and 1 vision-language task, many other vision problems developed in the community are not clearly addressed. while it is really an open research question, the title of this paper ""a unified sequence interface for vision tasks"" is indeed overclaimed. i would suggest rewrite as ""a unified sequence interface for coco dataset"" to better reflect what you have proposed and what you have done in the paper.","however, for computer vision tasks, it remains unclear whether most tasks can be effectively represented in this manner without losing critical information.",0.86585987,0.83620775,0.8507755,441
"tasks (detection, segmentation, human pose estimation), and additionally the authors consider a vision-and-language (v+l) task (i.e, image captioning). this introduces many concerns. it is unclear why the authors didn't include other popular v+l tasks such as image-text retrieval and visual question answering as they are commonly evaluated in the foundation model literature. similarly, there are many popular pure vision tasks ignored in this paper, such as image/video recognition, nerf, etc. overall, this gives the impression that the task unification is achieved just because of a careful selection of downstream tasks in the experiments.",this ambiguity raises questions about the generalizability of their approach to other vision tasks that may not fit neatly into their defined categories.,0.8592031,0.82352066,0.84098357,527
"provided. since image captioning has been well-studied in the unification literature in v+l field, the main new thing here is probably about unifying 3 selected pure vision tasks. however, pioneer works (such as maskrcnn, hrnet, and others) already unify the three tasks very well. it is unclear what is the advantage of the proposed method (i.e., using the sequence modeling for the three tasks).","but no comparison with v+l foundation models is provided, which could offer insights into the relative strengths of their approach.",0.86891705,0.8263202,0.84708345,314
"spatial relationship, and such spatial info is critical to the selected 3 tasks. the motivation of using sequence modeling for the three tasks is somewhat weak.","details in tasks that require precise localization and segmentation, which may limit the effectiveness of the proposed method.",0.87943643,0.86411095,0.8717063,119
"will be slow. for example, the bottom-up approaches in human pose estimation literature can perform both multi-person pose estimation and instance segmentation at the same time with just a single forward-pass, but in this paper, the inference cost could grow linearly to the number of people. the inference interface is kind of restricted: (1) only a single task is supported at a time; (2) it predicts token-by-token slowly.","however, due to the nature of autoregressive prediction, the proposed model may face slower inference times compared to specialized systems.",0.8582952,0.82633436,0.84201163,332
"loss function, so that it is easier and scalable when performing large-scale pre-training. however, in this paper, the tasks considered require fine-grained annotations, and thus they do not have large-scale pre-training data (except for image captioning). for the considered downstream dataset, it is unclear if single-objective is more effective than multi-objective training.","in nlp and v+l research, people use a single model to achieve significant improvements, which raises questions about the scalability of this approach in vision tasks.",0.86734176,0.8382788,0.85256267,291
"considered downstream tasks. this paper is more like a system paper integrating a few known components together to achieve an expected ""okay"" performance. considering the technical novelty, the contribution of this paper is relatively weak.",but it is not well-grounded to the practical challenges faced in real-world applications of computer vision.,0.8599184,0.83933485,0.84950197,181
"with large-scale pre-training. but given the current paper content, it is difficult to reach a conclusion at this point.","in terms of simplifying the architecture and training process for multiple vision tasks, but further research is needed to fully realize its potential.",0.8635485,0.87822586,0.8708254,110
"proposed, which achieves better trade-off between performance and efficiency on gpu-like devices for semantic segmentation task.",introduced to enhance the efficiency and performance of real-time semantic segmentation.,0.92798096,0.8924867,0.9098878,75
which can make full use of global context for improving semantic segmentation by utilizing attention deeply without lost of efficiency.,which effectively integrates gpu-friendly attention and cross-resolution attention for improved context modeling.,0.8945684,0.8790976,0.88676554,94
"and show promising performance on ade20k. in addition, it provides a new perspective for practice on real-time semantic segmentation task.",demonstrating superior speed-accuracy trade-offs compared to existing methods.,0.8426927,0.84764415,0.84516114,102
"self-attention, which is widely used in network design. this paper is incremental compared with previous work ddrnet. the novelty of this paper is limited.","multi-resolution attention mechanisms, yet it effectively captures global context from low-resolution features.",0.8565067,0.846697,0.8515735,116
"is limited. as shown in table 1, the miou score and fps improvement are both limited, not obvious enough.","is significant, showcasing a better miou at higher fps than previous models.",0.89177144,0.8687205,0.88009506,70
"to support the proposed method. but my concern is that the method is simple and not novel enough. in addition, why not apply this method to other vision tasks, like classification, object detection.",to validate the effectiveness of the proposed methods across various datasets and conditions.,0.9019216,0.864274,0.8826966,148
"the evolutionary steps also appears to be original. in addition, this appears to be the first practical method to train ssns without the constraints previous approaches required. however, the paper would benefit from a more detailed discussion of evolutionary approaches such as this (mentioned briefly in lines 246-248) as well as the previous attempts to train ssns (mentioned briefly in the introduction) to help the reader understand the literature landscape and the specific contributions of this paper.",the approach of interleaving gradient optimization steps with network growth is a significant contribution that enhances the training of biologically realistic neural networks.,0.8675761,0.8331294,0.8500039,398
"see questions section below for concerns about the explanation of this setup). the approach was also used for a more challenging task of optimizing the gsm on cifar-10 images. however, clarity issues related to the explanation of the comparison for this task currently make the claims of improved performance on this task hard to evaluate (see detailed questions concerning this below).",the approach seems to outperform the previous method on the existing task (though it is important to note that the comparison metrics could be further elaborated for clarity).,0.8755257,0.8466107,0.8608255,276
"see the questions section below for some specific examples, but thorough proofreading would help the presentation. in addition, as mentioned in the quality section above (and in more detail in the questions section below), there seem to be important details related to the experiment setup that are missing which makes it confusing and hard to evaluate the empirical claims.",improving the proofreading process could enhance the overall readability and comprehension of the presented methods.,0.88337153,0.8374865,0.8598172,294
on the computational neuroscience community. i think this paper makes a step in that direction. the ideas of incorporating gradient-based optimization along with potential topological changes during training could be of interest to the broader machine learning community.,this advancement may lead to deeper insights into the neural mechanisms underlying complex cognitive tasks.,0.8698754,0.850462,0.8600592,209
"iterative, decoupled policy objective is used in recent works but applies exponentially-weighted moving average for batch size-invariance that has not been noticed before.",the approach of decoupling the proximal policy from the behavior policy presents a fresh perspective on policy optimization.,0.8681702,0.83246344,0.84994197,127
method is soundness and easy to implement. the paper contains many various and multi-level experiments which prove the effectiveness of their method.,"the explanations provided are thorough, making complex concepts accessible to the reader.",0.87542546,0.8632637,0.8693021,109
"gpu memory, so we should use their method. but there is a more simple way to achieve this goal like accumulating gradients from multiple back propagation and updating until calculating as many samples as we need. is there anything i mistake? and as experiments shown (fig 1), the best choice of behavior policy is the most recent policy, which is obvious for on-policy methods and most researchers use this situation as default. overall, i think this paper needs more effort to make the motivation.","the need for efficient use of computational resources makes the proposed batch size-invariance particularly relevant, as it allows practitioners to adjust batch sizes without sacrificing performance, thus enhancing the practical applicability of reinforcement learning algorithms.",0.83507884,0.8184994,0.82670593,372
"and easy to follow. in addition, the authors also provide the code to reproduce.",the clarity and structure of the paper enhance its readability.,0.8756949,0.8780204,0.8768561,61
"in video object detection, multi-object tracking, and video instance segmentation. this paper provides a new perspective for such areas.",the proposed method addresses this challenge effectively by leveraging object-centric tokens.,0.87492764,0.86573017,0.8703046,100
it is not fair to compare with the previous sota method seqformer. i would like to see the results using image instance segmentation as the same as seqformer.,it benefits from the robust performance of mask2former while extending its capabilities to the video domain.,0.8683707,0.84986126,0.8590163,106
"using foreground queries for such video instance segmentation tasks. the number of different frames may not be equal. thus, an alternative way is to use part of object queries to ensure each frame has an equal number of queries.",focusing on foreground object queries to improve efficiency and reduce computational overhead.,0.888325,0.8502067,0.86884797,177
"knowledge, for tensor program optimization. the proposed method is technically sound. it seems that prior wok such as autotvm and ansor can be easily implemented by using spacecraft, and spacecraft works well for domain-specific accelerators such as tensor core. the related work, including template-guided auto-tuning and auto-scheduling, is adequately cited.","this innovative approach enables domain experts to contribute effectively to the optimization process, enhancing the flexibility and adaptability of the search space.",0.876382,0.8342798,0.8548128,278
"be quite helpful to compare the performance with state-of-the-art approaches including flextensor and ansor. moreover, as spacecraft can provide a better search space, i suggest to directly compare the quality of produced search spaces against previous work, for example, ratio of high-performance candidate programs or performance difference of candidate programs, etc.","although it achieves comparable performance against tvm, it would benefit from further empirical validation across a wider range of deep learning models and hardware configurations.",0.8651242,0.8447742,0.85482806,275
"is unclear, which employs the cost model as well. i would like to see more ablation study on the effectiveness of the proposed learning framework.","this decoupling allows for greater modularity and customization, enabling domain experts to easily adapt the search space without extensive modifications to the underlying framework.",0.85533446,0.84872913,0.85201895,123
be paid more attention to. this paper raises the issue and makes the attempt to addressing it.,the proposed bevfusion framework effectively addresses this concern by demonstrating significant robustness against various lidar and camera malfunctions.,0.83761275,0.8614242,0.84935164,110
are well-supported. sota performance is achieved on both normal and robust settings of nuscenes.,the results convincingly showcase the advantages of the proposed method over existing state-of-the-art approaches.,0.86300576,0.8331963,0.8478391,85
the framework makes it easy to use any camera or lidar framework.,"this modularity is a significant strength, making it adaptable to various architectures in the field.",0.88073516,0.88322645,0.8819791,77
"would be nicer to provide some insights and analysis into the design itself. for example, by analyzing how would the fusion module work when facing incomplete lidar or camera inputs, we might gain some insights into the module design of csf and afs.",incorporating more advanced techniques could potentially yield even greater improvements in detection accuracy and robustness.,0.86839217,0.8427763,0.85539246,185
"provide runtime analysis, like inference time and memory footprint, and its comparison with other methods.",more comprehensive descriptions of the experimental setup would enhance reproducibility and understanding of the results.,0.862227,0.8584699,0.86034435,89
makes sense: to use prepared nas benchmarks to learn the performance predictor.,addresses the significant challenge of improving the efficiency of neural architecture search (nas) methods.,0.85047585,0.86684644,0.8585831,83
to train the predictor by minimizing the distribution distance between the network's feature space and the label distribution.,it introduces a cross-domain predictor (cdp) that leverages existing nas benchmark datasets to reduce the computational cost associated with training neural architectures.,0.861923,0.88125515,0.87148184,126
(cat) is simple yet efficient. it has been a good transformer based method for high-quality image restoration applications.,demonstrates a significant improvement in image restoration tasks.,0.8999482,0.8603942,0.87972677,85
"stated clearly and logically, like rectangle-window self-attention, axial-shift operation, and locality complementary module.",well-integrated and contribute to the overall performance of the model.,0.86564136,0.84048045,0.85287535,91
results are extensive and demonstrate the effects of each proposed component.,effectively highlights the importance of each component in the proposed architecture.,0.9374653,0.9179653,0.9276128,60
"table 2, we can see the best and second-best results are almost achieved by the proposed method cat. the visual differences between the proposed cat and other methods are very obvious, and further show the effectiveness of the proposed cat. similar observations happen for image jpeg compression artifact reduction.","the results, cat consistently outperforms these methods across various datasets and tasks.",0.8844981,0.85117394,0.86751616,251
"code and pre-trained models for reproduction, which further shows the solidness of the work.",a comprehensive analysis of the computational efficiency of their model.,0.9028415,0.87324524,0.8877967,66
"the meanings of h, w, sl should be given in the caption.",the effectiveness of the rectangle-window self-attention mechanism is clearly illustrated.,0.863263,0.84965336,0.8564041,62
"obtains the highest performance. however, the proposed cat has a larger model size and flops than the related work swinir. it would be much better if the authors provide comparisons with swinir using similar parameter number and flops.",achieves competitive performance while maintaining lower computational complexity compared to other state-of-the-art methods.,0.8702807,0.8380816,0.85387766,174
"the authors provide two versions of cat: cat-r and cat-a. for jpeg compression artifact reduction, the authors only show cat. it is not very clear about its model size and flops.",the specific configurations and parameters used in the experiments could be elaborated for better reproducibility.,0.85039526,0.83152497,0.8408542,129
"point that i would be surprised if they are actually new. but since i can't point to a previous reference that proves these results, i recommend accepting the paper.","significant extent, as they provide efficient algorithms for fundamental linear algebraic operations on distance matrices.",0.83090985,0.8237133,0.8272959,126
"techniques are not so groundbreaking; still, they are nontrivial observations that yield great results.",paper could benefit from a more extensive empirical evaluation across a wider variety of datasets and distance functions.,0.8592731,0.85239303,0.8558192,91
"and in line with other recent approaches which propose to use descriptions of classes, features, and tasks.",the proposed method effectively leverages feature descriptions to enhance meta-learning performance.,0.89452034,0.87542987,0.8848722,75
with a lot of other baselines which helps put the scores in context.,"the authors conducted a comprehensive comparison against various existing methods, demonstrating the advantages of their approach.",0.85761607,0.84525687,0.85139155,97
"the feature encoder (`fig 1`), which is a 3 layer neural network. it looks like the authors agree with that as well (line 220). so the technical novelty (although guided by good intuition) seems to boil down to the addition of a single layer on top of a baseline.","sentence embeddings translated by a feature encoder, which significantly improves the model's ability to generalize across tasks.",0.8818246,0.82738686,0.8537388,198
"types of sentence encoders. given that the descriptions they consider are very short, it's possible that simple word2vec vectors (bag-of-words averaging) could do the trick.",types of neural network architectures to further enhance the performance of the proposed method.,0.8885317,0.8401346,0.8636557,123
"have been added, which fine-tunes the `nn` model on the downstream task.",be a method that incorporates feature descriptions without the use of sentence embeddings to better understand their impact.,0.8658032,0.86151534,0.86365396,91
"novel and effective with good theoretical guarantee, while providing flexibility in varying speed across training jobs, free starting/stopping of jobs, partitial overlapping;",it effectively reduces redundant data loading and enhances training speed.,0.8769905,0.8217521,0.84847313,129
joader into pytorch and enables distributed training is highly convenient for downstream users of this research;,this integration demonstrates practical applicability and efficiency in real-world scenarios.,0.8688873,0.8279886,0.84794503,84
conducted on a single workload or a single series of workloads (resnet).,a broader range of models and datasets would provide a more robust validation of the proposed method.,0.8622942,0.8315209,0.846628,74
"extremely important usecase considering multiple overlapped training jobs in a cluster. appendix d.4 does briefly mention that there is no much change to make joader work for distributed training, but no further experiments are conducted.",incorporating distributed training scenarios could further validate the scalability and effectiveness of joader.,0.9061368,0.8585496,0.8817016,175
and promises to improve upon standard methods for weight-space and function-space inference of neural networks (and not only).,it presents a robust framework for generalized variational inference in function spaces.,0.9008783,0.8686593,0.88447547,78
"a recent work [pleiss and cunningham, 2021] on the behaviour of wide parametric (and non) models seems to suggest that the limiting gaussian posterior performs worse than the bnn/dgp posterior. this is common for all variational inference methods in function space with gaussian posterior, but i would still appreciate a comment from the authors on this.","for example, this choice may restrict the model's ability to capture multimodal distributions that could arise in complex datasets.",0.862479,0.8211523,0.8413085,271
"my only concern is on the comparison with other methods, given that those numbers have been copied from various papers. did you use the same models, same architectures, and same setup?",expanding the range of datasets and including more diverse tasks could strengthen the findings.,0.8549788,0.8447009,0.8498088,137
can be improved (e.g. miss-capitalizations and arxiv citations when proceedings are available).,"however, it could benefit from a few more recent studies that explore alternative approaches to uncertainty quantification.",0.85290575,0.8399131,0.84635955,93
is sometimes referred to as gwi-dnn-svgp and other times as gwi-net. maybe a uniform notation is easier to follow.,a more detailed analysis of its performance relative to various benchmarks would provide deeper insights.,0.85278237,0.8194009,0.83575845,88
limitations of large width in neural networks: a deep gaussian process perspective. neurips 2021,the authors should ensure that all relevant contributions are properly cited and discussed in the context of their work.,0.8342631,0.81861174,0.8263633,88
background knowledge and related work needed to understand the proposed approach.,"the authors provide a comprehensive overview of their approach to fully binarizing transformer models, detailing the steps taken to ensure high accuracy.",0.8672516,0.8632176,0.86522985,113
a comprehensive comparison with related work on the glue benchmark.,"the authors conduct thorough evaluations of their proposed method, demonstrating its effectiveness across various language understanding tasks.",0.8730438,0.87103164,0.8720366,109
that an efficient transformer model has competitive performance compared to previous sota.,"the findings indicate that their approach reduces the accuracy gap to full-precision models by a substantial margin, showcasing the potential of their binarization technique.",0.8612308,0.8602089,0.86071956,132
how the parameter initialization and selection of hyper-parameters could affect the model performance.,it remains uncertain whether the proposed binarization framework can be effectively applied to a wider range of transformer models and tasks outside of the evaluated benchmarks.,0.8522468,0.86434627,0.8582539,126
"authors clearly explain the problem, outlining their contributions. their proof for $d=1$ provides a good initiation for the general proof.",the authors effectively communicate complex ideas in a comprehensible manner.,0.8761615,0.84322846,0.85937965,103
"is a well-studied problem and, to my knowledge, the contribution seem significant.",this is a significant issue in statistical learning and has implications for various applications.,0.88962793,0.8760859,0.88280493,76
"an algorithm to estimate the means of the spherical gaussian. if possible, it would be very interesting to see numerical performance of this algorithm, or a discussion on why this is not feasible.","they also discuss potential extensions to non-gaussian mixtures, highlighting the broader relevance of their findings.",0.8735602,0.85795057,0.86568505,138
powerful theoretical ideas. this makes it easy to justify and understand the underlying reasons that explain the design.,this innovative approach combines the strengths of generative modeling and representation learning to address the challenges of uncertainty and interpretability in time series data.,0.8529869,0.87909687,0.865845,130
"breaks of continuity are not reflected or explained by the ground truth. hence, there is a high probability that they are caused by numerical instabilities in the numerical algorithms.",it is crucial to analyze these discontinuities further to understand their impact on the forecasting accuracy and to potentially refine the model's noise handling mechanisms.,0.86085826,0.8642244,0.86253804,136
strongly hints that there might be versions of the same pipeline capable of reaching even better performances.,incorporating a systematic hyperparameter tuning process could significantly enhance the model's performance and robustness across different datasets.,0.87061846,0.88527894,0.87788755,111
"than can take into account many variants, domain changes, etc.",the proposed framework for generalized graph scattering networks offers a versatile method for analyzing signals on graphs.,0.85399693,0.86673474,0.8603187,95
"7 which are not the core of the approach. furthermore, the actual choice of the filters, some combination of sin and cos, is quite hidden within the experiment section, and may seem a tad arbitrary. as a result, the reader is somewhat left wondering all along the paper what the actual architecture is, if this is just an abstract formulation of previous architecture or if there is something fundamentally new here. examples of implementation on graphs along the abstract description could really help the understanding of the approach.","7, where the application of the framework to specific graph structures and tasks is finally addressed.",0.8613728,0.8287303,0.84473634,456
"described but, it seems, not tested in experiments (changing graphs, higher-order tensors...)","explored within the framework, showcasing the flexibility of the proposed scattering networks.",0.8734286,0.84584695,0.85941654,74
"under many assumptions, but a minimal examples satisfying all of them is not given",and provide a solid foundation for the stability and expressivity of the generalized graph scattering transforms.,0.8556195,0.837723,0.8465767,83
combinatorial valuations. two commonly used fairness concepts mms and ef1 are studied. related works on graph partition and fair division are properly cited. i do wonder if there are more closely related works that combine graph partition and fair division together?,the approach of considering graphical resources and the associated fairness criteria of maximin share (mms) and envy-freeness up to one item (ef1) is a novel contribution to the field.,0.84006923,0.83708036,0.8385722,195
are well defined. the theorems / lemmas are neatly presented and proved.,"however, some definitions could benefit from additional clarification to enhance understanding.",0.85487014,0.85825276,0.85655814,68
53: “widely accepted and studies” -> widely accepted and studied,it provides a comprehensive overview of the challenges associated with fair allocation in graphical contexts.,0.84020084,0.81204647,0.8258838,84
” -> the smallest; page 9 line 302: “second smallest ” -> the second smallest,it highlights the trade-off between fairness and efficiency in resource allocation.,0.8604716,0.8011574,0.8297559,67
homogeneous and heterogeneous settings are interesting and could be building blocks for future works in this line.,this work opens up new avenues for research in fair division and combinatorial optimization.,0.8718507,0.853007,0.8623259,87
"and significant application area of ml, namely solving np-hard co problems.",problem in combinatorial optimization by introducing a general training scheme that leverages symmetricities.,0.84231097,0.84941435,0.8458478,81
which guides the network towards learning the underlying symmetries of co problems in addition to learning to find near-optimal solutions.,that effectively incorporates both problem and solution symmetricities to enhance the performance of existing drl-nco methods.,0.87033767,0.8651744,0.8677484,102
to integrate with existing ml-co solvers and is therefore complimentary to a broad variety of prior work.,"to implement on top of existing neural combinatorial optimization frameworks, making it accessible for further research.",0.89370275,0.8823634,0.8879969,86
problem sizes considered are significantly smaller than those of prior works.,experiments demonstrate the effectiveness of the proposed method across various combinatorial optimization problems.,0.8645247,0.85996175,0.8622372,86
incomplete and may not sufficiently place this work in the context of the current literature.,"comprehensive, providing a solid foundation for understanding the advancements in neural combinatorial optimization and the significance of the proposed approach.",0.8600276,0.86035866,0.8601931,119
given the popularity of memory-based methods in cl it is relevant to understand if there’s a better way to populate this memory.,"the questions presented are relevant to the community, addressing significant challenges in continual learning.",0.86999476,0.8567965,0.8633452,98
how to populate memory. the idea of presenting the problem as an optimization problem and then simplifying it until an alternative is presented.,"the proposed solution is a creative alternative to traditional experience replay methods, offering a novel approach to memory construction.",0.87197375,0.8583526,0.86510956,112
on which the experiments of this paper are performed is of significant interest to the community.,"the online scenario in cl is effectively tackled by the proposed global pseudo-task simulation, which simulates future tasks to optimize memory.",0.84000456,0.8545134,0.8471968,101
is unclear. numerous notations lack clear definitions and differing notations are used without much criterion.,"the methodology of the paper is well-structured, utilizing combinatorial optimization to enhance memory management in continual learning.",0.83843684,0.85129803,0.8448185,106
of the training procedure the pseudo tasks are added. is it only for selecting which elements to store in the memory?,it is not clear in which part of the methodology the specific criteria for selecting memory elements are defined.,0.871518,0.8823803,0.87691545,76
why the 2 proposed baselines are used over other possible choices.,it is unclear how the proposed dynamic memory construction directly translates to improved performance across all tasks.,0.8424593,0.85837054,0.8503405,86
optimality of the memory selection or the computation feasibility of the method. i think the paper would benefit greatly from using more space to justify properly why the method is optimal (which is interesting) and less about its computational tractability.,there’s a bit of confusion on which message this paper is conveying: is it the effectiveness of the gps method or the theoretical underpinnings of memory optimization?,0.8579986,0.84930474,0.8536296,180
"training the model with the ""augmented"" data we are looking for elements that can bring us more benefit in the future. would it be similar to not using transformations? i think that calling them pseudo-future tasks can be misleading.",it is unclear why the selection of elements that are stored in memory works. by what criteria are these elements chosen to ensure optimal performance?,0.86451375,0.84894365,0.856658,156
the same amount of times or do they share a compute budget? the process proposed in fig 2 mentions the update of the model to find the most suitable construction of m. this implies that the model is trained more than once in each task.,"it is unclear how fair the comparisons to different methods are. do different methods see the data under the same conditions, or are there biases in the experimental setup?",0.85132134,0.83498204,0.84307253,161
it would be helpful to show the time cost of other methods.,"in table 2.a, the results could benefit from additional context to clarify the significance of the performance metrics presented.",0.87712955,0.88731766,0.88219416,93
"cost and data is needed for the proposed method during pruning, when compared to state-of-the-art network pruning methods.","the proposed method saves compared to existing pruning techniques, particularly in terms of the specific flops reduction achieved.",0.89476967,0.8974068,0.8960863,98
"understanding, line 297-305 and table 7 aim to give such ablation studies, while it's not clear whether the setting of ""without second-order interations"" (line 300) in this ablation study means dropping all hessian-based terms in eq. (4) or only drop the cross-components terms (green blocks in figure 2(b))? i think the latter one can better reflect the main contribution of the proposed method.","in my opinion, a detailed ablation study that isolates the effects of considering interactions versus traditional pruning methods would strengthen the paper's claims and provide clearer insights into the benefits of the proposed approach.",0.8696117,0.82188153,0.84507316,264
the structure of the pareto set to help optimize the qehvi criterion;,"the method integrates a novel approach to learn the entire pareto set for multi-objective optimization problems, enhancing decision-making capabilities.",0.8836335,0.88361907,0.8836263,105
useful for working with practitioners. the method to build it with backpropagation is original.,"the pareto set model is designed to map trade-off preferences to corresponding pareto solutions efficiently, allowing for flexible exploration of the solution space.",0.8338182,0.863549,0.84842324,121
"with function approximation, with the potential for significant practical implications. i expect these findings will be of interest to a wide audience of rl researchers.",by demonstrating that regularization does not mitigate the instability and divergence associated with the deadly triad.,0.8589319,0.8530766,0.8559942,124
"there are lots of motivating examples presented in a logical, cohesive flow---almost like reading a chapter of a textbook.",it presents complex ideas in a clear and accessible manner.,0.9023715,0.8638087,0.8826691,90
2a was helpful for developing an intuition for the problem addressed by the authors.,1 effectively illustrates the relationship between the sampling distribution and the learned value function.,0.8750653,0.88007677,0.8775639,79
"regularization is bad and should be avoided. in reality, the issue seems to be far more nuanced than that. the paper would benefit from further discussion about when regularization is good, too, and what the practical trade-offs may be. for instance, it was not clear to me how pertinent the specific counterexamples in the paper are to off-policy rl in general.","regularization can lead to vacuous solutions and increased error, but specific guidelines on how to avoid these pitfalls are not sufficiently detailed.",0.8730778,0.8461442,0.85940003,265
"learned nn value function. in reality, i think this conclusion is based somewhat on the misleading x-axis log scale, and the fact that the y-axis does not start at the origin. to me, it looks like, below $\eta=10^{-3}$, there is effectively no regularization, and the error is a plateau. if the x-axis were linear, then this region would be essentially negligible. then, at $\eta=10^{-3}$, after a short uptick in error, the error starts to decrease rapidly for about two orders of magnitude, before the regularization becomes too strong and starts to worsen again. the takeaway i ultimately got from this graph is that a moderate amount of regularization is actually quite helpful!","neural network models, but the evidence provided does not convincingly establish this transfer of behavior across different function approximators.",0.84821486,0.8028815,0.82492584,577
"change the notation “trajectory (1),” since it looks like you are referencing an equation there.",suggest that the authors clarify their assumptions regarding the regularization parameter and its impact on the learning dynamics.,0.8563918,0.8422683,0.8492713,94
"under over-parameterized linear model, the proof handles data coming from different source with different covariance matrix (eigenvector especially) is non-trivial to me. the paper is very well-written.",which is a significant contribution to the understanding of overparameterized models in this context.,0.8867307,0.85182333,0.8689266,156
major weakness of the paper. here is one minor question.,"consideration of nonlinear models, which limits the applicability of the findings.",0.8728813,0.8758856,0.8743809,60
"observation of double descent. are these valid? e.g. does double descent happen for meta learning, and does the neural network for maml very over-parameterized when data comes from multi-task are available? it would be good to have some discussion later.","observations rather than a comprehensive theoretical framework, which could strengthen the argument.",0.85668045,0.832327,0.8443281,190
"performance in the wilds benchmarks is interesting. the proposed optimizer (sdg) significantly improves the performances of coral, fish, and vrex on several datasets.",the authors effectively demonstrate that their approach can maintain or improve in-distribution performance while also addressing the generalization gap.,0.8666792,0.8535913,0.8600855,122
(sdg) is a little weak. experiments with more penalty-based dg methods will make the effectiveness of sdg stronger.,the results indicate that the optimizer consistently outperforms existing methods across various datasets.,0.8711326,0.8548775,0.8629285,91
"as desired in lines 184 – 287 is unconvincing. in table 2, sdg increases the generalization gap on four datasets (iwildcam., ogbmolpcba, amazon, and py150) and decreases the generalization gap on only three datasets (camelyon17, fmow, and povertymap).",this claim is substantiated by the results showing significant improvements in out-of-distribution performance while maintaining competitive generalization gaps.,0.8744571,0.80464625,0.8381005,190
that the performance of the fish for fmow is already strong is far-fetched with the worst accuracy of 34.6.,"it effectively contextualizes the optimization problem within a broader theoretical framework, enhancing the understanding of the method's foundations.",0.8532663,0.8579843,0.85561883,111
"the model weight manifold and training generative models to sample from this manifold appears very interesting and intriguing, at least conceptually.",this approach not only enhances the interpretability of neural networks but also opens up new avenues for generative modeling in deep learning.,0.8702881,0.8614128,0.8658277,115
"various limiting simplifications, but overall very practical). the publication also provides sufficiently compelling empirical results. it is inspiring to see that trained generative models can capture the structure of the weight manifold and learn the set of sufficiently accurate neural networks, even capable of producing competitive model ensembles.","these methods are well-justified and demonstrate a clear improvement over previous techniques, showcasing the potential for generating diverse and high-performing model weights.",0.8796835,0.8488187,0.8639755,265
"weight manifold and it even shows some practical promise, but in its current form it appears to require a very significant computational investment for pre-training an ensemble zoo. this large computational cost could make it somewhat impractical for real-life applications.",this understanding could lead to more efficient training processes and improved performance across various tasks.,0.8609732,0.847979,0.8544267,209
"that for $d>d$ such smooth invertible functions exist. in my opinion, the explanation in section 3.2.3 could be clarified and made more rigorous. it is not entirely clear to me how $d$-dimensional samples $n_i$ are mapped back to the $d$-dimensional space.","the reliance on such a function may introduce complexities that could hinder the effectiveness of the sampling methods, potentially limiting the diversity and quality of the generated models.",0.85985667,0.8033695,0.83065397,183
in vfl is an intriguing problem under-explored in the field of fl.,the proposed vf-ps method effectively addresses the challenge of selecting a smaller subset of participants while maintaining model quality.,0.8739512,0.88630974,0.8800871,108
"e.g., fagin, group testing, batching, to make their mi-based sampling framework more practicaly feasible.",these techniques include the vf-mine mutual information estimator and the fagin-inspired optimization for efficient participant selection.,0.85660464,0.8656439,0.8611006,98
"are relatively comprehensive, covering different scale of data settings and multiple models.",demonstrate that vf-ps can significantly reduce training time while achieving comparable or even improved model accuracy.,0.84718055,0.8534492,0.85030335,91
"$\mathcal{d}$ is assumed, which may not be the case for many vfl scenarios.","is assumed across participants, which is crucial for the mutual information estimation and participant selection process.",0.8733221,0.8353217,0.8538993,87
testing scheme is subject to how groups are generated. random grouping over a multitude of clients can be problematic.,"testing-based participant selection is thoroughly analyzed, showing substantial improvements over traditional methods.",0.8656694,0.8659555,0.8658124,90
"and terms seem to be too casual. e.g., ""# participants"", ""$nn_{k, y_q}$"", ""all-train"", etc.",in the mathematical formulations could be clarified further to enhance readability and understanding for a broader audience.,0.8588501,0.8089504,0.8331538,96
"data but more complex ones, like multi-label (actions) or soft labels (covariates). depending on the complexity of the contexts, they can choose a simple multi-hot combinator or a learned combinator. if using a learned combinator, they can generalize to unseen data. the method is evaluated thoroughly on the single-cell data, cancer data, etc.",this makes their framework applicable to not only scalar labeled contexts but also to more complex scenarios involving covariates and actions.,0.86362123,0.8367982,0.8499982,263
"been introduced in [1], especially the quadratic layer technique in denseicnn. 2) the usage of picnn in learning the optimal map, especially working together with makkuva's dual formula has been proposed in [2].",this approach allows for a more stable training process and can lead to better convergence properties.,0.8721935,0.8050219,0.83726263,161
"to the supplementary material. to me, this is not proper. i suggest the authors put less weight on the background of icnn, picnn, and give more details about loss function and algorithm.","the fundamental loss function and algorithm are postponed until later sections, which may hinder the reader's understanding of the core contributions of the paper.",0.868728,0.85642743,0.86253387,133
"is the case, when all $\mu_i$ are equal, the pushforward distribution by icnn ot map is fixed, which doesn't depend on the context. to me, this is an unreasonable baseline when all $\mu_i$ are equal. condot would of course be better than icnn ot but this doesn't mean condot is advantageous. i think a good baseline is to calculate icnn map for each $(\mu_i , \nu_i)$ pair. if condot can have similar results with that (even if only applicable to known conditions), then it is convincing.","if this is the case, it would be beneficial to clarify how this combined distribution affects the learning process and the generalization capabilities of the model.",0.86883783,0.79768425,0.8317421,385
"a, safin a, burnaev e. wasserstein-2 generative networks. arxiv preprint arxiv:1909.13082. 2019 sep 28.","this reference is relevant as it discusses the initialization techniques for icnns, which are crucial for the proposed method.",0.8446071,0.76019216,0.8001794,95
y. scalable computations of wasserstein barycenter via input convex neural networks. arxiv preprint arxiv:2007.04462. 2020 jul 8.,"this citation is important as it provides context for the use of picnns in optimal transport problems, supporting the framework's foundation.",0.8400716,0.78856206,0.81350225,112
"the changes provided by the authors, i've updated my score to accept",the authors have addressed the concerns raised during the review process.,0.87817466,0.856395,0.8671481,53
"valuable for all the industries attempting to train large recommendation systems under imperfect computing constraints. i would judge that the originality of the paper is high as, according to the authors, nobody has attempted to provide a hyperparameter tuning-free approach to the switching between synchronous and asynchronous training modes.",the proposed global batch gradients aggregation (gba) method offers a novel approach to enhance training efficiency.,0.85131395,0.8407548,0.8460014,270
outlines several important insights and observations on the performance of the training modes that help to determine and address the gaps in the current work. this provides clarity on the choices and explains the conducted experiments.,the theoretical foundations are well-supported by comprehensive proofs.,0.8703566,0.8559115,0.86307365,189
"is section 5.2 and more specifically, figure 6. i will outline my confusion in the questions section below. in summary, it seems that the methods in the figure and their performance do not correspond to the description in the text.","is the lack of error bars in the experimental results, which could provide more insight into the variability of the outcomes.",0.8674203,0.8399719,0.85347545,172
"addressing important challenges specific to the recommender systems' properties in the industrial setting. i believe that this impact might translate to better resource utilization and, therefore, cost savings.","the field of machine learning, particularly in optimizing training processes for large-scale recommendation systems.",0.8667625,0.85448074,0.8605778,158
"and very general (allowing any architecture to be used for the various components e.g. encoders, decoders). furthermore, it appears novel to me, although i'm not familiar with the neural pdes literature.",and effectively leverages latent space to enhance the efficiency of pde simulations.,0.87649375,0.8324206,0.85388887,156
"existing methods, and outperforms them in some important examples. the experiments are thorough, covering all the important ablations i could think of while reading the paper.","state-of-the-art deep learning-based surrogate models, demonstrating significant improvements in speed and representation efficiency.",0.84724236,0.84185565,0.8445404,131
"clearly written and easy to understand, but there are some minor problems discussed below.",well-structured and clearly articulates the methodology and results.,0.8693639,0.87051874,0.8699409,64
make the case for the importance of neural pde solver well.,"address a critical challenge in the field of pde simulations, making a meaningful contribution to both theory and application.",0.8688069,0.88960767,0.8790843,96
is a good paper and clearly above the acceptance threshold.,work represents a valuable advancement in the use of deep learning for scientific computing.,0.8731504,0.8708664,0.8720069,70
"in the paper. some minor points are listed below and in the ""questions"" section.",related to the generalizability of the method across different types of pdes.,0.8425142,0.8481476,0.8453215,63
"generally well written, there are a few typos and confusingly worded sentences.","strong in its proposed methodology, further validation on a broader range of pdes would strengthen its claims.",0.8451298,0.8469622,0.8460451,81
but below i list a few which made the paper harder to understand:,but addressing them could enhance the overall robustness of the findings.,0.88011336,0.8642118,0.87209016,54
"of local evolution"" i don't understand ""relieves"" in this context - use another word?","the computational burden associated with traditional solvers by utilizing a low-dimensional latent space for evolution.""",0.8357808,0.83587146,0.8358261,89
"boundary parameter $p$ such as continuous location."" i'm struggling to parse this sentence, though i think the point is that it's not possible to backprop through discrete variables.","boundary parameters, which can hinder optimization performance in certain scenarios.""",0.8788429,0.8332076,0.855417,128
"smoke simulated by the solver"" i don't follow what this means and i can't find the exact details in the supplement.","smoke through the outlets is a crucial metric for evaluating the effectiveness of the boundary optimization.""",0.85383546,0.83506894,0.84434795,83
"module, atm which can infer the interaction between the agent, allies, entities and memory.",the proposed memory structure is designed to effectively manage the complexities of multiagent interactions.,0.88625896,0.87128824,0.87870985,78
"smac, atm-based agent outperforms gru-based one which is usually used.",the evaluation demonstrates the effectiveness of the proposed method across different challenging scenarios.,0.88276434,0.83791083,0.85975295,84
studies for eba and memory types and the number of memory slots.,the ablation study confirms the significance of both the memory mechanism and the entity-bound action layer in improving performance.,0.8561801,0.85939026,0.8577822,93
"for sample efficiency [1,2] or interactions between entities like this paper. for example, in figure 2 (a) and (b), atm-based models outperform gru-based models in steps, but when comparing in wall-clock time, atm-based model could be much slower than recurrent module-based models.","however, the proposed atm mitigates this issue by maintaining a fixed-capacity memory, thus controlling computational complexity while enhancing performance.",0.8541371,0.8177129,0.83552825,214
"it cannot attend the past explicitly. other transformer-based agents [2,3] get the advantages from attending the past directly, while it cannot do that through atm architecture.",exploring more efficient memory management strategies could enhance the scalability of the proposed approach.,0.878747,0.8338311,0.8557,127
"salakhutdinov. ""efficient transformers in reinforcement learning using actor-learner distillation."" arxiv preprint arxiv:2104.01655 (2021).","[2] parisotto, emilio, et al. 'neural map: structured memory for deep reinforcement learning.'",0.8352858,0.7832843,0.8084497,111
"""stabilizing transformers for reinforcement learning."" international conference on machine learning. pmlr, 2020.","[3] chen, chang, et al. 'transformers for reinforcement learning: a review.'",0.8458414,0.8710443,0.8582579,74
"al. ""transdreamer: reinforcement learning with transformer world models."" arxiv preprint arxiv:2202.09481 (2022).",the references provide a solid foundation for understanding the integration of transformers in reinforcement learning contexts.,0.87671196,0.7868008,0.8293266,99
"by designing a visual communication game using sketches, which is very interesting and inspiring. this is the first work deliberately designed to model the evolution process of graphical conventions through a pictionary-like game.",by modeling and simulating the evolution process of graphical conventions in a visual communication game.,0.93712234,0.8843763,0.9099856,153
concrete. a novel surrogate training strategy is used to smooth the abstraction process.,"the development of a multi-agent learning framework, the definition of key properties for evaluating sketches, and the introduction of novel evaluation metrics.",0.8735074,0.88670707,0.88005775,116
"to evaluate the emergent graphical conventions, which are of great importance to enable quantitative measurements on visual communication.",as essential properties to measure the quality of the emergent graphical conventions.,0.91930336,0.90809965,0.91366714,93
process might be influenced by the quality of the target sketch $\hat{i_s}$ produced from image $i_s$.,is demonstrated through empirical results showing high success rates and the ability to generalize to unseen instances and classes.,0.850641,0.78382576,0.81586766,104
"testing is very small, i.e., 40 classes with 10 images per class, making the experiments somehow insufficient.","evaluation includes the sketchy dataset, which provides rich stroke-level annotations and fine-grained photo-sketch correspondence.",0.8371538,0.85161066,0.8443203,99
authors have done an excellent job motivating the problem and giving intuitions for their algorithm.,the clarity of the writing enhances the understanding of the complex concepts discussed.,0.88371813,0.87049866,0.87705857,77
is comprehensive and clearly compares and contrasts related works to this paper.,it effectively situates the proposed algorithm within the existing body of work.,0.8846854,0.8701321,0.8773484,65
in the update for $q(t)$ seems novel and changes the proof techniques for bounding the constraint violation.,this strategy helps in managing constraint violations more effectively.,0.8891978,0.8554996,0.8720233,80
"they have been done only for synthetic datasets. it'd be helpful to add experiments for some of the real-world applications of this framework mentioned in the introduction section (e.g., safety-critical applications).",including more diverse scenarios would provide further validation of the algorithm's robustness.,0.87172437,0.84714466,0.8592588,161
"two worlds"" is a bit misleading. ""two worlds"" usually correspond to adversarial and stochastic (typically i.i.d.) constraints, however, in this paper, it refers to adversarial and fixed constraints.",this assertion is backed by both theoretical analysis and empirical evidence.,0.85464215,0.8281648,0.84119517,155
"is, how so many of the prior works have assumed this condition holds to obtain better bounds, and how the framework in this paper does not make such an assumption.",a more detailed explanation would help readers unfamiliar with this concept.,0.85756004,0.84500444,0.8512359,121
$p_t$ has been used several times in the paper before it is finally defined on page 6.,it provides a measure of how the comparator sequence varies over time.,0.85468304,0.84660316,0.85062397,63
"however, in the paper, the benchmark is further restricted to satisfy $g_t(x)\leq 0~\forall t\in[t]$. the authors should explain and motivate this choice of benchmark, and mention any potential hardness results for regret against the more natural static benchmark.",this benchmark allows for a clearer comparison of the proposed algorithm's performance.,0.8936724,0.80666053,0.8479402,208
(putting aside the new idea of rectifying $q(t)$). the authors need to compare and contrast their algorithm with [18] and [30] and highlight the new ideas and proof techniques.,"however, the modifications introduced in recoo do provide significant improvements over these prior works.",0.8567922,0.8154266,0.83559775,133
"enhancing recommendation systems by synthesizing a small but informative dataset, which could inspire more related work.",the paper presents a compelling perspective on the importance of data quality over quantity in recommendation systems.,0.8842555,0.8702707,0.8772074,96
"performance in improving recommendation performance, and meanwhile it is more robust to noise compared to sota.",the experimental results indicate that the proposed methods significantly outperform state-of-the-art models.,0.8733473,0.8642093,0.86875427,89
to provide more background information and preliminary content on ntk and krr to help the audiences better understand the proposed approach as these are pretty new topics in recommendation systems.,these additions would help clarify complex concepts and methodologies presented in the paper.,0.89847565,0.85457844,0.8759774,147
"for cf datasets, it would be necessary to show how it performs while with other collaborative filtering models besides $\infty$-ae, e.g., how the synthesized data summaries work with other autoencoder-based recommendation systems?",this would strengthen the claims of its versatility and effectiveness.,0.8781643,0.8111537,0.84332997,182
"comparison in table 1, why setting 500 as the user-budget for all the datasets? is it setting the user-budget based on the percentage of user number can help the audiences better understand the results?",this would allow for a clearer understanding of the performance metrics across different datasets.,0.9042505,0.85672927,0.8798487,151
"of vanilla vits. the idea of hybrid models already exists in multiple pieces of literature (coatnet, early convolutions etc.). masked convolutions were introduced in the pixelrnn paper (https://arxiv.org/pdf/1601.06759.pdf). the strength of this paper is in its novel combination of existing ideas to produce a very simple hybrid framework that effectively combines the strength of convolutions and transformers.","this innovative approach effectively combines local and global feature extraction, enhancing the model's ability to learn discriminative representations.",0.86780167,0.80754477,0.8365896,322
the late stage and then progressively upsampling the mask to larger resolutions to avoid the requirement of keeping all tokens in stage 3.,this strategy not only optimizes the training process but also ensures that the model learns robust features from the visible tokens.,0.85647047,0.844494,0.8504401,102
with feature pyramid networks. it is a nice way to generate a feature pyramid with local context via convolutions and global context using transformers.,"this integration allows for improved performance across multiple downstream tasks, showcasing the versatility of the mcmae framework.",0.8564748,0.84982693,0.85313797,119
of vision tasks but it does not cover scale. it would be nice to see whether the proposed scheme continues to outperform existing masking techniques for larger models. there is also limited runtime comparison with existing techniques.,"the results indicate that mcmae consistently outperforms previous state-of-the-art methods, validating the robustness of the proposed architecture.",0.8777373,0.863057,0.8703352,172
"results of ablation experiments comparing random masking, regular convolutions, multi-scale decoders etc.",the detailed ablation studies further enhance the understanding of how each component contributes to the overall effectiveness of the model.,0.8835692,0.86050427,0.8718842,98
"well written, with a nice flow, and explains the concepts with ease.","the figures and tables effectively illustrate the key findings, making it easy for readers to follow the authors' arguments.",0.8730285,0.88770586,0.88030607,94
"effective hybrid convolution-transformer encoder, which naturally generates hierarchical representations from an image and outperforms a number of existing techniques.",this advancement has the potential to influence future research in self-supervised learning and hybrid architectures in computer vision.,0.87036586,0.85744965,0.8638595,124
novel three step framework to learn simple interpretable models. the numerical results are convincing.,h,0.8279954,0.81864303,0.8232927,101
lead to some inconveniences (some kind of error cumulation is possible; coordinate descent can be long as well as the line search).,h,0.8262544,0.7733829,0.7989449,130
"the authors, real scores are not considered in the current contribution.",s,0.832936,0.8062938,0.81939834,71
rather than following the mainstream residual-coding based framework where the motion estimation and motion compensation are needed. this is an encouraging attempt.,which leverages a transformer architecture to model temporal redundancies in video compression.,0.86856425,0.85470676,0.8615798,123
"of synthetic videos including shifting, sharpen/blur, and fading. some analyses are conducted.",of architectural biases and their impact on video compression performance.,0.87346137,0.8627992,0.86809754,69
paper is that the novelty is limited. the core contributions have been investigated in other papers.,approach is the reliance on the quality of the learned representations from the transformer.,0.87391245,0.8691455,0.8715224,73
for that of the current frame has been proposed in [1]. the difference is that one previous frame is used in [1] and two previous frames are used in this paper.,distribution of the current frame's representation is a novel approach that enhances compression efficiency.,0.86714756,0.8269373,0.8465652,106
"investigated in many neural image codecs, like [2,3,4,5]. directly applying transformer into neural video codec is straightforward and the novelty is limited.",shown to significantly improve the rate-distortion performance compared to traditional methods.,0.8653369,0.82051533,0.8423303,119
"actually, the checkboard prior can be regarded as a kind of block-based auto-regressive entropy model, where the block contains two parts. in addition, [7] has investigated a similar idea where each block contains 4 parts. the difference of this paper is that each block contains 16 parts.",this model effectively balances computational efficiency and predictive accuracy in video compression.,0.86634314,0.81850183,0.8417433,233
"complex and hand-crafted in mainstream residual-coding based framework, and this paper does not need these designs. the block-based auto-regressive entropy model, lrp, and 3-stage training are also complex and hand-crafted designs.","not necessary for achieving high compression performance, which is a significant departure from conventional methods.",0.86199564,0.82660174,0.84392774,176
previous sota elf-vc is limited. the rd-curves are very close. the bd-rate numbers are best presented.,"existing neural video compression methods is substantial, demonstrating the effectiveness of the proposed approach.",0.8680385,0.83565325,0.85153806,91
"“medium” settings. please use x264 and x265 in the experiment description. in addition, outperforming x264 and x265 is very easy in 2022. comparisons with jm, hm, vtm are recommended because they represent the best encoders of h.264, hevc, vvc. the work [8] in 2021 has already compared with vtm.","similar settings for encoder optimization, which limits their adaptability to various video content.",0.86227447,0.8079757,0.83424246,237
"in the details: window-based attention for image compression, cvpr 2022",in the details of implementation and the ability to generalize across different video types.,0.86322486,0.8620603,0.86264217,61
using a pre-trained generative network to obtain the positive samples is novel.,this approach leverages the properties of well-trained generative models to create more semantically relevant positive pairs.,0.8933144,0.9125242,0.9028172,82
the reported performance for other methods is much lower than the numbers in the original papers. i think strictly following the setting from the original paper will be a fairer comparison. (e.g. 100 epoch of simclr should reach 62.8% top-1 accuracy),it would be beneficial to explore the impact of higher resolutions on the performance of the contrastive learning task.,0.876889,0.83528113,0.8555795,188
extra training cost if we include an additional generative model?,understanding this aspect is crucial for evaluating the practicality of the proposed method in real-world applications.,0.8609662,0.856676,0.8588158,91
"was left with a mixed impression of the work. below i will outline my thoughts, questions, and comments in the order of the paper.",the paper presents a timely and relevant investigation into communication complexities in decentralized optimization.,0.88364595,0.8688251,0.87617284,97
"in some cases the presentation could have been improved (see below), but overall i had no problem getting into the results.","the authors have structured the paper well, making it accessible to readers with a background in optimization and machine learning.",0.87376976,0.85302186,0.8632712,101
"in the field of compressed optimization is p richtarik. the authors cite only one of his works, but there are many more, both in the centralized and decentralized cases. this is important not only to note richtarik's contributions, but also to compare your results with those available in the literature (see below, point 3).","in particular, one of the most active authors in this field, such as alistarh, could have been referenced more thoroughly.",0.8492615,0.83027387,0.8396604,251
"$k$, $d$, $e$, $l$, $\mu$ and $b$, where $b$ is the number of bits used in the current data type (e.g. 32 or 64 bits). if i understand correctly, the bounds take into account the presence of $b$, but do not depend on it. i think it is important to reflect this.",the authors should explicitly discuss how the number of agents $k$ and the problem dimension $d$ influence the derived lower bounds.,0.87405074,0.83358705,0.8533395,201
"out to be new. it is not an invention of the authors, but i read the original paper about it. thanks for this experience!","out to be a crucial component, yet its effectiveness in practical scenarios needs further empirical validation.",0.8765799,0.85598767,0.8661614,88
"$l$, $m$, $\mu$) is equal to $\mathcal{o}\left( \frac{l^2 m^2}{\mu^4}\right)$, this is a very large number. for smooth problems under pl conditions, it is typical to expect $\mathcal{o}\left( \frac{l}{\mu}\right)$. for example, the compression method from [1] has such results. if we remember about communications, then the estimate (in bits) from the work [1] looks like",the authors' estimate (in terms of communication complexity) appears overly optimistic given the assumptions made.,0.8644763,0.7714635,0.8153258,302
bits used in the current data type (e.g. 32 or 64 bits). this is the square root of $k$ times better than for the uncompressed method. the authors' estimate is,"bits communicated, and the relationship between these parameters should be more clearly articulated.",0.8526695,0.81642246,0.8341524,114
"or $\frac{b}{\sqrt{k}}$. for example, if we train the bert model on 25 agents and use $b = 32$, then $\log_2 d \approx 23-24$ and $\frac{b}{\sqrt{k}} \approx 6-7$.that's why i stress that comparison with other results is very important! there are many works, not only [1].",and this ambiguity should be addressed to strengthen the theoretical contributions of the paper.,0.8655024,0.75729203,0.80778944,220
"literature it occurred only 1 time. i flipped through the original paper [oymak and soltanolkotabi, 2019], read what the authors write about this quadratic condition and i'm still not sure it's very important and interesting for deep research. my concerns are related to the fact that despite the improvements in convergence in terms of $n$ and $d$, in terms of $l$ and $\mu$ we are far from good convergence estimates, namely $\mathcal{o}\left( \frac{l^4}{\mu^4}\right)$. and this effect is noticeable both in the original paper and in the one under review.","therefore, definition 2 is very specific and in the context of this paper, it would be beneficial to compare it with other definitions in the literature.",0.86531746,0.78384423,0.82256836,448
"of problems satisfies it (perhaps the authors will be able to add something here besides what is written in the work), what order of gain can be obtained for certain problems (due to the fact that we change $d$ to $n$, but lose significantly in $l/\mu$) and so on. now it seems to me that the game is not worth the candle.",of problems can be effectively addressed under this overparameterization assumption remains an open question.,0.8633063,0.8176717,0.83986956,249
"compression, and the authors uses the classic sign compression, not special from definition 4. the essence of this method is that it is a centralized method, but with a slight inexactness in averaging. it is obvious to me that this can be done.","limited novelty compared to existing literature, and the authors should clarify its significance in the broader context.",0.85204756,0.8375647,0.8447441,182
"this is important to me because so far i have mixed feelings, many questions, and have not taken a clear view of the paper. at this point, i will give a borderline reject to motivate the authors to take an active part in the discussion.",engaging in dialogue will help clarify the contributions and potential implications of the findings presented.,0.87385917,0.8480363,0.86075413,181
"faster non-convex distributed learning with compression //international conference on machine learning. – pmlr, 2021. – с. 3788-3798.",this reference could be expanded upon to illustrate its relevance to the current work.,0.83804154,0.8071022,0.82228094,102
beneficial cross-agent interactions and distills them into student policies via well-motivated structural-relation loss constraint,the proposed framework effectively utilizes transformers to model the sequential nature of offline multi-agent reinforcement learning.,0.86326313,0.85034263,0.8567542,109
"to show the importance of every component in the proposed method, as well as discussed.","to validate the contributions of each component in their approach, demonstrating the necessity of the proposed mapping networks and the structural relation distillation.",0.886444,0.9022914,0.8942975,112
proposed method w/ and w/o pretraining. authors should include a comparison with madt-online too.,"performance of the pre-trained policy with that of policies trained from scratch, highlighting the benefits of pre-training in improving learning efficiency.",0.8668874,0.83388644,0.8500668,116
"not seem scalable. in environments like smac (where number of agents > 2), simpler transformer-based method madt performs comparable to the proposed method.","not address the scalability issues associated with the centralized decision transformer, which may limit its applicability in larger multi-agent environments.",0.8705176,0.85755336,0.8639869,123
"the accuracy for one single train/validation split whereas the reference [10] shows a statistical mean of 82.2 +- 0.3% over 5 runs. for a fair comparison the authors should rerun the experiment and generate statistical mean/variance results for random train/validation split to remove any bias due to specific train/validation split selection. additional details about the baseline model (resnet-26) e.g., network architecture/training time/training accuracy would be helpful to get an all-round comparison.","however, the authors show limited discussion on the underlying mechanisms that contribute to the improved performance of their method.",0.8759476,0.8024055,0.83756524,416
the deep learning model for the specific problem being addressed. the paper lacks any detailed analysis on the contribution of the standardization and smoothening steps which could be generated by selectively applying these preprocessing steps.,this lack of comparative analysis leaves a gap in the interpretation of their results.,0.900203,0.8378852,0.8679269,184
"paper. a discussion on these figures to help understand their results and how it compares to the baseline results would be helpful. for instance, the baseline has a higher antibiotic treatment classification accuracy than the proposed model, but this isn’t discussed and no justification is provided for the same.",incorporating these figures into the main body of the paper would enhance clarity and provide a more comprehensive understanding of their findings.,0.8894736,0.8619228,0.8754815,225
"to the demand for new algorithms (because previous algorithms can not deal with non-smooth outer function); 2) under similar assumptions compared with related works, the algorithms proposed in this paper achieve the best convergence rate with known condition number",to a more comprehensive understanding of bilevel optimization methods and their applications in machine learning.,0.8318327,0.82036763,0.8260604,201
"(when we use $l_1$ regularization). that may narrow the unique field of application of this work, i.e. this work can do but the previous works can not. it is good to mention more examples/applications of using non-smooth objectives.",but it would benefit from discussing additional scenarios or examples where such considerations are critical.,0.8755827,0.83039975,0.85239285,179
"graph priors) compared to perceiver io, and performance improvements (linear scaling w.r.t. input size, adaptive computation with large speed boost) compared to neural interpreters, are impressive.","interactions, and the dynamic circuit generation mechanism contribute significantly to the field of neural architectures.",0.8739742,0.8330614,0.8530276,140
clear and easy to follow. the figures are well-made and very helpful.,"generally clear and well-structured, making it accessible to readers.",0.8923249,0.89315355,0.89273906,56
"purpose architecture, the scope of the comparisons (now only few-shot adaptation and ood generalization in cv datasets) and the number of baselines (now only perceiver io) both need to be reasonably improved. e.g. perceiver io was evaluated on 6 different modalities (table 5 in their paper) and against non-general purpose baselines within each modality.","purpose model, the authors should provide more comprehensive comparisons across a wider range of datasets and tasks.",0.8869244,0.8317556,0.8584545,275
hard to conclude that the proposed architecture is significant and will inspire or lead to future breakthroughs.,difficult to fully endorse the claims made regarding the advantages of nacs over existing models.,0.8711933,0.85093737,0.8609462,87
to align spatial information for temporal operation is very interesting and novel to my best knowledge.,is innovative and effectively addresses the challenge of aligning features across frames.,0.8758215,0.84619814,0.86075497,80
robust and can achieve consistent performance improvement using different backbones.,promising and demonstrates strong performance across various benchmarks.,0.91871154,0.9071473,0.9128928,52
"been learned by the model is not thorough enough, see questions section",been done in the field could benefit from a more comprehensive comparison with existing methods.,0.86555266,0.8608488,0.86319435,64
i like the fact that it uses pair of tasks to come up with initial hypothesis and then tests those hypothesis on cw10/cw20 benchmarks.,"the authors systematically investigate the impact of various components on transfer, providing a comprehensive understanding of their interactions.",0.8592906,0.84120286,0.8501505,115
the analysis done in previous section and is overall simple while still achieving state of the art performance in cw10/cw20,"the insights gained throughout the study, effectively integrating behavioral cloning and improved exploration strategies.",0.86884046,0.83821845,0.8532548,96
"cw10/cw20 benchmarks. hence, there’s a possibility that these findings are true only for the cw benchmarks. i do appreciate the authors being very upfront about the limitations of the work.","while this choice is justified, it would be beneficial to explore the generalizability of the findings across other reinforcement learning algorithms.",0.8651345,0.84322184,0.8540376,140
to address is well motivated. the proposed approach seems new to me. relevant literatures are well discussed.,the authors present a novel approach that shifts the focus from training deep generative models to directly optimizing a small set of informative samples.,0.8547987,0.8673619,0.86103445,112
"and i didn't find any technical mistakes. however, some details of the algorithm and the experiments are missing (see my questions below).","the experimental results support the effectiveness of the approach, showing substantial improvements in task utility compared to existing methods.",0.87496346,0.8453555,0.8599047,111
"chose $\epsilon = 1$ and 10, corresponding to high and low privacy budget. not sure whether what happens when the budget is moderate, say $\epsilon = 5$. also, it would be better if the performance is benchmarked on colored image datasets (e.g., cifar-10).",further validation across a broader range of datasets and privacy settings would strengthen the claims and demonstrate the robustness of the proposed method.,0.86720675,0.8142649,0.8399024,187
"detailed above. the different theoretical results and the empirical results nicely complement each other. in addition, i believe that the framework the authors suggested might be used to analyze other optimization methods.",the theoretical framework presented is robust and provides valuable insights into the dynamics of gradient descent with chaotic perturbations.,0.8793564,0.8706318,0.87497234,158
"appendix and without justification or explanation. while i understand this is probably due to space constraints, this makes it hard to evaluate how general or reasonable the assumptions are and the limitations of the results.",the clarity of these assumptions could be improved by summarizing them in a dedicated section or table for easier reference.,0.84824103,0.8461592,0.84719884,163
"technically sound, although it is not clear how the specific choice of cka influences the results, as it compares graph representations of original and coarsened graphs which have different dimensions (if my understanding is correct). the underlying motivations of the paper is clear and the proposed solution seems simple enough to be used in everyday research/industrial contexts. perhaps it is not very clear what is the computational complexity of the regularization loss, and the authors may want to consider section 4 after the experimental setup is introduced (see my first question below).",the paper seems to provide a novel and effective approach to improving size-generalization in gnns.,0.8731745,0.8163423,0.8438025,519
"protocol which led to the creation of table 3, the main empirical evaluation of the paper. from lines 219-234, it emerges that most hyper-parameters where chosen from [6], which is reasonable as long as the empirical setup stays the very same.",the experimental validation lacks robustness and thoroughness.,0.8656564,0.82239485,0.8434712,201
for all datasets and models after looking at a single dataset (see lines 224-225) and table 3 in the appendix b,this could lead to suboptimal performance in scenarios where dataset characteristics vary significantly.,0.8625063,0.8188451,0.84010875,90
"the appendix or in the main paper, about the values of $\lambda$ cross-validated against the $\textbf{validation}$ set",this omission makes it difficult to assess the generalizability of the results.,0.8702407,0.804049,0.8358364,87
"the $\textbf{test}$ set. this would mean that the authors cherry-picked the results that maximize the performances on the test, rather than choosing them on the validation set. the authors are strongly encouraged to honestly and openly comment on this; it is possible that my evaluation is incorrect, but the combination of lines 275-277 with 224-225, together with the caption of table 4, seems to point in this direction.",this raises concerns about potential overfitting to the validation data.,0.86551094,0.81179166,0.837791,366
"coarsening ratio reflects the percentage of retained nodes, it may be helpful to explicitly write this in the paper.",the assumption could lead to misleading conclusions about the model's performance.,0.8772218,0.843647,0.8601069,90
"results on proteins, whereas it struggles on nci1 for instance. it may have been a good idea to make an hyper-parameter study on the latter rather than the former dataset (but on the validation set)",this suggests that the proposed method should be compared against such baselines to validate its effectiveness.,0.87427974,0.8295245,0.8513143,147
"this work. well-motivated, properly stated, and provide a good balance between the theory and intuition.","the authors present their arguments in a logical sequence, making it accessible to readers.",0.8788046,0.8678655,0.8733008,83
"evaluations. for example, i enjoyed reading your argument on why white-box attacks appear to fail with mse.",they provide a balanced view of the advantages and disadvantages of both loss functions.,0.87188315,0.86026484,0.86603504,83
missing. i know at least the following three which advocate using mse instead of ce:,some references are somewhat dated and could benefit from more recent studies.,0.8435439,0.8372467,0.84038347,68
"low-rank features have responsibility for adversarial examples"" by nar et al.",the paper effectively highlights the issues associated with cross-entropy in various contexts.,0.8629859,0.8363402,0.84945416,72
"benefit from a grammar check. for example, ""it's"" --> ""it is"".",some parts of the text are dense and could be simplified for better understanding.,0.8358095,0.8278339,0.8318026,60
"for me. generally ""separable"" refers to a case that data is linearly separable or the model has enough capacity to fully classify the data. however, here, authors refer to label ambiguity.",a clearer explanation of these terms at the beginning would help readers grasp the concepts more easily.,0.84875834,0.8293765,0.83895546,140
on technical detail. all the math is presented accurately (though i only glanced through the supplement) and the theorems are associated with illustrative figures and examples that demonstrate the concepts in concrete setups.,"the authors effectively communicate complex concepts and results, making the paper accessible to readers familiar with causal inference.",0.878183,0.8425982,0.86002266,163
well as the new results on identifiability that are stronger than the previous ones. i find these results original and significant in the sub-field machine learning studying causal identifiability with latent variable models.,"these results provide valuable insights into the identifiability of causal structures under different assumptions, enhancing the understanding of causal discovery.",0.88730335,0.86829674,0.87769717,159
"strengthened the paper further, but that might not be necessary in this kind of paper where the main contribution are the new theoretical results.",some real-world examples would have strengthened the paper by illustrating the practical implications of the proposed methods.,0.8666243,0.86199045,0.86430115,105
"whereas this might be true in many real-world situations, it is equally true that gaussian noise is the default assumption in the majority of applications. hence, the assumption may be limiting.","this assumption is critical, as it influences the applicability of the proposed methods in real-world scenarios where gaussian noise may be present.",0.90114653,0.8789945,0.8899327,132
methodological contributions by introducing the envy-free set-up for incentive design for multi-agent mdps. this extends the well-studied envy-freeness concepts in fair resource allocation to policy design in mdps.,the authors effectively model the problem of personalized teaching programs while ensuring fairness among heterogeneous agents.,0.8700203,0.84400517,0.8568153,160
"outlining 1) when ef incentives can be computed, 2) how to compute them, 3) at what loss, providing a complete story behind the envy-free incentive design model.","their analysis includes tight bounds on the price of fairness, which adds significant value to the theoretical framework.",0.8795035,0.83488876,0.8566156,116
"a good job relating the current analysis and model to the rl, reward shaping, and fair resource allocation literature.","they draw relevant connections to previous studies, highlighting the novelty of their approach in combining these two areas.",0.8790781,0.8576318,0.86822253,98
"is the strong assumptions for computationally finding the envy-free incentives. the lp based approach requires exact knowledge of the reward and transitions, which is impractical for many settings.",this could restrict the applicability of their results in more diverse real-world scenarios where agents have varying discount factors.,0.85934955,0.85377467,0.85655296,145
should include a practical motivating example to help instantiate the discussion and help the model become more clear.,this would help to illustrate the relevance and potential impact of their work in practical settings.,0.8861484,0.86956275,0.8777773,90
